<!--./week1/001courseLogistics/index.Rmd
-->
---
title       : Course logistics
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
twitter     : "Data Analysis - Getting Started"
---

## Pre-requisites
* There are no formal pre-requisites
* But all data analysis will be performed in the R programming language: [http://www.r-project.org/](http://www.r-project.org/)
* You can find some videos on how to install R here: [http://bit.ly/UCJI9M](http://bit.ly/UCJI9M).
* Having a very basic knowledge of R will make the class much more accessible
* If you want to assess your knowledge, take this self-graded R pre-quiz: [http://www.biostat.jhsph.edu/~rpeng/coursera/selfquiz/](http://www.biostat.jhsph.edu/~rpeng/coursera/selfquiz/) (this quiz does not count toward your final grade for Data Analysis)
  1. Give yourself 1 point for each correct answer. 
  2. If it takes you less than 1 hour and you get a score of 10 or higher you should have no trouble with the level of R in the course
  3. If it takes you more than 1 hour or your score is less than 10, you might want to check out course videos for Computing for Data Analysis here: [http://bit.ly/UC5UDc](http://bit.ly/UC5UDc)


---

## Why R?

* It is free. 
* It is the most popular language for data analysis.
* Typing is better than point-and-click
  * Easier to communicate
  * Reproducible
  * Requires more thought
* It has a huge number of useful packages (as you will see)

---

## Course Structure

* My goal is to make all videos 10-15 minutes
* Several topics may be broken down into sub-components
* R code will be included in the slides
* Slides will be availble in pdf and html form. 

---

## Grading

* There will be a total of 8 weekly quizzes each worth 10 points. 
* There will be two peer-reviewed data analysis reports worth 40 points each. 
* There are 160 total points for the course
* To earn the certificate for the course you need to earn 100 points. 
* To earn distinction for the course you need to earn 144 points. 

---

## Grading

* You may attempt each quiz up to 4 times. Only the last attempt will count. 

* The data analysis you submit will be scored by your peers using a defined rubric. Your final score for the data analysis will be the median of the peer review scores. 

* You have up to 5 late days during the course of the term, which you may use on the quizzes. 

* You may not use [late days](http://help.coursera.org/customer/portal/articles/502593-how-much-work-should-i-expect-from-one-class-) on the peer-reviewed assignements.

* See the course logistics page for assignment due dates. 

---
## Scoring for the data analysis assignments

* You will get one week after the data analysis deadline to complete peer review of the assignment. 

* Each data analysis assignment has four parts: the main text, a figure and caption, the references and R code. 

* Each part will be scored on multiple criteria.

* When grading your peers you will give 0-5 points for each criteria.

* The final score will be the percentage of available points multiplied by 40. 

---

## Data analysis rubric
__Main text__
* Does the analysis have an introduction, methods, analysis, and conclusions? 
* Are figures labeled and referred to by number in the text? 
* Is the analysis written in grammatically correct English?
* Are the names of variables reported in plain language, rather than in coded names?
* Does the analysis report the number of samples?
* Does the analysis report any missing data or other unusual features?
* Does the analysis include a discussion of potential confounders?
* Are the statistical models appropriately applied?
* Are estimates reported with appropriate units and measures of uncertainty?
* Are estimators/predictions appropriately interpreted?
* Does the analysis make concrete conclusions?
* Does the analysis specify potential problems with the conclusions? 

---
## Data analysis rubric

__Figure__
* Is the figure caption descriptive enough to stand alone?
* Does the figure focus on a key issue in the processing/modeling of the data?
* Are axes labeled and are the labels large enough to read?

__References__
* Does the analysis include references for the statistical methods used?

__R script__

* Can the analysis be reproduced with the code provided?

---



## Typos/errors/differences of opinion

* I'm prone to a typo or two
* This is my first time giving video lectures
* I'm happy to get feedback in the "Feedback" forum
* I'll try to address as many of the issues as I can
* Keep in mind that currently data analysis is as much art as it is science. 

---

## Getting the slides

* Slides for this course were created with Slidify: [http://ramnathv.github.com/slidify/](http://ramnathv.github.com/slidify/). 
* They are available from [https://github.com/jtleek/dataanalysis](https://github.com/jtleek/dataanalysis).
* To re-compile the slides:
  1. Download the directory containing the lecture from Github
  2. Set the working directory to the lecture directory
  3. [Install Slidify](http://slidify.org/start)
  4. Run the following commands: 
  
```{r slidifyChunk, eval=FALSE}
library(slidify) 
slidify("index.Rmd")
```


<!--=*=*=*=*-->
<!--./week1/002gettingHelp/index.Rmd
-->
---
title       : Getting help
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : zenburn   # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---

## Asking questions

* __In a standard class__
  * There are 30-100 people
  * You raise your hand and ask a question
  * The instructor responds 
* __In a MOOC__
  * There are almost 100,000 people
  * You post a question to the message board
  * Others vote on your questions
  * Your instructor responds (as often as possible)
  * Your peers respond (as often as possible)
  

---

## Often the fastest answer is the one you find yourself

* It's important to try to answer your own questions first
* If the answer to your question is in the help file or the top hit on Google, the answer to your question will be, "Read the documentation" or "Google it"
* If you figure out the answer and see the same questions on the forum, post the solution you found

---

## Where to look for different types of questions

* R programming (see also: [http://bit.ly/Ufaadn](http://bit.ly/Ufaadn))
  * Search the archive of the class forums
  * Read the manual/help files
  * Search on the web 
  * Ask a skilled friend
  * Post to the class forums
  * Post to the [R mailing list](http://www.r-project.org/mail.html) or [Stackoverflow](http://stackoverflow.com/)
* Data Analysis/Statistics
  * Search the archive of the class forums
  * Search on the web
  * Ask a skilled friend
  * Post to the class forums
  * Post to [CrossValidated](http://stats.stackexchange.com/) 

---

## Some important R functions

__Access help file__
```{r,tidy=FALSE}

?rnorm

```

__Search help files__
```{r}
help.search("rnorm")
```

__Get arguments__
```{r}
args("rnorm")
```

---

## Some important R functions
__See code__
```{r}
rnorm
```

__R reference card__

[http://cran.r-project.org/doc/contrib/Short-refcard.pdf](http://cran.r-project.org/doc/contrib/Short-refcard.pdf)

---

## How to ask an R question
* What steps will reproduce the problem?
* What is the expected output? 
* What do you see instead?
* What version of the product (e.g. R, packages, etc.) 
are you using?
* What operating system? 

---

## How to ask a data analysis question
* What is the question you are trying to answer?
* What steps/tools did you use to answer it?
* What did you expect to see?
* What do you see instead?
* What other solutions have you thought about?

---

## Be specific in the title of your questions

* Bad:
  * HELP! Can't fit linear model!
  * HELP! Don't understand PCA!

* Better
  * R 2.15.0 lm() function produces seg fault with large data frame, Mac OS X 
10.6.3 
  * Applied principal component analysis to a matrix - what are U, D, and $V^T$?

* Even better
  * R 2.15.0 lm() function on Mac OS X 10.6.3 -- seg fault on large data frame
  * Using principal components to discover common variation in rows of a matrix, should I use U, D or $V^T$?

--- 

## Etiquette for forums/help sites: DOs 

* Describe the goal
* Be explicit
* Provide the minimum information
* Be courteous (never hurts)
* Follow up and post solutions
* Use the forums rather than email

---

## Etiquette for forums/help sites: DON'Ts

* Immediately assume you found a bug
* Grovel as a substitute for doing your work
* Post homework questions on mailing lists (people don't like doing your homework)
* Email multiple mailing lists at once/the wrong mailing list
* Ask others to fix your code without explaining the problem
* Ask about general data analysis questions on R forums.

---

## A note on Googling data analysis questions

* The best place to start for general questions is our forum
* [Stackoverflow](http://stackoverflow.com/),[R mailing list](http://www.r-project.org/mail.html) for software questions, [CrossValidated](http://stats.stackexchange.com/) for more general questions
* Otherwise Google "[data type] data analysis" or "[data type] R package"
* Try to identify what data analysis is called for your data type
  * [Biostatistics](http://en.wikipedia.org/wiki/Biostatistics) for medical data
  * [Data Science](http://en.wikipedia.org/wiki/Data_science) for data from web analytics
  * [Machine learning](http://en.wikipedia.org/wiki/Machine_learning) for data in computer science/computer vision
  * [Natural language processing](http://en.wikipedia.org/wiki/Natural_language_processing ) for data from texts
  * [Signal processing](http://en.wikipedia.org/wiki/Signal_processing) for data from electrical signals
  * [Business analytics](http://en.wikipedia.org/wiki/Business_analytics) for data on customers
  * [Econometrics](http://en.wikipedia.org/wiki/Econometrics) for economic data
  * [Statistical process control](http://en.wikipedia.org/wiki/Statistical_process_control) for data about industrial processes 
  * etc.


---

## Further resources

* Some R resources you might find useful
  * Roger's Computing for Data Analysis [Videos](http://www.youtube.com/user/rdpeng/videos?flow=grid&view=1) on Youtube
  * A set of [two-minute R tutorials](http://www.twotorials.com/)
  
* Some Data Analysis Resources you might find useful
  * [The Elements of Statistical Learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/download.html)
  * [Advanced Data Analysis from an Elementary Point of View](http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf)

---

## Credits

* Roger's [Getting Help Video](http://www.youtube.com/watch?v=ZFaWxxzouCY&list=PLjTlxb-wKvXNSDfcKPFH2gzHGyjpeCZmJ&index=3)
* Inspired by Eric Raymond's "How to ask questions the smart way"
<!--=*=*=*=*-->
<!--./week1/003whatIsData/index.Rmd
-->
---
title       : What is data? 
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : zenburn   # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---

## Definition of data
<q>Data are values of qualitative or quantitative variables, belonging to a set of items.</q>

[http://en.wikipedia.org/wiki/Data](http://en.wikipedia.org/wiki/Data)


---

## Definition of data
<q>Data are values of qualitative or quantitative variables, belonging to a <redtext>set of items</redtext>.</q>

[http://en.wikipedia.org/wiki/Data](http://en.wikipedia.org/wiki/Data)

__Set of items__: Sometimes called the population; the set of objects you are interested in



---

## Definition of data
<q>Data are values of qualitative or quantitative <redtext>variables</redtext>, belonging to a set of items.</q>

[http://en.wikipedia.org/wiki/Data](http://en.wikipedia.org/wiki/Data)

__Variables__: A measurement or characteristic of an item.


---

## Definition of data
<q>Data are values of <redtext>qualitative</redtext> or <redtext>quantitative</redtext> variables, belonging to a set of items.</q>

[http://en.wikipedia.org/wiki/Data](http://en.wikipedia.org/wiki/Data)


__Qualitative__: Country of origin, sex, treatment

__Quantitative__: Height, weight, blood pressure

---

## Raw versus processed data

__Raw data__
* The original source of the data
* Often hard to use for data analyses
* Data analysis _includes_ processing
* Raw data may only need to be processed once

[http://en.wikipedia.org/wiki/Raw_data](http://en.wikipedia.org/wiki/Raw_data)

__Processed data__
* Data that is ready for analysis
* Processing can include merging, subsetting, transforming, etc.
* There may be standards for processing
* All steps should be recorded 

[http://en.wikipedia.org/wiki/Computer_data_processing](http://en.wikipedia.org/wiki/Computer_data_processing)

---

## An example of a processing pipeline

<img class=center src=assets/img/hiseq.jpg height='80%'/>

[http://www.illumina.com.cn/support/sequencing/sequencing_instruments/hiseq_1000.asp](http://www.illumina.com.cn/support/sequencing/sequencing_instruments/hiseq_1000.asp)

---

## An example of a processing pipeline

<img class=center src=assets/img/processing.png height='80%'/>

[http://www.cbcb.umd.edu/~hcorrada/CMSC858B/lectures/lect22_seqIntro/seqIntro.pdf](http://www.cbcb.umd.edu/~hcorrada/CMSC858B/lectures/lect22_seqIntro/seqIntro.pdf)

---


## What do raw data look like? 

<img class=center src=assets/img/fastq.png height='80%'/>


[http://brianknaus.com/software/srtoolbox/s_4_1_sequence80.txt](http://brianknaus.com/software/srtoolbox/s_4_1_sequence80.txt)

---

## What do raw data look like? 

<img class=center src=assets/img/twitter.png height='80%'/>

[https://dev.twitter.com/docs/api/1/get/blocks/blocking](https://dev.twitter.com/docs/api/1/get/blocks/blocking)

---

## What do raw data look like? 

<img class=center src=assets/img/medicalrecord.png height='60%'/>

[http://blue-button.github.com/challenge/](http://blue-button.github.com/challenge/)

---

## What do processed data look like?

<img class=center src=assets/img/excel.png height='50%'/>

1. Each variable forms a column
2. Each observation forms a row
3. Each table/file stores data about one kind of observation (e.g. people/hospitals).


[http://vita.had.co.nz/papers/tidy-data.pdf](http://vita.had.co.nz/papers/tidy-data.pdf)

[Leek, Taub, and Pineda 2011 PLoS One](http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0026895)

---

## How much is there?

<img class=center src=assets/img/howmuch.png height='80%'/>

[http://mashable.com/2011/06/28/data-infographic/](http://mashable.com/2011/06/28/data-infographic/)

---

## So what about big data? 

<img class=center src=assets/img/bigdata.png height='80%'/>

---

## Depends on your perspective

<img class=center src=assets/img/ibm350.jpg height='80%'/>

---

## Why big data now?

<img class=center src=assets/img/milgram.jpg height='80%'/>

[Travers and Milgram (1969) Sociometry](http://www.jstor.org/discover/10.2307/2786545?uid=3739704&uid=2&uid=4&uid=3739256&sid=21101674727517)

---

## Why big data now?

<img class=center src=assets/img/leskovec.jpg height='80%'/>

[Leskovec and Horvitz WWW '08](http://arxiv.org/abs/0803.0939)


---

## Big or small - you need the right data
<q>The data may not contain the answer. The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data... </q> 

[Tukey](http://en.wikipedia.org/wiki/John_Tukey)


---

## Big or small - you need the right data

<q>...no matter how big the data are.</q>

[Leek](http://www1.usu.edu/utahstatetoday/images/mar%2003/JeffLeek.jpg)





<!--=*=*=*=*-->
<!--./week1/004representingData/index.Rmd
-->
---
title       : How do we represent data?
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : zenburn   # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 70)
opts_chunk$set(message = F, error = F, warning = F, echo = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache = T, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## How do we write about data?

* Each data point is usually represented by a capital letter. 
  * $H$ for height, $W$ for weight. 
* If there are more than one data point of the same type we use subscripts.
  * $H_1$, $H_2$, $H_3$ for three different people's heights.
* Sometimes it is more compact to write $X_1$ for height and $X_2$ for weight. 
* Then we need another subscript for the individual data point
  * $X_{11}$ for the height of the first person. 
* $Y$ representes general outcomes and $X$ general covariates. 
* In this course we will try to use informative letters when possible. 

---

## Randomness

* Variables like $X$ and $Y$ are called _random variables_ because we expect them to be _random_ in some way. 
* In general, randomness is a hard thing to define
* In this class a variable may be random because
  * It represents an incompletely measured variable
  * It represents a sample drawn from a population using a random mechanism.  
* Once we are talking about a specific value of a variable we have observed it isn't random anymore, we write these values with lower case letters $x,y$, etc. 
* We write $X=x$ or $X=1$ to indicate we have observed a specific value $x$ or $1$. 

---

## Randomness and measurement

* A coin flip is commonly considered random
* But it can be modeled by deterministic equations 
  * Dynamical bias in the coin toss [(Diaconis, Holmes and Montgomery SIAM Review 2007)](http://www-stat.stanford.edu/~cgates/PERSI/papers/dyn_coin_07.pdf)
  * Modeled the tossing as a dynamical system
  * Showed that a coin is more likely to land on the side it started
  * Did experiments that demonstrated it was a 51% chance
* Some have taken it a bit farther making [predictable coin flipping machines](http://www.dotmancando.info/index.php?/projects/coin-flipper/) based on [physical properties](http://www.dotmancando.info/index.php?/projects/coin-flipper/). 

--- 

## Distributions

* In statistical modeling, random variables like $X$ are assumed to be samples from a _distribution_
* A distribution tells us the possible values of $X$ and the probabilities for each value. 
* Probability is the chance something will happen and is abbreviated $Pr$
* The probabilities must all be between 0 and 1. 
* The probabilities must add up to 1. 
* An example:
  * Let's flip a coin and allow $X$ to represent whether it is heads or tails
  * $X = 1$ if it is heads and $X = 0$ if it is tails
  * We expect that about 50% of the time it will be heads.
  * The distribution can then be written Pr($X=1$)=0.5 and Pr($X=0$)=0.5

---

## Continuous versus discrete distributions

* _discrete_ distributions specify probabilities for discrete values
  * Qualitative variables are discrete
  * So are variables that take on all values 0,1,2,3...
* _continuous_ distributions specify probabilities for ranges of values
  * Quantitative variables are often assumed to be continuous
  * But we might only see specific values 


---


## Parameters

* Distributions are defined by a set of fixed values called _parameters_. 
* _parameters_ are sometimes represented by Greek letters like $\mu,\sigma,\tau$. 
* Distributions are written as letters with the parameters in parentheses like $N(\mu,\sigma)$ or $Poisson(\lambda)$.
* $X \sim N(\mu,\sigma)$ means that $X$ has the $N(\mu,\sigma)$ distribution. 


---

## The three most important parameters

* If $X$ is a random variable, the mean of that random variable is written $E[X]$
  * Stands for expected value
  * Measures the "center" of a distribution
* The variance of that random variable is written $Var[X]$
  * Measures how "spread out" a distribution is
  * Measurement is in (units of X)$^2$
* The standard deviation is written $SD[X] = \sqrt{Var[X]}$
  * Also measures how "spread out" a distribution is
  * Measurement is in units of X


---

## Conditioning

* The variables $X$ are considered to be random
* The parameters are considered to be fixed values
* Sometimes we want to talk about a case where one of the random variables is fixed
* To indicate what is fixed, we _condition_ using the symbol "$|$""
  * $X | \mu$ means that $X$ is a random variable with fixed parameter $\mu$
  * $Y | X = 2$ means $Y$ is the random variable $Y$ when $X$ is fixed at 2.


---


## Example distribution: Binomial

__Binomial distribution: $Bin(n,p)$__
* $X \sim Bin(10,0.5)$
```{r, echo=FALSE, fig.height = 4, fig.width = 4}
xvals = 0:10
plot(xvals,dbinom(xvals,10,0.5),type="h",col="blue",lwd=3,ylab="probability",xlab="X value")

```


---

## Example distribution: Normal

__Normal Distribution: $N(\mu,\sigma)$__
* $X \sim N(0,1)$
```{r, echo=FALSE, fig.height = 4, fig.width = 4}
xvals = seq(-10,10,length=100)
plot(xvals,dnorm(xvals,0,1),type="l",col="blue",lwd=3,ylab="density",xlab="X value")

```


---

## Example distribution: Uniform

__Uniform distribution: $U(\alpha,\beta)$__
* $X \sim U(0,1)$
```{r, echo=FALSE, fig.height = 4, fig.width = 4}
xvals = seq(-2,2,length=100)
plot(xvals,dunif(xvals,0,1),type="l",col="blue",lwd=3,ylab="density",xlab="X value")

```

---

## Changing parameters

__Normal Distribution: $N(\mu,\sigma)$__
* $X \sim N(0,1)$, $E[X] = \mu = 0$, $Var[X] = \sigma^2 = 1$
```{r, echo=FALSE, fig.height = 4, fig.width = 4}
xvals = seq(-10,10,length=100)
plot(xvals,dnorm(xvals,0,1),type="l",col="blue",lwd=3,ylab="density",xlab="X value")

```

---

## Changing parameters: the variance

__Normal Distribution: $N(\mu,\sigma)$__
* $X \sim N(0,5)$, $E[X] = \mu = 0$, $Var[X] = \sigma^2 = 25$
```{r, echo=FALSE, fig.height = 4, fig.width = 4}
xvals = seq(-10,10,length=100)
plot(xvals,dnorm(xvals,0,5),type="l",col="blue",lwd=3,ylab="density",xlab="X value")

```

---

## Changing parameters: the mean
__Normal Distribution: $N(\mu,\sigma)$__
* $X \sim N(5,1)$, $E[X] = \mu = 5$, $Var[X] = \sigma^2 = 1$
```{r, echo=FALSE, fig.height = 4, fig.width = 4}
xvals = seq(-10,10,length=100)
plot(xvals,dnorm(xvals,5,1),type="l",col="blue",lwd=3,ylab="density",xlab="X value")

```

---

## Example distribution: Binomial
__Binomial distribution: $Bin(n,p)$__
* $X \sim Bin(10,0.5)$, $E[X] = n \times p = 5$, $Var[X] = n \times p \times (1-p) = 2.5$
```{r, echo=FALSE, fig.height = 4, fig.width = 4}
xvals = 0:10
plot(xvals,dbinom(xvals,10,0.5),type="h",col="blue",lwd=3,ylab="probability",xlab="X value")

```


---

## Changing parameters: both mean and variance

__Binomial distribution: $Bin(n,p)$__
* $X \sim Bin(10,0.8)$, $E[X] = n \times p = 8$, $Var[X] = n \times p \times (1-p) = 1.6$
```{r, echo=FALSE, fig.height = 4, fig.width = 4}
xvals = 0:10
plot(xvals,dbinom(xvals,10,0.8),type="h",col="blue",lwd=3,ylab="probability",xlab="X value")

```

---


## Conditioning

* Suppose $Y \sim N(X,1)$ and $X \sim N(0,1)$, then the distribution of $Y |X = 5$ is

```{r, echo=FALSE, fig.height = 4, fig.width = 4}
xvals = seq(-10,10,length=100)
plot(xvals,dnorm(xvals,5,1),type="l",col="blue",lwd=3,ylab="density",xlab="Y value")

```

---

## Conditioning

* Suppose $Y \sim N(X,1)$ and $X \sim N(0,1)$, then the distribution of $Y$ is

```{r, echo=FALSE, fig.height = 4, fig.width = 4}
xvals = seq(-10,10,length=100)
plot(xvals,dnorm(xvals,0,2),type="l",col="blue",lwd=3,ylab="density",xlab="X value")

```

[http://en.wikipedia.org/wiki/Law_of_total_variance](http://en.wikipedia.org/wiki/Law_of_total_variance)

[http://en.wikipedia.org/wiki/Law_of_total_expectation](http://en.wikipedia.org/wiki/Law_of_total_expectation)

---


## Learning more about a specific distribution

<img class=center src=assets/img/poisson.png height='80%'/>

[http://en.wikipedia.org/wiki/Poisson_distribution](http://en.wikipedia.org/wiki/Poisson_distribution)

---

## Learning more about representing data

<img class=center src=assets/img/openintro.png height='80%'/>

[http://www.openintro.org/stat/textbook.php](http://www.openintro.org/stat/textbook.php)


<!--=*=*=*=*-->
<!--./week1/005representingDataR/index.Rmd
-->
---
title       : Representing data in R
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : zenburn   # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---
## Important data types in R

__Classes__
* Character, Numeric, Integer, Logical

***********

__Objects__

* Vectors, Matrices, Data frames, Lists, Factors, Missing values

***********
__Operations__

* Subsetting, Logical subsetting

***********

_For more information_: 
* [Data Types](http://www.youtube.com/watch?v=5AQM-yUX9zg&list=PLjTlxb-wKvXNSDfcKPFH2gzHGyjpeCZmJ&index=5)


---

## Character

```{r}
firstName = "jeff"
class(firstName)
firstName
```

---

## Numeric
```{r}
heightCM = 188.2
class(heightCM)
heightCM
```

---

## Integer
```{r}
numberSons = 1L
class(numberSons)
numberSons
```

---

## Logical
```{r}
teachingCoursera = TRUE
class(teachingCoursera)
teachingCoursera
```

---
## Vectors
A set of values with the same class
```{r}
heights = c(188.2, 181.3, 193.4)
heights

firstNames = c("jeff","roger","andrew","brian")
firstNames

```

---

## Lists
A vector of values of possibly different classes
```{r}
vector1 = c(188.2, 181.3, 193.4)
vector2 = c("jeff","roger","andrew","brian")
myList = list(heights=vector1,firstNames=vector2)
myList

```

---

## Matrices
Vectors with multiple dimensions
```{r}
myMatrix = matrix(c(1,2,3,4),byrow=T,nrow=2)
myMatrix

```

---

## Data frames
Multiple vectors of possibly different classes, of the same length
```{r}
vector1 = c(188.2, 181.3, 193.4)
vector2 = c("jeff","roger","andrew","brian")
myDataFrame = data.frame(heights=vector1,firstNames=vector2)
myDataFrame

```

---

## Data frames

```{r}
vector1 = c(188.2,181.3,193.4,192.3)
vector2 = c("jeff","roger","andrew","brian")
myDataFrame = data.frame(heights=vector1,firstNames=vector2)
myDataFrame
```

---
## Factors
Qualitative variables that can be included in models

```{r}
smoker = c("yes","no","yes","yes")
smokerFactor = as.factor(smoker)
smokerFactor

```

---

## Missing values
In R they are usually coded NA

```{r}
vector1 = c(188.2,181.3,193.4,NA)
vector1
is.na(vector1)
```


---

## Subsetting

```{r}
vector1 = c(188.2,181.3,193.4,192.3)
vector2 = c("jeff","roger","andrew","brian")
myDataFrame = data.frame(heights=vector1,firstNames=vector2)

vector1[1]
vector1[c(1,2,4)]

```

---

## Subsetting

```{r}
myDataFrame[1,1:2]
myDataFrame$firstNames

```
---

## Logical subsetting
```{r}
myDataFrame[myDataFrame$firstNames=="jeff",]
myDataFrame[myDataFrame$heights < 190,]
```

---

## Variable naming conventions

Variable names should be short, but descriptive. Here are some common styles

__Camel caps__
```{r}
myHeightCM = 188
```
__Underscore__
```{r}
my_height_cm = 188
```
__Dot separated__
```{r}
my.height.cm = 188
```

---

## Style guides

* [http://4dpiecharts.com/r-code-style-guide/](http://4dpiecharts.com/r-code-style-guide/)
* [http://google-styleguide.googlecode.com/svn/trunk/google-r-style.html](http://google-styleguide.googlecode.com/svn/trunk/google-r-style.html)
* [http://wiki.fhcrc.org/bioc/Coding_Standards](http://wiki.fhcrc.org/bioc/Coding_Standards)

<!--=*=*=*=*-->
<!--./week1/006simulationBasics/index.Rmd
-->
---
title       : Simulation basics
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : zenburn   # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---
```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 70)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache = T, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Important simulation functions

__Distributions__
* rbeta, rbinom, rcauchy, rchisq, rexp, rf, rgamma, rgeom, rhyper, rlogis, rlnorm, rnbinom, rnorm, rpois, rt, runif, rweibull

__Densities__

* dbeta,dbinom, dcauchy, dchisq, dexp, df, dgamma, dgeom, dhyper, dlogis, dlnorm, dnbinom, dnorm, dpois, dt, dunif, dweibull

__Sampling__
* sample(,replace=TRUE),sample(replace=FALSE)


---

## r_foo_ functions generate data 

__Normal__
```{r}
args(rnorm)
heights = rnorm(10,mean=188,sd=3)
heights
```

---

## r_foo_ functions generate data

__Binomial__
```{r}
args(rbinom)
coinFlips = rbinom(10,size=10,prob=0.5)
coinFlips
```


---

## Example distribution: Normal

__Normal Distribution: $N(\mu,\sigma)$__
* $X \sim N(0,1)$

```{r, echo=FALSE, fig.height = 4, fig.width = 4}
xvals = seq(-10,10,length=100)
plot(xvals,dnorm(xvals,0,1),type="l",col="blue",lwd=3,ylab="density",xlab="X value")

```

---

## d_foo_ functions calculate the density
__Normal__
```{r}
args(dnorm)
x = seq(from=-5,to=5,length=10)
normalDensity = dnorm(x,mean=0,sd=1)
round(normalDensity,2)

```

---


## Example distribution: Binomial

__Binomial distribution: $Bin(n,p)$__
* $X \sim Bin(10,0.5)$
```{r, echo=FALSE, fig.height = 4, fig.width = 4}
xvals = 0:10
plot(xvals,dbinom(xvals,10,0.5),type="h",col="blue",lwd=3,ylab="probability",xlab="X value")

```

---
## d_foo_ functions calculate the density
__Binomial__
```{r}
args(dbinom)
x = seq(0,10,by=1)
binomialDensity = dbinom(x,size=10,prob=0.5)
round(binomialDensity,2)
```

---
## Sample draws a random sample
```{r}
args(sample)
heights = rnorm(10,mean=188,sd=3)
heights
sample(heights,size=10,replace=TRUE)

```

---

## Sample draws a random sample

```{r}
heights
sample(heights,size=10,replace=FALSE)

```

---

## Sample can draw according to a set of probabilities
```{r}
heights
probs = c(0.4,0.3,0.2,0.1,0,0,0,0,0,0)
sum(probs)
sample(heights,size=10,replace=TRUE,prob=probs)

```
---

## Setting a seed

Setting a seed ensures reproducible results from random processes in R
```{r}
set.seed(12345)
rnorm(5,mean=0,sd=1)

set.seed(12345)
rnorm(5,mean=0,sd=1)
```


---

## For more information

__More on distributions in R__

[http://cran.r-project.org/web/views/Distributions.html](http://cran.r-project.org/web/views/Distributions.html)

__Computing for Data Analysis__

[Simulation in R](http://www.youtube.com/watch?v=tvv4IA8PEzw&list=PLjTlxb-wKvXOzI2h0F2_rYZHIXz8GWBop&index=6)

<!--=*=*=*=*-->
<!--./week1/007typesOfQuestions/index.Rmd
-->
---
title       : Types of Data Analysis Questions
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : zenburn   # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 70)
opts_chunk$set(message = F, error = F, warning = F, echo = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache = T, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Types of Data Analysis Questions

__In approximate order of difficulty__
* Descriptive
* Exploratory
* Inferential
* Predictive
* Causal
* Mechanistic


---

## About descriptive analyses
__Goal__: Describe a set of data

* The first kind of data analysis performed
* Commonly applied to census data
* The description and interpretation are different steps
* Descriptions can usually not be generalized without additional statistical modeling


---


## Descriptive analysis

<img class=center src=assets/img/census2010.png height='80%'/>

[http://www.census.gov/2010census/](http://www.census.gov/2010census/)

---

## Descriptive analysis

<img class=center src=assets/img/ngrams.png height='80%'/>

[http://books.google.com/ngrams](http://books.google.com/ngrams)

---

## About exploratory analysis

__Goal__: Find relationships you didn't know about

* Exploratory models are good for discovering new connections
* They are also useful for defining future studies
* Exploratory analyses are usually not the final say
* Exploratory analyses alone should not be used for generalizing/predicting
* [Correlation does not imply causation](http://en.wikipedia.org/wiki/Correlation_does_not_imply_causation)

---

## Exploratory analysis

<img class=center src=assets/img/brain.jpg width='90%'/>

[Liu et al. (2012) Scientific Reports](http://www.nature.com/srep/2012/121115/srep00834/full/srep00834.html)


---

## Exploratory analysis


<img class=center src=assets/img/sloan.png height='80%'/>

[http://www.sdss.org/](http://www.sdss.org/)


---

## About inferential analysis

__Goal__: Use a relatively small sample of data to say something about a bigger population

* Inference is commonly the goal of statistical models
* Inference involves estimating both the quantity you care about and your uncertainty about your estimate
* Inference depends heavily on both the population and the sampling scheme

---

## Inferential analysis

<img class=center src=assets/img/pollution.png height='80%'/>

[Correia et al. (2013) Epidemiology](http://journals.lww.com/epidem/Fulltext/2013/01000/Effect_of_Air_Pollution_Control_on_Life_Expectancy.4.aspx)


---

## About predictive analysis

__Goal__: To use the data on some objects to predict values for another object

* If $X$ predicts $Y$ it does not mean that $X$ causes $Y$
* Accurate prediction depends heavily on measuring the right variables
* Although there are better and worse prediction models, more data and a simple model [works really well](http://www.youtube.com/watch?v=yvDCzhbjYWs)
* Prediction is very hard, especially about the future [references](http://www.larry.denenberg.com/predictions.html) 

---

## Predictive analysis

<img class=center src=assets/img/fivethirtyeight.png height='80%'/>

[http://fivethirtyeight.blogs.nytimes.com/](http://fivethirtyeight.blogs.nytimes.com/)

---

## Predictive analysis

<img class=center src=assets/img/target.png height='80%'/>

[http://www.forbes.com/sites/kashmirhill/2012/02/16/how-target-figured-out-a-teen-girl-was-pregnant-before-her-father-did/](http://www.forbes.com/sites/kashmirhill/2012/02/16/how-target-figured-out-a-teen-girl-was-pregnant-before-her-father-did/)

---

## About causal analysis

__Goal__: To find out what happens to one variable when you make another variable change. 

* Usually randomized studies are required to identify causation
* There are approaches to inferring causation in non-randomized studies, but they are complicated and sensitive to assumptions
* Causal relationships are usually identified as average effects, but may not apply to every individual
* Causal models are usually the "gold standard" for data analysis

---

## Causal analysis

<img class=center src=assets/img/feces.png height='80%'/>

[van Nood et al. (2013) NEJM](http://www.nejm.org/doi/full/10.1056/NEJMoa1205037?query=featured_home)

---

## About mechanistic analysis

__Goal__: Understand the exact changes in variables that lead to changes in other variables for individual objects.

* Incredibly hard to infer, except in simple situations
* Usually modeled by a deterministic set of equations (physical/engineering science)
* Generally the random component of the data is measurement error
* If the equations are known but the parameters are not, they may be inferred with data analysis

---

## Mechanistic analysis

<img class=center src=assets/img/mechanistic.png height='80%'/>

[http://www.fhwa.dot.gov/resourcecenter/teams/pavement/pave_3pdg.pdf](http://www.fhwa.dot.gov/resourcecenter/teams/pavement/pave_3pdg.pdf)



<!--=*=*=*=*-->
<!--./week1/008sourcesOfDataSets/index.Rmd
-->
---
title       : Sources of data sets
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : zenburn   # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 70)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache = T, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Data are defined by how they are collected

__Main types__
* Census (descriptive)
* Observational study (inferential)
* Convenience sample (all types - may be biased)
* Randomized trial (causal)

__Other types__

* Prediction study  (prediction)
* Studies over time 
  * Cross sectional (inferential)
  * Longitudinal (inferential, predictive)
* Retrospective (inferential)

---
## A population

<img class=center src=assets/img/where/Slide01.jpg height='80%'/>

---

## Pick a person and measure

<img class=center src=assets/img/where/Slide02.jpg height='80%'/>

---

## Census 

<img class=center src=assets/img/where/Slide03.jpg height='80%'/>

---

## Observational study 
```{r}
set.seed(5)
sample(1:8,size=4,replace=FALSE)

```

<img class=center src=assets/img/where/Slide04.jpg height='60%'/>

---

## Convenience sample 
```{r}
probs = c(5,5,5,5,1,1,1,1)/24
sample(1:8,size=4,replace=FALSE,prob=probs)
```

<img class=center src=assets/img/where/Slide05.jpg height='60%'/>

---
## Randomized trial 
```{r}
treat1 = sample(1:8,size=2,replace=FALSE); treat2 = sample(2:7,size=2,replace=FALSE)
c(treat1,treat2)
```


<img class=center src=assets/img/where/Slide06.jpg height='50%'/>

---
## Prediction study: train 
```{r}
set.seed(5)
sample(1:8,size=4,replace=FALSE)
```


<img class=center src=assets/img/where/Slide04.jpg height='60%'/>

---
## Prediction study: test 
```{r}
sample(c(1,3,4,7),size=2,replace=FALSE)
```

<img class=center src=assets/img/where/Slide08.jpg height='60%'/>

---
## Study over time: cross-sectional 

<img class=center src=assets/img/where/Slide09.jpg height='80%'/>

---
## Study over time: longitudinal 

<img class=center src=assets/img/where/Slide10.jpg height='80%'/>

---
## Study over time: retrospective 
<img class=center src=assets/img/where/Slide11.jpg height='80%'/>



<!--=*=*=*=*-->
<!--./week2/001structureOfADataAnalysis1/index.Rmd
-->
---
title       : Structure of a Data Analysis 
subtitle    : Part 1
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : zenburn   # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache = T, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Steps in a data analysis

* Define the question
* Define the ideal data set
* Determine what data you can access
* Obtain the data
* Clean the data
* Exploratory data analysis
* Statistical prediction/modeling
* Interpret results
* Challenge results
* Synthesize/write up results
* Create reproducible code

---

## Steps in a data analysis

* <redtext>Define the question</redtext>
* <redtext>Define the ideal data set</redtext>
* <redtext>Determine what data you can access</redtext>
* <redtext>Obtain the data</redtext>
* <redtext>Clean the data </redtext>
* Exploratory data analysis
* Statistical prediction/modeling
* Interpret results
* Challenge results
* Synthesize/write up results
* Create reproducible code


--- 

## The key challenge in data analysis

<q>Ask yourselves, what problem have you solved, ever, that was worth solving, where you knew knew all of the given information in advance? Where you didn’t have a surplus of information and have to filter it out, or you didn’t have insufficient information and have to go find some?</q>

<img src=assets/img/meyer.jpg height='30%' /> [Dan Myer, Mathematics Educator](http://www.ted.com/talks/dan_meyer_math_curriculum_makeover.html)



---

## Defining a question

<img class=center src=assets/img/stat-projects.jpg height='65%' />

1. Statistical methods development
2. [Danger zone!!!](http://www.drewconway.com/zia/?p=2378)
3. Proper data analysis 


---

## An example

__Start with a general question__

Can I automatically detect emails that are SPAM that are not?

__Make it concrete__

Can I use quantitative characteristics of the emails to classify them as SPAM/HAM?

---

## Define the ideal data set

* The data set may depend on your goal
  * Descriptive - a whole population
  * Exploratory - a random sample with many variables measured
  * Inferential - the right population, randomly sampled
  * Predictive - a training and test data set from the same population
  * Causal - data from a randomized study
  * Mechanistic - data about all components of the system
  

---

## Our example

<img class=center src=assets/img/datacenter.png height='80%' />
[http://www.google.com/about/datacenters/inside/](http://www.google.com/about/datacenters/inside/)


---

## Determine what data you can access

* Sometimes you can find data free on the web
* Other times you may need to buy the data
* Be sure to respect the terms of use
* If the data don't exist, you may need to generate it yourself


---

## Back to our example

<img class=center src=assets/img/security.png height='80%' />

[Google data center security](http://www.youtube.com/watch?v=1SCZzgfdTBo)


---

## A possible solution


<img class=center src=assets/img/uci.png height='80%' />

[http://archive.ics.uci.edu/ml/datasets/Spambase](http://archive.ics.uci.edu/ml/datasets/Spambase)


---

## Obtain the data

* Try to obtain the raw data
* Be sure to reference the source
* Polite emails go a long way
* If you will load the data from an internet source, record the url and time accessed

---

## Our data set

<img class=center src=assets/img/spamR.png height='80%' />

[http://rss.acs.unt.edu/Rdoc/library/kernlab/html/spam.html](http://rss.acs.unt.edu/Rdoc/library/kernlab/html/spam.html)


---

## Clean the data

* Raw data often needs to be processed
* If it is pre-processed, make sure you understand how
* Understand the source of the data (census, sample, convenience sample, etc.)
* May need reformating, subsampling - record these steps
* __Determine if the data are good enough__ - if not, quit or change data

---

## Our cleaned data set

```{r}
# If it isn't installed, install the kernlab package
library(kernlab)
data(spam)
dim(spam)
```


[http://rss.acs.unt.edu/Rdoc/library/kernlab/html/spam.html](http://rss.acs.unt.edu/Rdoc/library/kernlab/html/spam.html)

---

## Subsampling our data set
We need to generate a test and training set (prediction)
```{r}
set.seed(3435)
trainIndicator = rbinom(4601,size=1,prob=0.5)
table(trainIndicator)
trainSpam = spam[trainIndicator==1,]
testSpam = spam[trainIndicator==0,]
dim(trainSpam)


```


<!--=*=*=*=*-->
<!--./week2/002structureOfADataAnalysis2/index.Rmd
-->
---
title       : Structure of a Data Analysis 
subtitle    : Part 2
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : zenburn   # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = TRUE, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Steps in a data analysis

* Define the question
* Define the ideal data set
* Determine what data you can access
* Obtain the data
* Clean the data
* Exploratory data analysis
* Statistical prediction/modeling
* Interpret results
* Challenge results
* Synthesize/write up results
* Create reproducible code

---

## Steps in a data analysis

* Define the question
* Define the ideal data set
* Determine what data you can access
* Obtain the data
* Clean the data
* <redtext>Exploratory data analysis</redtext>
* <redtext>Statistical prediction/modeling</redtext>
* <redtext>Interpret results</redtext>
* <redtext>Challenge results</redtext>
* <redtext>Synthesize/write up results</redtext>
* <redtext>Create reproducible code</redtext>



---

## An example

__Start with a general question__

Can I automatically detect emails that are SPAM that are not?

__Make it concrete__

Can I use quantitative characteristics of the emails to classify them as SPAM/HAM?


--- 

## Our data set

<img class=center src=assets/img/spamR.png height='80%' />

[http://rss.acs.unt.edu/Rdoc/library/kernlab/html/spam.html](http://rss.acs.unt.edu/Rdoc/library/kernlab/html/spam.html)

--- 

## Subsampling our data set
We need to generate a test and training set (prediction)
```{r}
# If it isn't installed, install the kernlab package
library(kernlab)
data(spam)
# Perform the subsampling
set.seed(3435)
trainIndicator = rbinom(4601,size=1,prob=0.5)
table(trainIndicator)
trainSpam = spam[trainIndicator==1,]
testSpam = spam[trainIndicator==0,]
```

---

## Exploratory data analysis

* Look at summaries of the data
* Check for missing data
* Create exploratory plots
* Perform exploratory analyses (e.g. clustering)

---

## Names
```{r}
names(trainSpam)
```


---

## Head
```{r}
head(trainSpam)
```

---

## Summaries
```{r}
table(trainSpam$type)
```

---

## Plots
```{r,fig.height=5,fig.width=5}
plot(trainSpam$capitalAve ~ trainSpam$type)
```

---

## Plots 
```{r, fig.height=5,fig.width=5}
plot(log10(trainSpam$capitalAve + 1) ~ trainSpam$type)
```

---

## Relationships between predictors
```{r, fig.height=5,fig.width=5}
plot(log10(trainSpam[,1:4]+1))
```

---

## Clustering
```{r,echo=FALSE}
par(mar=c(0,0,0,0))

```

```{r, fig.height=5,fig.width=5}
hCluster = hclust(dist(t(trainSpam[,1:57])))
plot(hCluster)
```

---
## New clustering
```{r, fig.height =6,fig.width=6}
hClusterUpdated = hclust(dist(t(log10(trainSpam[,1:55]+1))))
plot(hClusterUpdated)
```

---
## Statistical prediction/modeling

* Should be informed by the results of your exploratory analysis
* Exact methods depend on the question of interest
* Transformations/processing should be accounted for when necessary
* Measures of uncertainty should be reported

---
## Statistical prediction/modeling
```{r}
trainSpam$numType = as.numeric(trainSpam$type)-1
costFunction = function(x,y){sum(x!=(y > 0.5))}
cvError = rep(NA,55)
library(boot)
for(i in 1:55){
  lmFormula = as.formula(paste("numType~",names(trainSpam)[i],sep=""))
  glmFit = glm(lmFormula,family="binomial",data=trainSpam)
  cvError[i] = cv.glm(trainSpam,glmFit,costFunction,2)$delta[2]
}
which.min(cvError)
names(trainSpam)[which.min(cvError)]

```

---

## Get a measure of uncertainty
```{r}
predictionModel = glm(numType ~ charDollar,family="binomial",data=trainSpam)
predictionTest = predict(predictionModel,testSpam)
predictedSpam = rep("nonspam",dim(testSpam)[1])
predictedSpam[predictionModel$fitted > 0.5] = "spam"
table(predictedSpam,testSpam$type)
(61+458)/(1346+458 + 61 + 449)

```

---

## Interpret results

* Use the appropriate language
  * describes 
  * correlates with/associated with
  * leads to/causes
  * predicts
* Give an explanation
* Interpret coefficients
* Interpret measures of uncertainty

---

## Our example

* The fraction of charcters that are dollar signs can be used to predict if an email is Spam
* Anything with more than 6.6% dollar signs is classified as Spam
* More dollar signs always means more Spam under our prediction
* Our test set error rate was 22.4% 

---

## Challenge results

* Challenge all steps:
  * Question
  * Data source
  * Processing 
  * Analysis 
  * Conclusions
* Challenge measures of uncertainty
* Challenge choices of terms to include in models
* Think of potential alternative analyses 

---

## Synthesize/write-up results

* Lead with the question
* Summarize the analyses into the story 
* Don't include every analysis, include it
  * If it is needed for the story
  * If it is needed to address a challenge
* Order analyses according to the story, rather than chronologically
* Include "pretty" figures that contribute to the story 

---

## In our example

* Lead with the question
  * Can I use quantitative characteristics of the emails to classify them as SPAM/HAM?
* Describe the approach
  * Collected data from UCI -> created training/test sets
  * Explored relationships
  * Choose logistic model on training set by cross validation
  * Applied to test, 78% test set accuracy
* Interpret results
  * Number of dollar signs seems reasonable, e.g. "Make money with Viagra \\$ \\$ \\$ \\$!"
* Challenge results
  * 78% isn't that great
  * I could use more variables
  * Why logistic regression?


---

## Create reproducible code

<img class=center src=assets/img/rmarkdown.png height='80%' />



<!--=*=*=*=*-->
<!--./week2/003organizingADataAnalysis/index.Rmd
-->
---
title       : Organizing a data analysis
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : zenburn   # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---

```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 70)
opts_chunk$set(message = F, error = F, warning = F, echo = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache = T, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```


## Data analysis files

* Data
  * Raw data
  * Processed data
* Figures
  * Exploratory figures
  * Final figures
* R code
  * Raw scripts
  * Final scripts
  * R Markdown files (optional)
* Text
  * Readme files
  * Text of analysis


---

## Raw Data

<img class=center src=assets/img/medicalrecord.png height='50%'/>

* Should be stored in your analysis folder
* If accessed from the web, include url, description, and date accessed in README

---

## Processed data

<img class=center src=assets/img/excel.png height='50%'/>
* Processed data should be named so it is easy to see which script generated the data. 
* The processing script - processed data mapping should occur in the README
* Processed data should be [tidy](http://vita.had.co.nz/papers/tidy-data.pdf)

---

## Exploratory figures

<img class=center src=assets/img/example10.png height='50%'/>

* Figures made during the course of your analysis, not necessarily part of your final report.
* They do not need to be "pretty"

---

## Final Figures

<img class=center src=assets/img/figure1final.png height='50%'/>

* Usually a small subset of the original figures
* Axes/colors set to make the figure clear
* Possibly multiple panels

---

## Raw scripts

<img class=center src=assets/img/raw.png height='60%'/>

* May be less commented (but comments help you!)
* May be multiple versions
* May include analyses that are later discarded

---

## Final scripts

<img class=center src=assets/img/finalscript.png height='50%'/>

* Clearly commented
  * Small comments liberally - what, when, why, how
  * Bigger commented blocks for whole sections
* Include processing details
* Only analyses that appear in the final write-up

---

## R markdown files

<img class=center src=assets/img/rmd.png height='70%'/>

* [R markdown](http://www.rstudio.com/ide/docs/authoring/using_markdown) files can be used to generate reproducible reports
* Text and R code are integrated
* Very easy to create in [Rstudio](http://www.rstudio.com/)

---

## Readme files

<img class=center src=assets/img/readme.png height='70%'/>

* Not necessary if you use R markdown
* Should contain step-by-step instructions for analysis
* Here is an example [https://github.com/jtleek/swfdr/blob/master/README.md](https://github.com/jtleek/swfdr/blob/master/README.md)

---

## Text of the document

<img class=center src=assets/img/swfdr.png height='50%'/>

* It should include a title, introduction (motivation), methods (statistics you used), results (including measures of uncertainty), and conclusions (including potential problems)
* It should tell a story
* _It should not include every analysis you performed_
* References should be included for statistical methods

---

## Further resources

* Information about a non-reproducible study that led to cancer patients being mistreated: [The Duke Saga Starter Set](http://simplystatistics.org/2012/02/27/the-duke-saga-starter-set/)
* [Reproducible research and Biostatistics](http://biostatistics.oxfordjournals.org/content/10/3/405.full)
* [Managing a statistical analysis project guidelines and best practices](http://www.r-statistics.com/2010/09/managing-a-statistical-analysis-project-guidelines-and-best-practices/)
* [Project template](http://projecttemplate.net/) - a pre-organized set of files for data analysis


<!--=*=*=*=*-->
<!--./week2/004gettingData1/index.Rmd
-->
---
title       : Getting Data (Part 1)
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : zenburn   # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---

```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 70)
opts_chunk$set(message = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache=F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```


## Get/set your working directory

Roger's lectures [windows](http://www.youtube.com/watch?v=XBcvH1BpIBo&list=PLjTlxb-wKvXNSDfcKPFH2gzHGyjpeCZmJ&index=2), [mac](http://www.youtube.com/watch?v=8xT3hmJQskU&list=PLjTlxb-wKvXNSDfcKPFH2gzHGyjpeCZmJ&index=1)
Andrew Jaffe's [lecture notes](https://dl.dropbox.com/u/7710864/courseraPublic/otherResources/lecture1/index.html)

```{r}
getwd()
setwd("/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week2/004gettingData1/data")
getwd()
```

Important difference with Windows: 

```{r, eval=FALSE}
 setwd("C:\\Users\\Andrew\\Downloads")
```


---

## Get/set your working directory (relative paths)

```{r}
getwd()
setwd("./data")
getwd()
setwd("../")
getwd()
```


---

## Get/set your working directory (absolute paths)

```{r}
getwd()
setwd("/Users/jtleek/Dropbox/Jeff/teaching/2013/coursera/week2/004gettingData1/data")
getwd()
```


---

## Types of files data may come from

* Tab-delimited text
* Comma-separated text
* Excel file
* JSON File 
* HTML/XML file
* Database


---

## Where you can get data

* From a colleague
* From the web
* From an application programming interface
* By scraping a web page 


---

## Getting data from the internet - download.file()

* Downloads a file from the internet
* Even if you could do this by hand, helps with reproducibility
* Important parameters are _url_, _destfile_, _method_
* Useful for downloading tab-delimited, csv, etc. 

---

## Example - Baltimore camera data

<img class=center src=assets/img/cameras.png height='80%'/>

[https://data.baltimorecity.gov/Transportation/Baltimore-Fixed-Speed-Cameras/dz54-2aru](https://data.baltimorecity.gov/Transportation/Baltimore-Fixed-Speed-Cameras/dz54-2aru)


---

## Example - Baltimore camera data,csv

<img class=center src=assets/img/cameraslink.png height='80%'/>

[https://data.baltimorecity.gov/Transportation/Baltimore-Fixed-Speed-Cameras/dz54-2aru](https://data.baltimorecity.gov/Transportation/Baltimore-Fixed-Speed-Cameras/dz54-2aru)


---

## Download a file from the web


```{r}
fileUrl <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType=DOWNLOAD"
download.file(fileUrl,destfile="./data/cameras.csv",method="curl")
list.files("./data")
dateDownloaded <- date()
dateDownloaded
```


---

## Some notes about download.file()

* If the url starts with _http_ you can use download.file()
* If the url starts with _https_ on Windows you may be ok
* If the url starts with _https_ on Mac you may need to set _method="curl"_
* If the file is big, this might take a while
* Be sure to record when you downloaded. 

---

## Loading data you have saved - read.table()

* This is the main function for reading data into R
* Flexible and robust but requires more parameters
* Reads the data into RAM - big data can cause problems
* Important parameters _file_, _header_, _sep_, _row.names_, _nrows_
* Related: _read.csv()_, _read.csv2()_

---

## Example: Baltimore camera data

```{r}
getwd()
cameraData <- read.table("./data/cameras.csv")
head(cameraData)

```


---

## Example: Baltimore camera data

```{r}
getwd()
cameraData <- read.table("./data/cameras.csv",sep=",",header=TRUE)
head(cameraData)

```


---

## Example: Baltimore camera data

read.csv sets _sep=","_ and _header=TRUE_ 
```{r}
cameraData <- read.csv("./data/cameras.csv")
head(cameraData)
```



---

## read.xlsx(), read.xlsx2() {xlsx package}


* Reads .xlsx files, but slow
* Important parameters _file_, _sheetIndex_, _sheetIndex_, _rowIndex_, _colIndex_, _header_
* read.xlsx2() relies more on low level Java functions so may be a bit faster


---

## read.xlsx() - Baltimore camera data

```{r}
library(xlsx)
fileUrl <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.xlsx?accessType=DOWNLOAD"
download.file(fileUrl,destfile="./data/camera.xlsx",method="curl")
cameraData <- read.xlsx2("./data/camera.xlsx",sheetIndex=1)
head(cameraData)
```


---

## Picking a file - less reproducible, but useful

```{r,eval=FALSE}
cameraData <- read.csv(file.choose())
```

<img class=center src=assets/img/filechoose.png height='80%'/>


<!--=*=*=*=*-->
<!--./week2/005gettingData2/index.Rmd
-->
---
title       : Getting Data (Part 2)
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : zenburn   # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---

```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 70)
opts_chunk$set(message = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache=F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```


## Interacting more directly with files

* file - open a connection to a text file
* url - open a connection to a url
* gzfile - open a connection to a .gz file
* bzfile - open a connection to a .bz2 file
* _?connections_ for more information
* <redtext>Remember to close connections </redtext>


---

## readLines() - local file

* readLines - a function to read lines of text from a connection
* Important parameters: _con_, _n_, _encoding_

```{r cachedChunk, echo=FALSE}
fileUrl <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType=DOWNLOAD"
download.file(fileUrl,destfile="./data/cameras.csv",method="curl")
```


```{r, dependson="cachedChunk"}
con <- file("./data/cameras.csv","r")
cameraData <- read.csv(con)
close(con)
head(cameraData)
```


---

## readLines() - from the web


```{r}

con <- url("http://simplystatistics.org","r")
simplyStats <- readLines(con)
close(con)
head(simplyStats)

```

---

## Reading JSON files {RJSONIO}


```{r}
library(RJSONIO)
fileUrl <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.json?accessType=DOWNLOAD"
download.file(fileUrl,destfile="./data/camera.json",method="curl")
con = file("./data/camera.json")
jsonCamera = fromJSON(con)
close(con)
head(jsonCamera)
```


---

## Writing data - write.table()

* The opposite of read.table
* Important parameters: _x_, _file_, _quote_, _sep_, _row.names_, _col.names_

```{r}
cameraData <- read.csv("./data/cameras.csv")
tmpData <- cameraData[,-1]
write.table(tmpData,file="./data/camerasModified.csv",sep=",")
cameraData2 <- read.csv("./data/camerasModified.csv")
head(cameraData2)
```


--- 

## Writing data - save(), save.image()


* save is used to save R objects
* Important parameters: _list of objects_, _file_
* save.image saves everything in your working directory

```{r}
cameraData <- read.csv("./data/cameras.csv")
tmpData <- cameraData[,-1]
save(tmpData,cameraData,file="./data/cameras.rda")

```


---

## Reading saved data - load()

* Opposite of save()
* Important parameters: _file_

```{r}
# Remove everything from the workspace
rm(list=ls())
ls()

# Load data 
load("./data/cameras.rda")
ls()

```


---

## paste() and paste0()

* These functions are for pasting character strings together. 
* Important parameters: _list of text strings_, _sep_
* paste0() is the same as paste but with _sep=""_
* Great for looping over files
* See also [file.path](http://stat.ethz.ch/R-manual/R-patched/library/base/html/file.path.html)

```{r}
for(i in 1:5){
  fileName = paste0("./data",i,".csv")
  print(fileName)
}
```




---

## Getting data off webpages 

<img class=center src=assets/img/googlescholar.png height='80%'/>

[http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en](http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en)

---

## Getting data off webpages


```{r}
library(XML)
con = url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
htmlCode = readLines(con)
close(con)
htmlCode
```



---

## Getting data off webpages

```{r}
html3 <- htmlTreeParse("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en", useInternalNodes=T)
xpathSApply(html3, "//title", xmlValue)
xpathSApply(html3, "//td[@id='col-citedby']", xmlValue)
```





---

## Further resources

* Packages:
  * [httr](http://cran.r-project.org/web/packages/httr/index.html) - for working with http connections 
  * [RMySQL](http://cran.r-project.org/web/packages/RMySQL/index.html) - for interfacing with mySQL
  * [bigmemory](http://www.bigmemory.org/) - for handling data larger than RAM
  * [RHadoop](https://github.com/RevolutionAnalytics/RHadoop/wiki) - for interfacing R and Hadoop (by [Revolution Analytics](http://www.revolutionanalytics.com/))
  * [foreign](http://cran.r-project.org/web/packages/foreign/index.html) - for getting data into R from SAS, SPSS, Octave, etc. 
  
* Reading/writing R videos [Part 1](http://www.youtube.com/watch?v=aBzAels6jPk&list=PLjTlxb-wKvXNSDfcKPFH2gzHGyjpeCZmJ&index=8), [Part 2](http://www.youtube.com/watch?v=cUUqDWttMws&list=PLjTlxb-wKvXNSDfcKPFH2gzHGyjpeCZmJ&index=9)













<!--=*=*=*=*-->
<!--./week2/006dataResources/index.Rmd
-->
---
title       : Data Resources
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : zenburn   # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---

```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 70)
opts_chunk$set(message = F, error = F, warning = F, echo = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache = T, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```


## Open Government Data (U.S.)


<img class=center src=assets/img/datagov.png height='80%'/>

[http://www.data.gov/](http://www.data.gov/)

---

## Open Government Data (France)

<img class=center src=assets/img/francedata.png height='80%'/>

[http://www.data.gouv.fr/](http://www.data.gouv.fr/)

---

## Open Government Data (UK)

<img class=center src=assets/img/ukdata.png height='80%'/>

[http://data.gov.uk/](http://data.gov.uk/)

---

## Gapminder

<img class=center src=assets/img/gapminder.png height='80%'/>

[http://www.gapminder.org/](http://www.gapminder.org/)

---

## More open government data (possibly overlapping)

* [http://opengovernmentdata.org/data/catalogues/](http://opengovernmentdata.org/data/catalogues/)
* [http://wiki.civiccommons.org/Initiatives](http://wiki.civiccommons.org/Initiatives)
* [List of cities/states with open data](http://simplystatistics.org/2012/01/02/list-of-cities-states-with-open-data-help-me-find/)


---

## Survey data from the United States

<img class=center src=assets/img/asdfree.png height='80%'/>

[http://www.asdfree.com/](http://www.asdfree.com/)

---

## Infochimps Marketplace

<img class=center src=assets/img/infochimps.png height='80%'/>

[http://www.infochimps.com/marketplace](http://www.infochimps.com/marketplace)

---

## Kaggle

<img class=center src=assets/img/kaggle.png height='80%'/>

[http://www.kaggle.com/](http://www.kaggle.com/)


---

## More specialized collections

* [Hilary Mason's research data](http://bitly.com/bundles/hmason/1)
* [Stanford Large Newtork Data](http://snap.stanford.edu/data/)
* [UCI Machine Learning](http://archive.ics.uci.edu/ml/)
* [KDD Nugets Datasets](http://www.kdnuggets.com/datasets/index.html)
* [CMU Statlib](http://lib.stat.cmu.edu/datasets/)
* [Gene expression omnibus](http://www.ncbi.nlm.nih.gov/geo/)
* [ArXiv Data](http://arxiv.org/help/bulk_data)

---

## Some API's

* [twitter](https://dev.twitter.com/) and [twitteR](http://cran.r-project.org/web/packages/twitteR/index.html) package
* [figshare](http://api.figshare.com/docs/intro.html) and [rfigshare](http://cran.r-project.org/web/packages/rfigshare/index.html)
* [PLoS](http://api.plos.org/) and [rplos](http://cran.r-project.org/web/packages/rplos/rplos.pdf)
* [rOpenSci](http://ropensci.org/packages/index.html)


<!--=*=*=*=*-->
<!--./week2/007summarizingData/index.Rmd
-->
---
title       : Summarizing data
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : zenburn   # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---

```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 70)
opts_chunk$set(message = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache = T, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Why summarize?

* Data are often too big to look at the whole thing
* The first step in an analysis is to find problems
* When you do these summaries you should be looking for
  * Missing values
  * Values outside of expected ranges
  * Values that seem to be in the wrong units
  * Mislabled variables/columns 
  * Variables that are the wrong class

---

## Earthquake data

<img class=center src=assets/img/earthquakes.png height='80%'/>

[https://explore.data.gov/Geography-and-Environment/Worldwide-M1-Earthquakes-Past-7-Days/7tag-iwnu](https://explore.data.gov/Geography-and-Environment/Worldwide-M1-Earthquakes-Past-7-Days/7tag-iwnu)


---

## Earthquake data

```{r cachedChunk,cache=TRUE}
fileUrl <- "http://earthquake.usgs.gov/earthquakes/catalogs/eqs7day-M1.txt"
download.file(fileUrl,destfile="./data/earthquakeData.csv",method="curl")
dateDownloaded <- date()
dateDownloaded
eData <- read.csv("./data/earthquakeData.csv")
```

---

## Looking at data - the whole thing
```{r,dependson="cachedChunk"}
eData
```

---

## Looking at data - dim(),names(),nrow(),ncol()
```{r, dependson="cachedChunk"}
dim(eData)
names(eData)
nrow(eData)
```

---

## Looking at the data - quantile(),summary()

```{r, dependson="cachedChunk"}
quantile(eData$Lat)
summary(eData)
```



---

## Looking at data - class()

```{r, dependson="cachedChunk"}
class(eData)
sapply(eData[1,],class)
```

---

## Looking at data - unique(),length(),table()

```{r, dependson="cachedChunk"}
unique(eData$Src)
length(unique(eData$Src))
table(eData$Src)
```

---

## Looking at data - table()

```{r, dependson="cachedChunk"}
table(eData$Src,eData$Version)
```


---

## Looking at data - any(), all()
```{r, dependson="cachedChunk"}
eData$Lat[1:10]
eData$Lat[1:10] > 40
any(eData$Lat[1:10] > 40)
```

---

## Looking at data - all()
```{r, dependson="cachedChunk"}

eData$Lat[1:10] > 40
all(eData$Lat[1:10] > 40)
```



---

## Looking at subsets - &

```{r, dependson="cachedChunk"}
eData[eData$Lat > 0 & eData$Lon > 0,c("Lat","Lon")]

```

---

## Looking at subsets - |

```{r, dependson="cachedChunk"}
eData[eData$Lat > 0 | eData$Lon > 0,c("Lat","Lon")]

```


---

## Peer review experiment data

* Data on submissions/reviews in an experiment 

<img class=center src=assets/img/cooperation.png height='60%'/>

[http://www.plosone.org/article/info:doi/10.1371/journal.pone.0026895](http://www.plosone.org/article/info:doi/10.1371/journal.pone.0026895)


---

## Peer review data


```{r reviewDownload, cache=TRUE}
fileUrl1 <- "https://dl.dropbox.com/u/7710864/data/reviews-apr29.csv"
fileUrl2 <- "https://dl.dropbox.com/u/7710864/data/solutions-apr29.csv"
download.file(fileUrl1,destfile="./data/reviews.csv",method="curl")
download.file(fileUrl2,destfile="./data/solutions.csv",method="curl")
reviews <- read.csv("./data/reviews.csv"); solutions <- read.csv("./data/solutions.csv")
head(reviews,2)
head(solutions,2)
```

---

## Find if there are missing values - is.na()

```{r, dependson="reviewDownload"}
is.na(reviews$time_left[1:10])
sum(is.na(reviews$time_left))
table(is.na(reviews$time_left))
```

---

## Important table()/NA issue

```{r}
table(c(0,1,2,3,NA,3,3,2,2,3))
table(c(0,1,2,3,NA,3,3,2,2,3),useNA="ifany")
```

---

## Summarizing columns/rows - rowSums(),rowMeans(),colSums(),colMeans()

* Important parameters: _x_, _na.rm_
```{r, dependson="reviewDownload"}
colSums(reviews)
```

---

## Summarizing columns/rows - rowSums(),rowMeans(),colSums(),colMeans()

```{r, dependson="reviewDownload"}
colMeans(reviews,na.rm=TRUE)
rowMeans(reviews,na.rm=TRUE)
```





<!--=*=*=*=*-->
<!--./week2/008dataMungingBasics/index.Rmd
-->
---
title       : Data munging basics
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : zenburn   # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---

```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 70)
opts_chunk$set(message = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache = T, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```


## Recall Tidy Data

<img class=center src=assets/img/excel.png height='50%'/>

1. Each variable forms a column
2. Each observation forms a row
3. Each table/file stores data about one kind of observation (e.g. people/hospitals).


[http://vita.had.co.nz/papers/tidy-data.pdf](http://vita.had.co.nz/papers/tidy-data.pdf)

[Leek, Taub, and Pineda 2011 PLoS One](http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0026895)

---

## Where we would like to be

* [Tidy data]([http://vita.had.co.nz/papers/tidy-data.pdf](http://vita.had.co.nz/papers/tidy-data.pdf) refers to the shape of the data
  * Variables in columns
  * Observations in rows
  * Tables holding elements of only one kind
* Plus
  * Column names are easy to use and informative
  * Row names are easy to use and informative
  * Obvious mistakes in the data have been removed
  * Variable values are internally consistent
  * Appropriate transformed variables have been added


---

## A partial list of munging operations

* Fix variable names
* Create new variables 
* Merge data sets
* Reshape data sets
* Deal with missing data 
* Take transforms of variables
* Check on and remove inconsistent values

__These steps must be recorded__

__90% of your effort will often be spent here__

---

## A partial list of munging operations

* <redtext> Fix variable names </redtext>
* <redtext> Create new variables </redtext>
* <redtext> Merge data sets</redtext>
* <redtext> Reshape data sets</redtext>
* Deal with missing data 
* Take transforms of variables
* Check on and remove inconsistent values


---

## Fixing character vectors - tolower(), toupper()

```{r cachedChunk,echo=FALSE}
fileUrl <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType=DOWNLOAD"
download.file(fileUrl,destfile="./data/cameras.csv",method="curl")
```

```{r,dependson="cachedChunk"}
cameraData <- read.csv("./data/cameras.csv")
names(cameraData)
tolower(names(cameraData))
```


---

## Fixing character vectors - strsplit()

* Good for automatically splitting variable names
* Important parameters: _x_, _split_

```{r splitNames,dependson="cachedChunk"}
splitNames = strsplit(names(cameraData),"\\.")
splitNames[[5]]
splitNames[[6]]
```
---

## Fixing character vectors - sapply()

* Applies a function to each element in a vector or list
* Important parameters: _X_,_FUN_

```{r,dependson="splitNames"}
splitNames[[6]][1]
firstElement <- function(x){x[1]}
sapply(splitNames,firstElement)
```

---

## Peer review experiment data

* Data on submissions/reviews in an experiment 

<img class=center src=assets/img/cooperation.png height='60%'/>

[http://www.plosone.org/article/info:doi/10.1371/journal.pone.0026895](http://www.plosone.org/article/info:doi/10.1371/journal.pone.0026895)


---

## Peer review data


```{r reviewDownload, cache=TRUE}
fileUrl1 <- "https://dl.dropbox.com/u/7710864/data/reviews-apr29.csv"
fileUrl2 <- "https://dl.dropbox.com/u/7710864/data/solutions-apr29.csv"
download.file(fileUrl1,destfile="./data/reviews.csv",method="curl")
download.file(fileUrl2,destfile="./data/solutions.csv",method="curl")
reviews <- read.csv("./data/reviews.csv"); solutions <- read.csv("./data/solutions.csv")
head(reviews,2)
head(solutions,2)
```

---

## Fixing character vectors - sub(),gsub()

* Important parameters: _pattern_, _replacement_, _x_

```{r, dependson="reviewDownload"}
names(reviews)
sub("_","",names(reviews),)

```

---

## Fixing character vectors - sub(),gsub()

```{r, dependson="reviewDownload"}
testName <- "this_is_a_test"
sub("_","",testName)
gsub("_","",testName)
```


---

## Quantitative variables in ranges -  - cut()

* Important parameters: _x_,_breaks_

```{r, dependson="reviewDownload"}
reviews$time_left[1:10]
timeRanges <- cut(reviews$time_left,seq(0,3600,by=600))
timeRanges[1:10]
```

---

## Quantitative variables in ranges -  - cut()

```{r, dependson="reviewDownload"}
class(timeRanges)
table(timeRanges,useNA="ifany")
```

---

## Quantitative variables in ranges - cut2() {Hmisc}

```{r, dependson="reviewDownload"}
library(Hmisc)
timeRanges<- cut2(reviews$time_left,g=6)
table(timeRanges,useNA="ifany")
```


---

## Adding an extra variable 

```{r, dependson="reviewDownload"}
timeRanges<- cut2(reviews$time_left,g=6)
reviews$timeRanges <- timeRanges
head(reviews,2)
```

---

## Merging data - merge()

* Merges data frames
* Important parameters: _x_,_y_,_by_,_by.x_,_by.y_,_all_
```{r, dependson="reviewDownload"}
names(reviews)
names(solutions)
```

---

## Merging data - merge()

```{r, dependson="reviewDownload"}
mergedData <- merge(reviews,solutions,all=TRUE)
head(mergedData)
```

---

## Merging data - merge()

```{r mergedData, dependson="reviewDownload"}
mergedData2 <- merge(reviews,solutions,by.x="solution_id",by.y="id",all=TRUE)
head(mergedData2[,1:6],3)
reviews[1,1:6]
```


---

## Sorting values - sort()

* Important parameters: _x_, _decreasing_

```{r, dependson="mergedData"}
mergedData2$reviewer_id[1:10]
sort(mergedData2$reviewer_id)[1:10]
```

---

## Ordering values - order()

* Important parameters: _list of variables to order_, _na.last_,_decreasing_

```{r, dependson="mergedData"}
mergedData2$reviewer_id[1:10]
order(mergedData2$reviewer_id)[1:10]
mergedData2$reviewer_id[order(mergedData2$reviewer_id)]
```

---

## Reordering a data frame

```{r, dependson="mergedData"}
head(mergedData2[,1:6],3)
sortedData <- mergedData2[order(mergedData2$reviewer_id),]
head(sortedData[,1:6],3)
```

---

## Reordering by multiple variables

```{r, dependson="mergedData"}
head(mergedData2[,1:6],3)
sortedData <- mergedData2[order(mergedData2$reviewer_id,mergedData2$id),]
head(sortedData[,1:6],3)
```


---

## Reshaping data - example

```{r hadleyExample,cache=TRUE}
misShaped <- as.data.frame(matrix(c(NA,5,1,4,2,3),byrow=TRUE,nrow=3))
names(misShaped) <- c("treatmentA","treatmentB")
misShaped$people <- c("John","Jane","Mary")
misShaped
```

[http://vita.had.co.nz/papers/tidy-data.pdf](http://vita.had.co.nz/papers/tidy-data.pdf)


---

## Reshaping data - melt() {reshape2}

* Important parameters: _id.vars_, _measure.vars_, _variable.name_

```{r,dependson="hadleyExample"}
melt(misShaped,id.vars="people",variable.name="treatment",value.name="value")
```


---

## More resources

* [Tidy data and tidy tools](http://vita.had.co.nz/papers/tidy-data-pres.pdf)
* Andrew Jaffe's [Data Cleaning Lecture](https://dl.dropbox.com/u/7710864/courseraPublic/otherResources/lecture3/index.html)
* Hadley Wickham on [regular expressions](https://dl.dropbox.com/u/7710864/courseraPublic/otherResources/14-reg-exp.pdf)
* Long, painful experience :-) 

<!--=*=*=*=*-->
<!--./week3/001exploratoryGraphs1/index.Rmd
-->
---
title       : Exploratory graphs
subtitle    : Part 1
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow  # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache = T, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Why do we use graphs in data analysis? 

* To understand data properties
* To find patterns in data
* To suggest modeling strategies
* To "debug" analyses
* To communicate results

---

## Exploratory graphs

* <redtext>To understand data properties</redtext>
* <redtext>To find patterns in data</redtext>
* <redtext>To suggest modeling strategies</redtext>
* <redtext>To "debug" analyses</redtext>
* To communicate results

---

## Characteristics of exploratory graphs

* They are made quickly
* A large number are made
* The goal is for personal understanding
* Axes/legends are generally cleaned up
* Color/size are primarily used for information

---

## Background - perceptual tasks

<img class=center src=assets/img/perceptual.png height='80%'/>

[Graphical perception: Theory, Experimentation, and Applications to the Development of Graphical Models](http://www.jstor.org/discover/10.2307/2288400?uid=3739704&uid=2&uid=4&uid=3739256&sid=21101742782357)

---

## Position versus length

<img class=center src=assets/img/barcharts.png height='50%'/>

[Graphical perception: Theory, Experimentation, and Applications to the Development of Graphical Models](http://www.jstor.org/discover/10.2307/2288400?uid=3739704&uid=2&uid=4&uid=3739256&sid=21101742782357)


---

## Position versus length - results

<img class=center src=assets/img/poslength.png height='50%'/>


[Graphical perception: Theory, Experimentation, and Applications to the Development of Graphical Models](http://www.jstor.org/discover/10.2307/2288400?uid=3739704&uid=2&uid=4&uid=3739256&sid=21101742782357)

---

## Position versus angle

<img class=center src=assets/img/piechart.png height='50%'/>


[Graphical perception: Theory, Experimentation, and Applications to the Development of Graphical Models](http://www.jstor.org/discover/10.2307/2288400?uid=3739704&uid=2&uid=4&uid=3739256&sid=21101742782357)


---

## Position versus angle - results


<img class=center src=assets/img/posangle.png height='40%'/>

<br><br>
[Graphical perception: Theory, Experimentation, and Applications to the Development of Graphical Models](http://www.jstor.org/discover/10.2307/2288400?uid=3739704&uid=2&uid=4&uid=3739256&sid=21101742782357)


---

## More experimental results

<img class=center src=assets/img/bigexp.png height='80%'/>

[Graphical Perception and Graphical Methods for Analyzing Scientific Data](http://www.sciencemag.org/content/229/4716/828.refs)

---

## Summary

* Use common scales when possible
* When possible use position comparisons
* Angle comparisons are frequently hard to interpret (no piecharts!)
* No 3-D barcharts


---

## Housing data


<img class=center src=assets/img/acs.png height='70%'/>

<br>

```{r downloadData, echo=FALSE, cache=TRUE}
download.file("https://dl.dropbox.com/u/7710864/data/csv_hid/ss06pid.csv",destfile="./data/ss06pid.csv",method="curl")
```


```{r loadData,dependson="downloadData"}
pData <- read.csv("./data/ss06pid.csv")
```


--- 

## Boxplots

* Important parameters: _col_,_varwidth_,_names_,_horizontal_
```{r , dependson="loadData",fig.height=4,fig.width=4}
boxplot(pData$AGEP,col="blue")
```

--- 

## Boxplots

```{r , dependson="loadData",fig.height=5,fig.width=5}
boxplot(pData$AGEP ~ as.factor(pData$DDRS),col="blue")
```


--- 

## Boxplots

```{r , dependson="loadData",fig.height=5,fig.width=5}
boxplot(pData$AGEP ~ as.factor(pData$DDRS),col=c("blue","orange"),names=c("yes","no"),varwidth=TRUE)
```



--- 

## Barplots

```{r , dependson="loadData",fig.height=5,fig.width=5}
barplot(table(pData$CIT),col="blue")
```



--- 

## Histograms

* Important parameters: _breaks_,_freq_,_col_,_xlab_,_ylab_, _xlim, _ylim_ ,_main_
```{r , dependson="loadData",fig.height=4,fig.width=4}
hist(pData$AGEP,col="blue")
```

--- 

## Histograms

```{r , dependson="loadData",fig.height=5,fig.width=5}
hist(pData$AGEP,col="blue",breaks=100,main="Age")
```


--- 

## Density plots

Important parameters (to plot): _col_,_lwd_,_xlab_,_ylab_,_xlim_,_ylim_
```{r , dependson="loadData",fig.height=4,fig.width=4}
dens <- density(pData$AGEP)
plot(dens,lwd=3,col="blue")
```


--- 

## Density plots - multiple distributions

```{r , dependson="loadData",fig.height=4,fig.width=4}
dens <- density(pData$AGEP)
densMales <- density(pData$AGEP[which(pData$SEX==1)])
plot(dens,lwd=3,col="blue")
lines(densMales,lwd=3,col="orange")
```




<!--=*=*=*=*-->
<!--./week3/002exploratoryGraphs2/index.Rmd
-->
---
title       : Exploratory graphs
subtitle    : Part 2
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow   # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache = T, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Why do we use graphs in data analysis? 

* To understand data properties
* To find patterns in data
* To suggest modeling strategies
* To "debug" analyses
* To communicate results

---

## Exploratory graphs

* <redtext>To understand data properties</redtext>
* <redtext>To find patterns in data</redtext>
* <redtext>To suggest modeling strategies</redtext>
* <redtext>To "debug" analyses</redtext>
* To communicate results

---

## Characteristics of exploratory graphs

* They are made quickly
* A large number are made
* The goal is for personal understanding
* Axes/legends are generally not cleaned up
* Color/size are primarily used for information

---

## Housing data


<img class=center src=assets/img/acs.png height='70%'/>

<br>

```{r downloadData, echo=FALSE, cache=TRUE}
download.file("https://dl.dropbox.com/u/7710864/data/csv_hid/ss06pid.csv",destfile="./data/ss06pid.csv",method="curl")
```


```{r loadData,dependson="downloadData"}
pData <- read.csv("./data/ss06pid.csv")
```


--- 

## Scatterplots

* Important paramters: _x_,_y_,_type_,_xlab_,_ylab_,_xlim_,_ylim_,_cex_,_col_,_bg_
* See ?par for more

```{r , dependson="loadData",fig.height=4,fig.width=4}
plot(pData$JWMNP,pData$WAGP,pch=19,col="blue")
```


--- 

## Scatterplots - size matters


```{r , dependson="loadData",fig.height=5,fig.width=5}
plot(pData$JWMNP,pData$WAGP,pch=19,col="blue",cex=0.5)
```


--- 

## Scatterplots - using color

```{r , dependson="loadData",fig.height=5,fig.width=5}
plot(pData$JWMNP,pData$WAGP,pch=19,col=pData$SEX,cex=0.5)
```

--- 

## Scatterplots - using size

```{r , dependson="loadData",fig.height=5,fig.width=5}
percentMaxAge <- pData$AGEP/max(pData$AGEP)
plot(pData$JWMNP,pData$WAGP,pch=19,col="blue",cex=percentMaxAge*0.5)
```


--- 

## Scatterplots - overlaying lines/points

```{r , dependson="loadData",fig.height=4,fig.width=4}
plot(pData$JWMNP,pData$WAGP,pch=19,col="blue",cex=0.5)
lines(rep(100,dim(pData)[1]),pData$WAGP,col="grey",lwd=5)
points(seq(0,200,length=100),seq(0,20e5,length=100),col="red",pch=19)
```



--- 

## Scatterplots - numeric variables as factors

```{r , dependson="loadData",fig.height=5,fig.width=5}
library(Hmisc)
ageGroups <- cut2(pData$AGEP,g=5)
plot(pData$JWMNP,pData$WAGP,pch=19,col=ageGroups,cex=0.5)
```

---

## If you have a lot of points

```{r,fig.height=4,fig.width=4}
x <- rnorm(1e5)
y <- rnorm(1e5)
plot(x,y,pch=19)
```


---

## If you have a lot of points - sampling

```{r,fig.height=4,fig.width=4}
x <- rnorm(1e5)
y <- rnorm(1e5)
sampledValues <- sample(1:1e5,size=1000,replace=FALSE)
plot(x[sampledValues],y[sampledValues],pch=19)
```


---

## If you have a lot of points - smoothScatter

```{r,fig.height=4,fig.width=4}
x <- rnorm(1e5)
y <- rnorm(1e5)
smoothScatter(x,y)
```



---

## If you have a lot of points - hexbin {hexbin}

```{r,fig.height=4,fig.width=4}
library(hexbin)
x <- rnorm(1e5)
y <- rnorm(1e5)
hbo <- hexbin(x,y)
plot(hbo)
```




---

## QQ-plots

* Important parameters: _x_,_y_

```{r ,fig.height=4,fig.width=4}
x <- rnorm(20); y <- rnorm(20)
qqplot(x,y)
abline(c(0,1))
```


--- 

## Matplot and spaghetti

* Important paramters: _x_, _y_, _lty_,_lwd_,_pch_,_col_

```{r ,fig.height=4,fig.width=4}
X <- matrix(rnorm(20*5),nrow=20)
matplot(X,type="b")
```


--- 

## Heatmaps

* Important paramters: _x_,_y_,_z_,_col_
```{r , dependson="loadData",fig.height=4,fig.width=4}
image(1:10,161:236,as.matrix(pData[1:10,161:236]))
```

---

## Heatmaps - matching intuition

```{r , dependson="loadData",fig.height=4,fig.width=4}
newMatrix <- as.matrix(pData[1:10,161:236])
newMatrix <- t(newMatrix)[,nrow(newMatrix):1]
image(161:236, 1:10, newMatrix)
```


---

## Maps - very basics 

```{r,fig.height=5,fig.width=5}
library(maps)
map("world")
lat <- runif(40,-180,180); lon <- runif(40,-90,90)
points(lat,lon,col="blue",pch=19)

```


---

## Missing values and plots

```{r,fig.height=4,fig.width=4}
x <- c(NA,NA,NA,4,5,6,7,8,9,10)
y <- 1:10
plot(x,y,pch=19,xlim=c(0,11),ylim=c(0,11))
```


---

## Missing values and plots

```{r,fig.height=4,fig.width=4}
x <- rnorm(100)
y <- rnorm(100)
y[x < 0] <- NA
boxplot(x ~ is.na(y))
```


---

## Further resources

* [R Graph Gallery](http://gallery.r-enthusiasts.com/)
* [ggplot2](http://cran.r-project.org/web/packages/ggplot2/index.html),[ggplot2 basic introduction](http://www.r-bloggers.com/basic-introduction-to-ggplot2/)
* [lattice package](http://cran.r-project.org/web/packages/lattice/index.html),[lattice introduction](http://lmdvr.r-forge.r-project.org/figures/figures.html)
* [R bloggers](http://www.r-bloggers.com/)











<!--=*=*=*=*-->
<!--./week3/003expositoryGraphs/index.Rmd
-->
---
title       : Expository graphs
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow  # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache = T, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Why do we use graphs in data analysis? 

* To understand data properties
* To find patterns in data
* To suggest modeling strategies
* To "debug" analyses
* To communicate results

---

## Expository graphs

* To understand data properties
* To find patterns in data
* To suggest modeling strategies
* To "debug" analyses
* <redtext> To communicate results </redtex>


---

## Characteristics of expository graphs

* The goal is to communicate information
* Information density is generally good
* Color/size are used both for aesthetics and communication
* Expository figures have understandable axes, titles, and legends


---

## Housing data

<img class=center src=assets/img/acs.png height='70%'/>

<br>

```{r downloadData, echo=FALSE, cache=TRUE}
download.file("https://dl.dropbox.com/u/7710864/data/csv_hid/ss06pid.csv",destfile="./data/ss06pid.csv",method="curl")
```


```{r loadData,dependson="downloadData"}
pData <- read.csv("./data/ss06pid.csv")
```


---

## Axes

Important parameters: _xlab_,_ylab_,_cex.lab_,_cex.axis_
```{r , dependson="loadData",fig.height=4,fig.width=4}
plot(pData$JWMNP,pData$WAGP,pch=19,col="blue",cex=0.5,
     xlab="Travel time (min)",ylab="Last 12 month wages (dollars)")
```


---

## Axes

```{r , dependson="loadData",fig.height=4.5,fig.width=4.5}
plot(pData$JWMNP,pData$WAGP,pch=19,col="blue",cex=0.5,
     xlab="Travel time (min)",ylab="Last 12 month wages (dollars)",cex.lab=2,cex.axis=1.5)
```


---

## Legends

* Important paramters: _x_,_y_,_legend_, _other plotting parameters_
```{r , dependson="loadData",fig.height=4,fig.width=4}
plot(pData$JWMNP,pData$WAGP,pch=19,col="blue",cex=0.5,xlab="TT (min)",ylab="Wages (dollars)")
legend(100,200000,legend="All surveyed",col="blue",pch=19,cex=0.5)
```

---

## Legends


```{r , dependson="loadData",fig.height=4.5,fig.width=4.5}
plot(pData$JWMNP,pData$WAGP,pch=19,cex=0.5,xlab="TT (min)",ylab="Wages (dollars)",col=pData$SEX)
legend(100,200000,legend=c("men","women"),col=c("black","red"),pch=c(19,19),cex=c(0.5,0.5))
```


---

## Titles

```{r , dependson="loadData",fig.height=4.5,fig.width=4.5}
plot(pData$JWMNP,pData$WAGP,pch=19,cex=0.5,xlab="CT (min)",
     ylab="Wages (dollars)",col=pData$SEX,main="Wages earned versus commute time")
legend(100,200000,legend=c("men","women"),col=c("black","red"),pch=c(19,19),cex=c(0.5,0.5))
```


---

## Multiple panels


```{r , dependson="loadData",fig.height=4,fig.width=8}
par(mfrow=c(1,2))
hist(pData$JWMNP,xlab="CT (min)",col="blue",breaks=100,main="")
plot(pData$JWMNP,pData$WAGP,pch=19,cex=0.5,xlab="CT (min)",ylab="Wages (dollars)",col=pData$SEX)
legend(100,200000,legend=c("men","women"),col=c("black","red"),pch=c(19,19),cex=c(0.5,0.5))
```



---

## Adding text

```{r , dependson="loadData",fig.height=4,fig.width=8}
par(mfrow=c(1,2))
hist(pData$JWMNP,xlab="CT (min)",col="blue",breaks=100,main="")
mtext(text="(a)",side=3,line=1)
plot(pData$JWMNP,pData$WAGP,pch=19,cex=0.5,xlab="CT (min)",ylab="Wages (dollars)",col=pData$SEX)
legend(100,200000,legend=c("men","women"),col=c("black","red"),pch=c(19,19),cex=c(0.5,0.5))
mtext(text="(b)",side=3,line=1)
```


---

## Figure captions

```{r , echo=FALSE, dependson="loadData",fig.height=4,fig.width=8}
par(mfrow=c(1,2))
hist(pData$JWMNP,xlab="CT (min)",col="blue",breaks=100,main="")
mtext(text="(a)",side=3,line=1)
plot(pData$JWMNP,pData$WAGP,pch=19,cex=0.5,xlab="CT (min)",ylab="Wages (dollars)",col=pData$SEX)
legend(100,200000,legend=c("men","women"),col=c("black","red"),pch=c(19,19),cex=c(0.5,0.5))
mtext(text="(b)",side=3,line=1)
```

__Figure 1. Distribution of commute time and relationship to wage earned by sex__ __(a)__ Commute times in the American Community Survey (ACS) are right skewed. __(b)__ Commute times do not appear to be strongly correlated with wage for either sex. 

---

## Colorblindness


<img class=center src=assets/img/vischeck.png height='80%'/>


[http://www.vischeck.com/](http://www.vischeck.com/vischeck/vischeckImage.php)


---

## Graphical workflow

* Start with a rough plot
* Tweak it to make it expository
* <redtext> Save the file </redtext>
* Include it in presentations

Saving files in R is done with graphics _devices_. Use the command ?Devices to see a list. Here we will go over the most popular devices. 

---

## pdf

* Important parameters: _file_, _height_,_width_
```{r, dependson="loadData"}
pdf(file="twoPanel.pdf",height=4,width=8)
par(mfrow=c(1,2))
hist(pData$JWMNP,xlab="CT (min)",col="blue",breaks=100,main="")
mtext(text="(a)",side=3,line=1)
plot(pData$JWMNP,pData$WAGP,pch=19,cex=0.5,xlab="CT (min)",ylab="Wages (dollars)",col=pData$SEX)
legend(100,200000,legend=c("men","women"),col=c("black","red"),pch=c(19,19),cex=c(0.5,0.5))
mtext(text="(b)",side=3,line=1)

dev.off()
```

---

## png

* Important parameters: _file_, _height_,_width_
```{r, dependson="loadData"}
png(file="twoPanel.png",height=480,width=(2*480))
par(mfrow=c(1,2))
hist(pData$JWMNP,xlab="CT (min)",col="blue",breaks=100,main="")
mtext(text="(a)",side=3,line=1)
plot(pData$JWMNP,pData$WAGP,pch=19,cex=0.5,xlab="CT (min)",ylab="Wages (dollars)",col=pData$SEX)
legend(100,200000,legend=c("men","women"),col=c("black","red"),pch=c(19,19),cex=c(0.5,0.5))
mtext(text="(b)",side=3,line=1)
dev.off()
```


---

## dev.copy2pdf

```{r, dependson="loadData",fig.height=3,fig.width=7}
par(mfrow=c(1,2))
hist(pData$JWMNP,xlab="CT (min)",col="blue",breaks=100,main="")
plot(pData$JWMNP,pData$WAGP,pch=19,cex=0.5,xlab="CT (min)",ylab="Wages (dollars)",col=pData$SEX)
dev.copy2pdf(file="twoPanelv2.pdf")
```


---

## Something to avoid

<img class=center src=assets/img/broman.png height='80%'/>

[http://www.biostat.wisc.edu/~kbroman/topten_worstgraphs/](http://www.biostat.wisc.edu/~kbroman/topten_worstgraphs/)


---

## Something to aspire to

<img class=center src=assets/img/facebook.png height='80%'/>

[http://www.facebook.com/notes/facebook-engineering/visualizing-friendships/469716398919](http://www.facebook.com/notes/facebook-engineering/visualizing-friendships/469716398919)



---

## Further resources

* [How to display data badly](http://www.jstor.org/discover/10.2307/2683253?uid=3739704&uid=2&uid=4&uid=3739256&sid=21101619120151)
* [The visual display of quantitative information](http://www.amazon.com/exec/obidos/ASIN/0961392142/7210-20)
* [Creating more effective graphs](http://www.amazon.com/exec/obidos/ASIN/047127402X/7210-20)
* [R Graphics Cookbook](http://www.amazon.com/R-Graphics-Cookbook-Winston-Chang/dp/1449316956)
* [ggplot2: Elegant Graphics for Data Analysis](http://www.amazon.com/ggplot2-Elegant-Graphics-Data-Analysis/dp/0387981403)
* [Flowing Data](http://flowingdata.com/)


<!--=*=*=*=*-->
<!--./week3/004hierachicalClustering/index.Rmd
-->
---
title       : Hierarchical clustering
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow   # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache = T, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Can we find things that are close together? 

Clustering organizes things that are __close__ into groups


* How do we define close?
* How do we group things?
* How do we visualize the grouping? 
* How do we interpret the grouping? 

---

## Hugely important/impactful

<img class=center src=assets/img/cluster.png height='80%'/>

[http://scholar.google.com/scholar?hl=en&q=cluster+analysis&btnG=&as_sdt=1%2C21&as_sdtp=](http://scholar.google.com/scholar?hl=en&q=cluster+analysis&btnG=&as_sdt=1%2C21&as_sdtp=)

---

## Hierarchical clustering

* An agglomerative approach
  * Find closest two things
  * Put them together
  * Find next closest
* Requires
  * A defined distance
  * A merging approach
* Produces
  * A tree showing how close things are to each other


---


## How do we define close?

* Most important step
  * Garbage in -> garbage out
* Distance or similarity
  * Continuous - euclidean distance
  * Continuous - correlation similarity
  * Binary - manhattan distance
* Pick a distance/similarity that makes sense for your problem
  
  

---

## Example distances - Euclidean

<img class=center src=assets/img/distance.png height='80%'/>

[http://rafalab.jhsph.edu/688/lec/lecture5-clustering.pdf](http://rafalab.jhsph.edu/688/lec/lecture5-clustering.pdf)


---

## Example distances - Euclidean

<img class=center src=assets/img/distance2.png height='60%'/>

In general:

$$\sqrt{(A_1-A_2)^2 + (B_1-B_2)^2 + \ldots + (Z_1-Z_2)^2}$$
[http://rafalab.jhsph.edu/688/lec/lecture5-clustering.pdf](http://rafalab.jhsph.edu/688/lec/lecture5-clustering.pdf)



---

## Example distances - Manhattan

<img class=center src=assets/img/manhattan.svg height='60%'/>

In general:

$$|A_1-A_2| + |B_1-B_2| + \ldots + |Z_1-Z_2|$$

[http://en.wikipedia.org/wiki/Taxicab_geometry](http://en.wikipedia.org/wiki/Taxicab_geometry)



---

## Hierarchical clustering - example

```{r createData, fig.height=3.5,fig.width=3.5}
set.seed(1234); par(mar=c(0,0,0,0))
x <- rnorm(12,mean=rep(1:3,each=4),sd=0.2)
y <- rnorm(12,mean=rep(c(1,2,1),each=4),sd=0.2)
plot(x,y,col="blue",pch=19,cex=2)
text(x+0.05,y+0.05,labels=as.character(1:12))
```


---

## Hierarchical clustering - dist

* Important parameters: _x_,_method_
```{r dependson="createData",fig.height=4,fig.width=4}
dataFrame <- data.frame(x=x,y=y)
dist(dataFrame)
```

---

## Hierarchical clustering - #1

```{r dependson="createData",echo=FALSE, fig.height=4,fig.width=8}
library(fields)
dataFrame <- data.frame(x=x,y=y)
rdistxy <- rdist(dataFrame)
diag(rdistxy) <- diag(rdistxy) + 1e5

# Find the index of the points with minimum distance
ind <- which(rdistxy == min(rdistxy),arr.ind=TRUE)
par(mfrow=c(1,2),mar=rep(0.2,4))
# Plot the points with the minimum overlayed
plot(x,y,col="blue",pch=19,cex=2)
text(x+0.05,y+0.05,labels=as.character(1:12))
points(x[ind[1,]],y[ind[1,]],col="orange",pch=19,cex=2)

# Make a cluster and cut it at the right height
distxy <- dist(dataFrame)
hcluster <- hclust(distxy)
dendro <- as.dendrogram(hcluster)
cutDendro <- cut(dendro,h=(hcluster$height[1]+0.00001) )
plot(cutDendro$lower[[11]],yaxt="n")
```



---

## Hierarchical clustering - #2

```{r dependson="createData",echo=FALSE, fig.height=4,fig.width=4}
library(fields)
dataFrame <- data.frame(x=x,y=y)
rdistxy <- rdist(dataFrame)
diag(rdistxy) <- diag(rdistxy) + 1e5

# Find the index of the points with minimum distance
ind <- which(rdistxy == min(rdistxy),arr.ind=TRUE)
par(mar=rep(0.2,4))
# Plot the points with the minimum overlayed
plot(x,y,col="blue",pch=19,cex=2)
text(x+0.05,y+0.05,labels=as.character(1:12))
points(x[ind[1,]],y[ind[1,]],col="orange",pch=19,cex=2)
points(mean(x[ind[1,]]),mean(y[ind[1,]]),col="black",cex=3,lwd=3,pch=3)
points(mean(x[ind[1,]]),mean(y[ind[1,]]),col="orange",cex=5,lwd=3,pch=1)


```



---

## Hierarchical clustering - #3

```{r dependson="createData",echo=FALSE, fig.height=4,fig.width=8}
library(fields)
dataFrame <- data.frame(x=x,y=y)
rdistxy <- rdist(dataFrame)
diag(rdistxy) <- diag(rdistxy) + 1e5

# Find the index of the points with minimum distance
ind <- which(rdistxy == rdistxy[order(rdistxy)][3],arr.ind=TRUE)
par(mfrow=c(1,3),mar=rep(0.2,4))
# Plot the points with the minimum overlayed
plot(x,y,col="blue",pch=19,cex=2)
text(x+0.05,y+0.05,labels=as.character(1:12))
points(x[c(5,6)],y[c(5,6)],col="orange",pch=19,cex=2)
points(x[ind[1,]],y[ind[1,]],col="red",pch=19,cex=2)

# Make dendogram plots
distxy <- dist(dataFrame)
hcluster <- hclust(distxy)
dendro <- as.dendrogram(hcluster)
cutDendro <- cut(dendro,h=(hcluster$height[2]) )
plot(cutDendro$lower[[10]],yaxt="n")
plot(cutDendro$lower[[5]],yaxt="n")

```



---

## Hierarchical clustering - hclust

```{r, dependson="createData", fig.height=4,fig.width=4}
dataFrame <- data.frame(x=x,y=y)
distxy <- dist(dataFrame)
hClustering <- hclust(distxy)
plot(hClustering)
```


---

## Prettier dendrograms

```{r plclust}

myplclust <- function( hclust, lab=hclust$labels, lab.col=rep(1,length(hclust$labels)), hang=0.1,...){
  ## modifiction of plclust for plotting hclust objects *in colour*!
  ## Copyright Eva KF Chan 2009
  ## Arguments:
  ##    hclust:    hclust object
  ##    lab:        a character vector of labels of the leaves of the tree
  ##    lab.col:    colour for the labels; NA=default device foreground colour
  ##    hang:     as in hclust & plclust
  ## Side effect:
  ##    A display of hierarchical cluster with coloured leaf labels.
  y <- rep(hclust$height,2); x <- as.numeric(hclust$merge)
  y <- y[which(x<0)]; x <- x[which(x<0)]; x <- abs(x)
  y <- y[order(x)]; x <- x[order(x)]
  plot( hclust, labels=FALSE, hang=hang, ... )
  text( x=x, y=y[hclust$order]-(max(hclust$height)*hang),
        labels=lab[hclust$order], col=lab.col[hclust$order], 
        srt=90, adj=c(1,0.5), xpd=NA, ... )
}

```


---

## Pretty dendrograms

```{r, dependson="createData", fig.height=4,fig.width=4}
dataFrame <- data.frame(x=x,y=y)
distxy <- dist(dataFrame)
hClustering <- hclust(distxy)
myplclust(hClustering,lab=rep(1:3,each=4),lab.col=rep(1:3,each=4))
```

---

## Even Prettier dendrograms


<img class=center src=assets/img/prettydendro.png height='80%'/>


[http://gallery.r-enthusiasts.com/RGraphGallery.php?graph=79](http://gallery.r-enthusiasts.com/RGraphGallery.php?graph=79)


---

## Merging points - complete

```{r,echo=FALSE,dependson="createData",fig.height=4,fig.width=4}
dataFrame <- data.frame(x=x,y=y)
par(mar=rep(0.1,4))
plot(x,y,col="blue",pch=19,cex=2)
points(x[8],y[8],col="orange",pch=3,lwd=3,cex=3)
points(x[1],y[1],col="orange",pch=3,lwd=3,cex=3)
segments(x[8],y[8],x[1],y[1],lwd=3,col="orange")

```



---

## Merging points - average

```{r,echo=FALSE,dependson="createData",fig.height=4,fig.width=4}
dataFrame <- data.frame(x=x,y=y)
par(mar=rep(0.1,4))
plot(x,y,col="blue",pch=19,cex=2)
points(mean(x[1:4]),mean(y[1:4]),col="orange",pch=3,lwd=3,cex=3)
points(mean(x[5:8]),mean(y[5:8]),col="orange",pch=3,lwd=3,cex=3)
segments(mean(x[1:4]),mean(y[1:4]),mean(x[5:8]),mean(y[5:8]),lwd=3,col="orange")

```


---

## heatmap()

```{r,dependson="createData",fig.height=4,fig.width=4}
dataFrame <- data.frame(x=x,y=y)
set.seed(143)
dataMatrix <- as.matrix(dataFrame)[sample(1:12),]
heatmap(dataMatrix)
```



---

## Notes and further resources

* Gives an idea of the relationships between variables/observations
* The picture may be unstable
  * Change a few points
  * Have different missing values
  * Pick a different distance
  * Change the merging strategy
  * Change the scale of points for one variable
* But it is deterministic
* Choosing where to cut isn't always obvious
* Should be primarily used for exploration 
* [Rafa's Distances and Clustering Video](http://www.youtube.com/watch?v=wQhVWUcXM0A)
* [Elements of statistical learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)








<!--=*=*=*=*-->
<!--./week3/005kmeansClustering/index.Rmd
-->
---
title       : K-means clustering
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow   # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache = T, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Can we find things that are close together? 

* How do we define close?
* How do we group things?
* How do we visualize the grouping? 
* How do we interpret the grouping? 


---

## How do we define close?

* Most important step
  * Garbage in -> garbage out
* Distance or similarity
  * Continuous - euclidean distance
  * Continuous - correlation similarity
  * Binary - manhattan distance
* Pick a distance/similarity that makes sense for your problem
  

---

## K-means clustering

* A partitioning approach
  * Fix a number of clusters
  * Get "centroids" of each cluster
  * Assign things to closest centroid
  * Recalculate centroids
* Requires
  * A defined distance metric
  * A number of clusters
  * An initial guess as to cluster centroids
* Produces
  * Final estimate of cluster centroids
  * An assignment of each point to clusters
  

---

## K-means clustering -  example


```{r createData, fig.height=3.5,fig.width=3.5}
set.seed(1234); par(mar=c(0,0,0,0))
x <- rnorm(12,mean=rep(1:3,each=4),sd=0.2)
y <- rnorm(12,mean=rep(c(1,2,1),each=4),sd=0.2)
plot(x,y,col="blue",pch=19,cex=2)
text(x+0.05,y+0.05,labels=as.character(1:12))
```


---

## K-means clustering -  starting centroids


```{r,dependson="createData",echo=FALSE,fig.height=5,fig.width=5}
par(mar=rep(0.2,4))
plot(x,y,col="blue",pch=19,cex=2)
text(x+0.05,y+0.05,labels=as.character(1:12))
cx <- c(1,1.8,2.5)
cy <- c(2,1,1.5)
points(cx,cy,col=c("red","orange","purple"),pch=3,cex=2,lwd=2)
```

---

## K-means clustering -  assign to closest centroid

```{r,dependson="createData",echo=FALSE,fig.height=5,fig.width=5}
par(mar=rep(0.2,4))
plot(x,y,col="blue",pch=19,cex=2)
cols1 <- c("red","orange","purple")
text(x+0.05,y+0.05,labels=as.character(1:12))
cx <- c(1,1.8,2.5)
cy <- c(2,1,1.5)
points(cx,cy,col=cols1,pch=3,cex=2,lwd=2)

## Find the closest centroid
distTmp <- matrix(NA,nrow=3,ncol=12)
distTmp[1,] <- (x-cx[1])^2 + (y-cy[1])^2
distTmp[2,] <- (x-cx[2])^2 + (y-cy[2])^2
distTmp[3,] <- (x-cx[3])^2 + (y-cy[3])^2
newClust <- apply(distTmp,2,which.min)
points(x,y,pch=19,cex=2,col=cols1[newClust])
```

---

## K-means clustering -  recalculate centroids

```{r,dependson="createData",echo=FALSE,fig.height=5,fig.width=5}
par(mar=rep(0.2,4))
plot(x,y,col="blue",pch=19,cex=2)
cols1 <- c("red","orange","purple")
text(x+0.05,y+0.05,labels=as.character(1:12))

## Find the closest centroid
distTmp <- matrix(NA,nrow=3,ncol=12)
distTmp[1,] <- (x-cx[1])^2 + (y-cy[1])^2
distTmp[2,] <- (x-cx[2])^2 + (y-cy[2])^2
distTmp[3,] <- (x-cx[3])^2 + (y-cy[3])^2
newClust <- apply(distTmp,2,which.min)
points(x,y,pch=19,cex=2,col=cols1[newClust])
newCx <- tapply(x,newClust,mean)
newCy <- tapply(y,newClust,mean)

## Old centroids

cx <- c(1,1.8,2.5)
cy <- c(2,1,1.5)

points(newCx,newCy,col=cols1,pch=3,cex=2,lwd=2)

```


---

## K-means clustering -  reassign values

```{r,dependson="createData",echo=FALSE,fig.height=5,fig.width=5}
par(mar=rep(0.2,4))
plot(x,y,col="blue",pch=19,cex=2)
cols1 <- c("red","orange","purple")
text(x+0.05,y+0.05,labels=as.character(1:12))


cx <- c(1,1.8,2.5)
cy <- c(2,1,1.5)


## Find the closest centroid
distTmp <- matrix(NA,nrow=3,ncol=12)
distTmp[1,] <- (x-cx[1])^2 + (y-cy[1])^2
distTmp[2,] <- (x-cx[2])^2 + (y-cy[2])^2
distTmp[3,] <- (x-cx[3])^2 + (y-cy[3])^2
newClust <- apply(distTmp,2,which.min)
newCx <- tapply(x,newClust,mean)
newCy <- tapply(y,newClust,mean)

## Old centroids

points(newCx,newCy,col=cols1,pch=3,cex=2,lwd=2)


## Iteration 2
distTmp <- matrix(NA,nrow=3,ncol=12)
distTmp[1,] <- (x-newCx[1])^2 + (y-newCy[1])^2
distTmp[2,] <- (x-newCx[2])^2 + (y-newCy[2])^2
distTmp[3,] <- (x-newCx[3])^2 + (y-newCy[3])^2
newClust2 <- apply(distTmp,2,which.min)

points(x,y,pch=19,cex=2,col=cols1[newClust2])

```



---

## K-means clustering -  update centroids

```{r,dependson="createData",echo=FALSE,fig.height=5,fig.width=5}
par(mar=rep(0.2,4))
plot(x,y,col="blue",pch=19,cex=2)
cols1 <- c("red","orange","purple")
text(x+0.05,y+0.05,labels=as.character(1:12))


cx <- c(1,1.8,2.5)
cy <- c(2,1,1.5)

## Find the closest centroid
distTmp <- matrix(NA,nrow=3,ncol=12)
distTmp[1,] <- (x-cx[1])^2 + (y-cy[1])^2
distTmp[2,] <- (x-cx[2])^2 + (y-cy[2])^2
distTmp[3,] <- (x-cx[3])^2 + (y-cy[3])^2
newClust <- apply(distTmp,2,which.min)
newCx <- tapply(x,newClust,mean)
newCy <- tapply(y,newClust,mean)



## Iteration 2
distTmp <- matrix(NA,nrow=3,ncol=12)
distTmp[1,] <- (x-newCx[1])^2 + (y-newCy[1])^2
distTmp[2,] <- (x-newCx[2])^2 + (y-newCy[2])^2
distTmp[3,] <- (x-newCx[3])^2 + (y-newCy[3])^2
finalClust <- apply(distTmp,2,which.min)


## Final centroids
finalCx <- tapply(x,finalClust,mean)
finalCy <- tapply(y,finalClust,mean)
points(finalCx,finalCy,col=cols1,pch=3,cex=2,lwd=2)



points(x,y,pch=19,cex=2,col=cols1[finalClust])

```


---

## kmeans()

* Important parameters: _x_,_centers_,_iter.max_,_nstart_

```{r kmeans,dependson="createData"}
dataFrame <- data.frame(x,y)
kmeansObj <- kmeans(dataFrame,centers=3)
names(kmeansObj)
kmeansObj$cluster
```

---

## kmeans()

```{r, dependson="kmeans",fig.height=4,fig.width=4}
par(mar=rep(0.2,4))
plot(x,y,col=kmeansObj$cluster,pch=19,cex=2)
points(kmeansObj$centers,col=1:3,pch=3,cex=3,lwd=3)
```

---

## Heatmaps

```{r, dependson="kmeans",fig.height=3,fig.width=6}
set.seed(1234)
dataMatrix <- as.matrix(dataFrame)[sample(1:12),]
kmeansObj2 <- kmeans(dataMatrix,centers=3)
par(mfrow=c(1,2),mar=rep(0.2,4))
image(t(dataMatrix)[,nrow(dataMatrix):1],yaxt="n")
image(t(dataMatrix)[,order(kmeansObj2$cluster)],yaxt="n")
```



---

## Notes and further resources

* K-means requires a number of clusters
  * Pick by eye/intuition
  * Pick by cross validation/information theory, etc.
  * [Determining the number of clusters](http://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set)
* K-means is not deterministic
  * Different # of clusters 
  * Different number of iterations
* [Rafa's Distances and Clustering Video](http://www.youtube.com/watch?v=wQhVWUcXM0A)
* [Elements of statistical learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)



<!--=*=*=*=*-->
<!--./week3/006dimensionReduction/index.Rmd
-->
---
title       : Principal components analysis and singular value decomposition
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow   # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache = T, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Matrix data 

```{r randomData,fig.height=4,fig.width=3}
set.seed(12345); par(mar=rep(0.2,4))
dataMatrix <- matrix(rnorm(400),nrow=40)
image(1:10,1:40,t(dataMatrix)[,nrow(dataMatrix):1])
```

---

## Cluster the data 

```{r, dependson ="randomData",fig.height=5,fig.width=4}
par(mar=rep(0.2,4))
heatmap(dataMatrix)
```

---

## What if we add a pattern?

```{r onePattern, dependson ="randomData",fig.height=4,fig.width=3}
set.seed(678910)
for(i in 1:40){
  # flip a coin
  coinFlip <- rbinom(1,size=1,prob=0.5)
  # if coin is heads add a common pattern to that row
  if(coinFlip){
    dataMatrix[i,] <- dataMatrix[i,] + rep(c(0,3),each=5)
  }
}
```


---

## What if we add a pattern? - the data

```{r dependson ="onePattern",fig.height=4,fig.width=3}
par(mar=rep(0.2,4))
image(1:10,1:40,t(dataMatrix)[,nrow(dataMatrix):1])
```


---

## What if we add a pattern? - the clustered data

```{r dependson ="onePattern",fig.height=5,fig.width=4}
par(mar=rep(0.2,4))
heatmap(dataMatrix)
```



---

## Patterns in rows and columns



```{r oChunk, dependson ="onePattern",fig.height=4,fig.width=8}
hh <- hclust(dist(dataMatrix)); dataMatrixOrdered <- dataMatrix[hh$order,]
par(mfrow=c(1,3))
image(t(dataMatrixOrdered)[,nrow(dataMatrixOrdered):1])
plot(rowMeans(dataMatrixOrdered),40:1,,xlab="Row",ylab="Row Mean",pch=19)
plot(colMeans(dataMatrixOrdered),xlab="Column",ylab="Column Mean",pch=19)
```

---

## Related problems

You have multivariate variables $X_1,\ldots,X_n$ so $X_1 = (X_{11},\ldots,X_{1m})$

* Find a new set of multivariate variables that are uncorrelated and explain as much variance as possible.
* If you put all the variables together in one matrix, find the best matrix created with fewer variables (lower rank) that explains the original data.


The first goal is <font color="#330066">statistical</font> and the second goal is <font color="#993300">data compression</font>.

---

## Related solutions - PCA/SVD

__SVD__

If $X$ is a matrix with each variable in a column and each observation in a row then the SVD is a "matrix decomposition"

$$ X = UDV^T$$

where the columns of $U$ are orthogonal (left singular vectors), the columns of $V$ are orthogonal (right singular vectors) and $D$ is a diagonal matrix (singular values). 

__PCA__

The principal components are equal to the right singular values if you first scale (subtract the mean, divide by the standard deviation) the variables.

---

## Components of the SVD - u and v

```{r dependson ="oChunk",fig.height=4,fig.width=8}
svd1 <- svd(scale(dataMatrixOrdered))
par(mfrow=c(1,3))
image(t(dataMatrixOrdered)[,nrow(dataMatrixOrdered):1])
plot(svd1$u[,1],40:1,,xlab="Row",ylab="First left singular vector",pch=19)
plot(svd1$v[,1],xlab="Column",ylab="First right singular vector",pch=19)
```


---

## Components of the SVD - d and variance explained

```{r dependson ="oChunk",fig.height=4,fig.width=8}
svd1 <- svd(scale(dataMatrixOrdered))
par(mfrow=c(1,2))
plot(svd1$d,xlab="Column",ylab="Singular value",pch=19)
plot(svd1$d^2/sum(svd1$d^2),xlab="Column",ylab="Percent of variance explained",pch=19)
```


---

## Relationship to principal components

```{r dependson ="oChunk",fig.height=4,fig.width=4}
svd1 <- svd(scale(dataMatrixOrdered))
pca1 <- prcomp(dataMatrixOrdered,scale=TRUE)
plot(pca1$rotation[,1],svd1$v[,1],pch=19,xlab="Principal Component 1",ylab="Right Singular Vector 1")
abline(c(0,1))
```

---

## Components of the SVD - variance explained

```{r dependson ="oChunk",fig.height=3,fig.width=6}
constantMatrix <- dataMatrixOrdered*0
for(i in 1:dim(dataMatrixOrdered)[1]){constantMatrix[i,] <- rep(c(0,1),each=5)}
svd1 <- svd(constantMatrix)
par(mfrow=c(1,3))
image(t(constantMatrix)[,nrow(constantMatrix):1])
plot(svd1$d,xlab="Column",ylab="Singular value",pch=19)
plot(svd1$d^2/sum(svd1$d^2),xlab="Column",ylab="Percent of variance explained",pch=19)
```


---

## What if we add a second pattern?

```{r twoPattern, dependson ="randomData",fig.height=4,fig.width=3}
set.seed(678910)
for(i in 1:40){
  # flip a coin
  coinFlip1 <- rbinom(1,size=1,prob=0.5)
  coinFlip2 <- rbinom(1,size=1,prob=0.5)
  # if coin is heads add a common pattern to that row
  if(coinFlip1){
    dataMatrix[i,] <- dataMatrix[i,] + rep(c(0,5),each=5)
  }
  if(coinFlip2){
    dataMatrix[i,] <- dataMatrix[i,] + rep(c(0,5),5)
  }
}
hh <- hclust(dist(dataMatrix)); dataMatrixOrdered <- dataMatrix[hh$order,]
```

---

## Singular value decomposition - true patterns 

```{r  dependson ="twoPattern",fig.height=4,fig.width=8}
svd2 <- svd(scale(dataMatrixOrdered))
par(mfrow=c(1,3))
image(t(dataMatrixOrdered)[,nrow(dataMatrixOrdered):1])
plot(rep(c(0,1),each=5),pch=19,xlab="Column",ylab="Pattern 1")
plot(rep(c(0,1),5),pch=19,xlab="Column",ylab="Pattern 2")
```

---

##  v and patterns of variance in rows

```{r  dependson ="twoPattern",fig.height=4,fig.width=8}
svd2 <- svd(scale(dataMatrixOrdered))
par(mfrow=c(1,3))
image(t(dataMatrixOrdered)[,nrow(dataMatrixOrdered):1])
plot(svd2$v[,1],pch=19,xlab="Column",ylab="First right singular vector")
plot(svd2$v[,2],pch=19,xlab="Column",ylab="Second right singular vector")
```


---

##  d and variance explained

```{r dependson ="twoPattern",fig.height=4,fig.width=8}
svd1 <- svd(scale(dataMatrixOrdered))
par(mfrow=c(1,2))
plot(svd1$d,xlab="Column",ylab="Singular value",pch=19)
plot(svd1$d^2/sum(svd1$d^2),xlab="Column",ylab="Percent of variance explained",pch=19)
```



---

## fast.svd function {corpcor}

Important parameters: _m_,_tol_
```{r dependson ="twoPattern",fig.height=4,fig.width=8}
bigMatrix <- matrix(rnorm(1e4*40),nrow=1e4)
system.time(svd(scale(bigMatrix)))
system.time(fast.svd(scale(bigMatrix),tol=0))
```


---

## Missing values

```{r,dependson="twoPattern",error=TRUE}
dataMatrix2 <- dataMatrixOrdered
dataMatrix2[sample(1:100,size=40,replace=F)] <- NA
svd1 <- svd(scale(dataMatrix2))
```


---

## Imputing {impute}

```{r,dependson="twoPattern",fig.height=4,fig.width=8}
library(impute)
dataMatrix2 <- dataMatrixOrdered
dataMatrix2[sample(1:100,size=40,replace=F)] <- NA
dataMatrix2 <- impute.knn(dataMatrix2)$data
svd1 <- svd(scale(dataMatrixOrdered)); svd2 <- svd(scale(dataMatrix2))
par(mfrow=c(1,2)); plot(svd1$v[,1],pch=19); plot(svd2$v[,1],pch=19)
```



---

## Face example

```{r loadFaceData ,fig.height=4,fig.width=4}
download.file("https://spark-public.s3.amazonaws.com/dataanalysis/face.rda",destfile="./data/face.rda",method="curl")
load("./data/face.rda")
image(t(faceData)[,nrow(faceData):1])
```


---

## Face example - variance explained

```{r,dependson="loadFaceData",fig.height=4,fig.width=4}
svd1 <- svd(scale(faceData))
plot(svd1$d^2/sum(svd1$d^2),pch=19,xlab="Singular vector",ylab="Variance explained")
```

---

## Face example - create approximations

```{r approximations,dependson="loadFaceData",fig.height=4,fig.width=4}

svd1 <- svd(scale(faceData))
# %*% is matrix multiplication

# Here svd1$d[1] is a constant
approx1 <- svd1$u[,1] %*% t(svd1$v[,1]) * svd1$d[1]

# In these examples we need to make the diagonal matrix out of d
approx5 <- svd1$u[,1:5] %*% diag(svd1$d[1:5])%*% t(svd1$v[,1:5]) 
approx10 <- svd1$u[,1:10] %*% diag(svd1$d[1:10])%*% t(svd1$v[,1:10]) 
```

---

## Face example - plot approximations
```{r dependson="approximations",fig.height=4,fig.width=8}
par(mfrow=c(1,4))
image(t(faceData)[,nrow(faceData):1])
image(t(approx10)[,nrow(approx10):1])
image(t(approx5)[,nrow(approx5):1])
image(t(approx1)[,nrow(approx1):1])
```


---

## Notes and further resources

* Scale matters
* PC's/SV's may mix real patterns
* Can be computationally intensive
* [Advanced data analysis from an elementary point of view](http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf)
* [Elements of statistical learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)
* Alternatives
  * [Factor analysis](http://en.wikipedia.org/wiki/Factor_analysis)
  * [Independent components analysis](http://en.wikipedia.org/wiki/Independent_component_analysis)
  * [Latent semantic analysis](http://en.wikipedia.org/wiki/Latent_semantic_analysis)










<!--=*=*=*=*-->
<!--./week4/001clusteringExample/index.Rmd
-->
---
title       : Clustering example
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow   # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache = T, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Samsung Galaxy S3

<img class=center src=assets/img/samsung.png height='80%'/>

[http://www.samsung.com/global/galaxys3/](http://www.samsung.com/global/galaxys3/)


---

## Samsung Data

<img class=center src=assets/img/ucisamsung.png height='60%'/>

[http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones](http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones)


---

## Slightly processed data

```{r loadData,cache=FALSE}
download.file("https://dl.dropbox.com/u/7710864/courseraPublic/samsungData.rda"
              ,destfile="./data/samsungData.rda",method="curl")
load("./data/samsungData.rda")
names(samsungData)[1:12]
table(samsungData$activity)
```

---

## Plotting average acceleration for first subject

```{r processData,dependson="loadData",fig.height=4.5,fig.width=8}
par(mfrow=c(1,2))
numericActivity <- as.numeric(as.factor(samsungData$activity))[samsungData$subject==1]
plot(samsungData[samsungData$subject==1,1],pch=19,col=numericActivity,ylab=names(samsungData)[1])
plot(samsungData[samsungData$subject==1,2],pch=19,col=numericActivity,ylab=names(samsungData)[2])
legend(150,-0.1,legend=unique(samsungData$activity),col=unique(numericActivity),pch=19)
```

---

## Clustering based just on average acceleration


```{r dependson="processData",fig.height=4,fig.width=4,cache=TRUE}
source("http://dl.dropbox.com/u/7710864/courseraPublic/myplclust.R")
distanceMatrix <- dist(samsungData[samsungData$subject==1,1:3])
hclustering <- hclust(distanceMatrix)
myplclust(hclustering,lab.col=numericActivity)
```


---

## Plotting max acceleration for the first subject

```{r ,dependson="processData",fig.height=4,fig.width=8}
par(mfrow=c(1,2))
plot(samsungData[samsungData$subject==1,10],pch=19,col=numericActivity,ylab=names(samsungData)[10])
plot(samsungData[samsungData$subject==1,11],pch=19,col=numericActivity,ylab=names(samsungData)[11])
```

---

## Clustering based on maximum acceleration

```{r dependson="processData",fig.height=4,fig.width=4,cache=TRUE}
source("http://dl.dropbox.com/u/7710864/courseraPublic/myplclust.R")
distanceMatrix <- dist(samsungData[samsungData$subject==1,10:12])
hclustering <- hclust(distanceMatrix)
myplclust(hclustering,lab.col=numericActivity)
```



---

## Singular value decomposition

```{r svdChunk,dependson="processData",fig.height=4,fig.width=8,cache=TRUE}
svd1 = svd(scale(samsungData[samsungData$subject==1,-c(562,563)]))
par(mfrow=c(1,2))
plot(svd1$u[,1],col=numericActivity,pch=19)
plot(svd1$u[,2],col=numericActivity,pch=19)
```

---

## Find maximum contributor

```{r dependson="svdChunk",fig.height=4,fig.width=4,cache=TRUE}
plot(svd1$v[,2],pch=19)

```


---

##  New clustering with maximum contributer

```{r dependson="svdChunk",fig.height=4.5,fig.width=4.5,cache=TRUE}
maxContrib <- which.max(svd1$v[,2])
distanceMatrix <- dist(samsungData[samsungData$subject==1,c(10:12,maxContrib)])
hclustering <- hclust(distanceMatrix)
myplclust(hclustering,lab.col=numericActivity)                             
```


---

##  New clustering with maximum contributer

```{r dependson="svdChunk",fig.height=4.5,fig.width=4.5,cache=TRUE}
names(samsungData)[maxContrib]                          
```

---

##  K-means clustering (nstart=1, first try)

```{r kmeans1,dependson="processData",fig.height=4,fig.width=4}
kClust <- kmeans(samsungData[samsungData$subject==1,-c(562,563)],centers=6)
table(kClust$cluster,samsungData$activity[samsungData$subject==1])
```



---

##  K-means clustering (nstart=1, second try)

```{r dependson="kmeans1",fig.height=4,fig.width=4,cache=TRUE}
kClust <- kmeans(samsungData[samsungData$subject==1,-c(562,563)],centers=6,nstart=1)
table(kClust$cluster,samsungData$activity[samsungData$subject==1])
```


---

##  K-means clustering (nstart=100, first try)

```{r dependson="kmeans1",fig.height=4,fig.width=4,cache=TRUE}
kClust <- kmeans(samsungData[samsungData$subject==1,-c(562,563)],centers=6,nstart=100)
table(kClust$cluster,samsungData$activity[samsungData$subject==1])
```



---

##  K-means clustering (nstart=100, second try)

```{r kmeans100,dependson="kmeans1",fig.height=4,fig.width=4,cache=TRUE}
kClust <- kmeans(samsungData[samsungData$subject==1,-c(562,563)],centers=6,nstart=100)
table(kClust$cluster,samsungData$activity[samsungData$subject==1])
```

---

##  Cluster 1 Variable Centers (Laying)

```{r dependson="kmeans100",fig.height=4,fig.width=8,cache=FALSE}
plot(kClust$center[1,1:10],pch=19,ylab="Cluster Center",xlab="")
```


---

##  Cluster 2 Variable Centers (Walking)

```{r dependson="kmeans100",fig.height=4,fig.width=8,cache=FALSE}
plot(kClust$center[6,1:10],pch=19,ylab="Cluster Center",xlab="")
```



<!--=*=*=*=*-->
<!--./week4/002basicLeastSquares/index.Rmd
-->
---
title       : Basic least squares
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow  # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Goals of statistical modeling

* Describe the distribution of variables
* Describe the relationship between variables
* Make inferences about distributions or relationships


---

## Example: Average parent and child heights



<img class=center src=assets/img/galton.jpg height='80%'/>

[http://www.nature.com/ejhg/journal/v17/n8/full/ejhg20095a.html](http://www.nature.com/ejhg/journal/v17/n8/full/ejhg20095a.html)


---


## Still relevant


<img class=center src=assets/img/height.png height='60%'/>

[http://www.nature.com/ejhg/journal/v17/n8/full/ejhg20095a.html](http://www.nature.com/ejhg/journal/v17/n8/full/ejhg20095a.html)

[Predicting height: the Victorian approach beats modern genomics](http://www.wired.com/wiredscience/2009/03/predicting-height-the-victorian-approach-beats-modern-genomics/)


---

## Load Galton Data

```{r galton,fig.height=4,fig.width=8}
library(UsingR); data(galton)
par(mfrow=c(1,2))
hist(galton$child,col="blue",breaks=100)
hist(galton$parent,col="blue",breaks=100)
```

---

## The distribution of child heights

```{r, dependson="galton",fig.height=4,fig.width=4}
hist(galton$child,col="blue",breaks=100)
```


---

## Only know the child  - average height

```{r, dependson="galton",fig.height=4,fig.width=4}
hist(galton$child,col="blue",breaks=100)
meanChild <- mean(galton$child)
lines(rep(meanChild,100),seq(0,150,length=100),col="red",lwd=5)
```

---

## Only know the child  - why average?

If $C_i$ is the height of child $i$ then the average is the value of $\mu$ that minimizes:

$$ \sum_{i=1}^{928}(C_i - \mu)^2 $$

---

## What if we plot child versus average parent

```{r, dependson="galton",fig.height=4,fig.width=4}
plot(galton$parent,galton$child,pch=19,col="blue")
```

---

## Jittered plot

```{r, dependson="galton",fig.height=4,fig.width=4}
set.seed(1234)
plot(jitter(galton$parent,factor=2),jitter(galton$child,factor=2),pch=19,col="blue")
```

---

## Average parent = 65 inches tall

```{r, dependson="galton",fig.height=4,fig.width=4}
plot(galton$parent,galton$child,pch=19,col="blue")
near65 <- galton[abs(galton$parent - 65)<1, ]
points(near65$parent,near65$child,pch=19,col="red")
lines(seq(64,66,length=100),rep(mean(near65$child),100),col="red",lwd=4)
```

---

## Average parent = 71 inches tall

```{r, dependson="galton",fig.height=4,fig.width=4}
plot(galton$parent,galton$child,pch=19,col="blue")
near71 <- galton[abs(galton$parent - 71)<1, ]
points(near71$parent,near71$child,pch=19,col="red")
lines(seq(70,72,length=100),rep(mean(near71$child),100),col="red",lwd=4)
```


---

## Fitting a line



```{r, dependson="lm1",fig.height=4,fig.width=4}
plot(galton$parent,galton$child,pch=19,col="blue")
lm1 <- lm(galton$child ~ galton$parent)
lines(galton$parent,lm1$fitted,col="red",lwd=3)
```

---

## Why not this line?


```{r, dependson="galton",fig.height=4,fig.width=4}
plot(galton$parent,galton$child,pch=19,col="blue")
lines(galton$parent, 26 + 0.646*galton$parent)
```

---

## The equation for a line

If $C_i$ is the height of child $i$ and $P_i$ is the height of the average parent, then we can imagine writing the equation for a line

$$C_i = b_0 + b_1 P_i$$


---

## Not all points are on the line

```{r, dependson="lm1",fig.height=4,fig.width=4}
plot(galton$parent,galton$child,pch=19,col="blue")
lines(galton$parent,lm1$fitted,col="red",lwd=3)
```


---

## Allowing for variation

If $C_i$ is the height of child $i$ and $P_i$ is the height of the average parent, then we can imagine writing the equation for a line

$$C_i = b_0 + b_1 P_i + e_i$$

$e_i$ is everything we didn't measure (how much they eat, where they live, do they stretch in the morning...)

---

## How do we pick best?

If $C_i$ is the height of child $i$ and $P_i$ is the height of the average parent, pick the line that makes the child values $C_i$ and our guesses

$$ \sum_{i=1}^{928}(C_i - \{b_0 + b_1 P_i\})^2 $$


---

## Plot what is leftover

```{r, dependson="lm1",fig.height=4,fig.width=8}
par(mfrow=c(1,2))
plot(galton$parent,galton$child,pch=19,col="blue")
lines(galton$parent,lm1$fitted,col="red",lwd=3)
plot(galton$parent,lm1$residuals,col="blue",pch=19)
abline(c(0,0),col="red",lwd=3)
```


<!--=*=*=*=*-->
<!--./week4/003inferenceBasics/index.Rmd
-->
---
title       : Inference basics
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow  # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
par(mar=c(5,4,1,2))
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Fit a line to the Galton Data

```{r galton,fig.height=4,fig.width=4}
library(UsingR); data(galton);
plot(galton$parent,galton$child,pch=19,col="blue")
lm1 <- lm(galton$child ~ galton$parent)
lines(galton$parent,lm1$fitted,col="red",lwd=3)
```

---

## Fit a line to the Galton Data

```{r, dependson="galton",fig.height=4,fig.width=4}
lm1
```


---

## Create a "population" of 1 million families

```{r newGalton,dependson="galton",fig.height=4,fig.width=4}
newGalton <- data.frame(parent=rep(NA,1e6),child=rep(NA,1e6))
newGalton$parent <- rnorm(1e6,mean=mean(galton$parent),sd=sd(galton$parent))
newGalton$child <- lm1$coeff[1] + lm1$coeff[2]*newGalton$parent + rnorm(1e6,sd=sd(lm1$residuals))
smoothScatter(newGalton$parent,newGalton$child)
abline(lm1,col="red",lwd=3)
```


---

## Let's take a sample

```{r sampleGalton1 ,dependson="newGalton",fig.height=4,fig.width=4}
set.seed(134325); sampleGalton1 <- newGalton[sample(1:1e6,size=50,replace=F),]
sampleLm1 <- lm(sampleGalton1$child ~ sampleGalton1$parent)
plot(sampleGalton1$parent,sampleGalton1$child,pch=19,col="blue")
lines(sampleGalton1$parent,sampleLm1$fitted,lwd=3,lty=2)
abline(lm1,col="red",lwd=3)
```

---

## Let's take another sample

```{r sampleGalton2 ,dependson="sampleGalton1",fig.height=4,fig.width=4}
sampleGalton2 <- newGalton[sample(1:1e6,size=50,replace=F),]
sampleLm2 <- lm(sampleGalton2$child ~ sampleGalton2$parent)
plot(sampleGalton2$parent,sampleGalton2$child,pch=19,col="blue")
lines(sampleGalton2$parent,sampleLm2$fitted,lwd=3,lty=2)
abline(lm1,col="red",lwd=3)
```

---

## Let's take another sample

```{r sampleGalton3 ,dependson="sampleGalton2",fig.height=4,fig.width=4}
sampleGalton3 <- newGalton[sample(1:1e6,size=50,replace=F),]
sampleLm3 <- lm(sampleGalton3$child ~ sampleGalton3$parent)
plot(sampleGalton3$parent,sampleGalton3$child,pch=19,col="blue")
lines(sampleGalton3$parent,sampleLm3$fitted,lwd=3,lty=2)
abline(lm1,col="red",lwd=3)
```

---

## Many samples

```{r manySamples,dependson="sampleGalton2",fig.height=4,fig.width=4}
sampleLm <- vector(100,mode="list")
for(i in 1:100){
  sampleGalton <- newGalton[sample(1:1e6,size=50,replace=F),]
  sampleLm[[i]] <- lm(sampleGalton$child ~ sampleGalton$parent)
}
```


---

## Many samples

```{r ,dependson="manySamples",fig.height=4,fig.width=4}
smoothScatter(newGalton$parent,newGalton$child)
for(i in 1:100){abline(sampleLm[[i]],lwd=3,lty=2)}
abline(lm1,col="red",lwd=3)
```


---

## Histogram of estimates


```{r ,dependson="manySamples",fig.height=4,fig.width=8}
par(mfrow=c(1,2))
hist(sapply(sampleLm,function(x){coef(x)[1]}),col="blue",xlab="Intercept",main="")
hist(sapply(sampleLm,function(x){coef(x)[2]}),col="blue",xlab="Slope",main="")
```

---

## Distribution of coefficients

From the [central limit theorem](https://www.khanacademy.org/math/probability/statistics-inferential/sampling_distribution/v/central-limit-theorem) it turns out that in many cases:

$$\hat{b}_0 \sim N(b_0, Var(\hat{b}_0))$$
$$\hat{b}_1 \sim N(b_1, Var(\hat{b}_1))$$

which we can estimate with:

$$\hat{b}_0 \approx N(b_0, \hat{Var}(\hat{b}_0))$$
$$\hat{b}_1 \approx N(b_1, \hat{Var}(\hat{b}_1))$$

$\sqrt{\hat{Var}(\hat{b}_0)}$ is the "standard error" of the estimate $\hat{b}_0$ and is abbreviated $S.E.(\hat{b}_0)$


---

## Estimating the values in R

```{r sampleReg,dependson="newGalton"}
sampleGalton4 <- newGalton[sample(1:1e6,size=50,replace=F),]
sampleLm4 <- lm(sampleGalton4$child ~ sampleGalton4$parent)
summary(sampleLm4)
```

---

## Estimating the values in R

```{r,dependson="sampleReg",fig.height=4,fig.width=4}
hist(sapply(sampleLm,function(x){coef(x)[2]}),col="blue",xlab="Slope",main="",freq=F)
lines(seq(0,5,length=100),dnorm(seq(0,5,length=100),mean=coef(sampleLm4)[2],
      sd=summary(sampleLm4)$coeff[2,2]),lwd=3,col="red")
```


---

## Why do we standardize?

<img class=center src=assets/img/therm.jpg height='60%'/>

$$K^{\circ} = C^{\circ} + 273.15 $$
$$K^{\circ} = \frac{F^{\circ} + 459.67}{1.8}$$
 
[http://en.wikipedia.org/wiki/Kelvin](http://en.wikipedia.org/wiki/Kelvin)


---

## Why do we standardize?


```{r ,dependson="manySamples",fig.height=4,fig.width=8}
par(mfrow=c(1,2))
hist(sapply(sampleLm,function(x){coef(x)[1]}),col="blue",xlab="Intercept",main="")
hist(sapply(sampleLm,function(x){coef(x)[2]}),col="blue",xlab="Slope",main="")
```


---

## Standardized coefficients

$$\hat{b}_0 \approx N(b_0, \hat{Var}(\hat{b}_0))$$
$$\hat{b}_1 \approx N(b_1, \hat{Var}(\hat{b}_1))$$

and

$$\frac{\hat{b}_0 - b_0}{S.E.(\hat{b}_0)} \sim t_{n-2} $$
$$\frac{\hat{b}_1 - b_1}{S.E.(\hat{b}_1)} \sim t_{n-2}$$

Degrees of Freedom $\approx$ number of samples - number of things you estimated. 


---

## $t_{n-2}$ versus $N(0,1)$

```{r,fig.height=4.5,fig.width=4.5}
x <- seq(-5,5,length=100)
plot(x,dnorm(x),type="l",lwd=3)
lines(x,dt(x,df=3),lwd=3,col="red")
lines(x,dt(x,df=10),lwd=3,col="blue")
```

---

## Confidence intervals

We have an estimate $\hat{b}_1$ and we want to know something about how good our estimate is. 

One way is to create a "level $\alpha$ confidence interval".

A confidence interval will include the real parameter $\alpha$ percent of the time in repeated studies. 


---

## Confidence intervals

$$(\hat{b}_1 - T_{\alpha/2}\times S.E.(\hat{b}_1),\hat{b}_1 + T_{\alpha/2} \times S.E.(\hat{b}_1))$$

```{r,dependson="sampleReg"}
summary(sampleLm4)$coeff
confint(sampleLm4,level=0.95)
```


---

## Confidence intervals

```{r,dependson="manySamples",fig.height=3,fig.width=3}
# ??? What does the first argument to plot do? and how does it interact with xlim
# ??? why is it even there (ie 1:10) when xlim is c(0,1.5)
par(mar=c(4,4,0,2));plot(1:10,type="n",xlim=c(0,1.5),ylim=c(0,100),
                         xlab="Coefficient Values",ylab="Replication")
for(i in 1:100){
    ci <- confint(sampleLm[[i]]); color="red";
    if((ci[2,1] < lm1$coeff[2]) & (lm1$coeff[2] < ci[2,2])){color = "grey"}
    segments(ci[2,1],i,ci[2,2],i,col=color,lwd=3)
}
lines(rep(lm1$coeff[2],100),seq(0,100,length=100),lwd=3)
```


---

## How you report the inference

```{r,dependson="sampleReg"}
sampleLm4$coeff
confint(sampleLm4,level=0.95)
```

A one inch increase in parental height is associated with a 0.77 inch increase in child's height (95% CI: 0.42-1.12 inches).

<!--=*=*=*=*-->
<!--./week4/004pValues/index.Rmd
-->
---
title       : P-values
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow  # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## P-values

* Most common measure of "statistical significance"
* Commonly reported in papers
* Used for decision making (e.g. FDA)
* Controversial among statisticians
  * [http://warnercnr.colostate.edu/~anderson/thompson1.html](http://warnercnr.colostate.edu/~anderson/thompson1.html)

---

## Not everyone thinks P-values are awful

<img class=center src=assets/img/pvals.png height='80%'/>


[http://simplystatistics.org/2012/01/06/p-values-and-hypothesis-testing-get-a-bad-rap-but-we/](http://simplystatistics.org/2012/01/06/p-values-and-hypothesis-testing-get-a-bad-rap-but-we/)

---


## What is a P-value? 

__Idea__: Suppose nothing is going on - how unusual is it to see the estimate we got?

__Approach__: 

1. Define the hypothetical distribution of a data summary (statistic) when "nothing is going on" (_null hypothesis_)
2. Calculate the summary/statistic with the data we have (_test statistic_)
3. Compare what we calculated to our hypothetical distribution and see if the value is "extreme" (_p-value_)


---

## Galton data

```{r loadGalton,fig.height=4,fig.width=4}
library(UsingR); data(galton)
plot(galton$parent,galton$child,pch=19,col="blue")
lm1 <- lm(galton$child ~ galton$parent)
abline(lm1,col="red",lwd=3)
```

If there was no relation between mid-parent/child height would we be surprised to see a line that looks like this?

---

## Null hypothesis/distribution

<br><br>
$$\frac{\hat{b}_1 - b_1}{S.E.(\hat{b}_1)} \sim t_{n-2}$$
<br><br>
__$H_0$__: That there is no relationship between parent and child height ($b_1=0$). Under the null hypothesis the distribution is:

<br><br>
$$\frac{\hat{b}_1}{S.E.(\hat{b}_1)} \sim t_{n-2}$$


---

## Null distribution

```{r, dependson="loadGalton",fig.height=4,fig.width=4}

x <- seq(-20,20,length=100)
plot(x,dt(x,df=(928-2)),col="blue",lwd=3,type="l")
```

---

## Null distribution + observed statistic

```{r, dependson="loadGalton",fig.height=4,fig.width=4}
x <- seq(-20,20,length=100)
plot(x,dt(x,df=(928-2)),col="blue",lwd=3,type="l")
arrows(summary(lm1)$coeff[2,3],0.25,summary(lm1)$coeff[2,3],0,col="red",lwd=4)
```

---

## Calculating p-values

```{r, dependson="loadGalton",fig.height=4,fig.width=4}
summary(lm1)
```


---

## A quick simulated example 

```{r simulatedExample}
set.seed(9898324)
yValues <- rnorm(10); xValues <- rnorm(10)
lm2 <- lm(yValues ~ xValues)
summary(lm2)
```


---

## A quick simulated example 

```{r, dependson="simulatedExample",fig.height=4,fig.width=4}
x <- seq(-5,5,length=100)
plot(x,dt(x,df=(10-2)),col="blue",lwd=3,type="l")
arrows(summary(lm2)$coeff[2,3],0.25,summary(lm2)$coeff[2,3],0,col="red",lwd=4)
```

---

## A quick simulated example 

```{r, dependson="simulatedExample",fig.height=4,fig.width=4}
xCoords <- seq(-5,5,length=100)
plot(xCoords,dt(xCoords,df=(10-2)),col="blue",lwd=3,type="l")
xSequence <- c(seq(summary(lm2)$coeff[2,3],5,length=10),summary(lm2)$coeff[2,3])
ySequence <- c(dt(seq(summary(lm2)$coeff[2,3],5,length=10),df=8),0)
polygon(xSequence,ySequence,col="red"); polygon(-xSequence,ySequence,col="red")
```


---

## Simulate a ton of data sets with no signal

```{r,fig.height=4,fig.width=4}
set.seed(8323); pValues <- rep(NA,100)
for(i in 1:100){
  xValues <- rnorm(20);yValues <- rnorm(20)
  pValues[i] <- summary(lm(yValues ~ xValues))$coeff[2,4]
}
hist(pValues,col="blue",main="",freq=F)
abline(h=1,col="red",lwd=3)
```

---

## Simulate a ton of data sets with signal

```{r,fig.height=4,fig.width=4}
set.seed(8323); pValues <- rep(NA,100)
for(i in 1:100){
  xValues <- rnorm(20);yValues <- 0.2 * xValues + rnorm(20)
  pValues[i] <- summary(lm(yValues ~ xValues))$coeff[2,4]
}
hist(pValues,col="blue",main="",freq=F,xlim=c(0,1)); abline(h=1,col="red",lwd=3)
```

---

## Simulate a ton of data sets with signal 

```{r,fig.height=4,fig.width=4}
set.seed(8323); pValues <- rep(NA,100)
for(i in 1:100){
  xValues <- rnorm(100);yValues <- 0.2* xValues + rnorm(100)
  pValues[i] <- summary(lm(yValues ~ xValues))$coeff[2,4]
}
hist(pValues,col="blue",main="",freq=F,xlim=c(0,1)); abline(h=1,col="red",lwd=3)
```


---

## Some typical values (single test)

* P < 0.05 (significant)
* P < 0.01 (strongly significant)
* P < 0.001 (very significant)

In modern analyses, people generally report both the confidence interval and P-value. This is less true if many many hypotheses are tested. 



---

## How you interpret the results

```{r, dependson="loadGalton",fig.height=4,fig.width=4}
summary(lm(galton$child ~ galton$parent))$coeff
```

A one inch increase in parental height is associated with a 0.77 inch increase in child's height (95% CI: 0.42-1.12 inches). This difference was statistically significant ($P < 0.001$). 


---

## Be careful!

<img class=center src=assets/img/mt1.png height='80%'/>

[http://xkcd.com/882/](http://xkcd.com/882/)

---

## Be careful!

<img class=center src=assets/img/mt2.png height='80%'/>

[http://xkcd.com/882/](http://xkcd.com/882/)
<!--=*=*=*=*-->
<!--./week4/005factorVariables/index.Rmd
-->
---
title       : Regression with factor variables
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow  # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Key ideas

* Outcome is still quantitative
* Covariate(s) are factor variables
* Fitting lines = fitting means
* Want to evaluate contribution of all factor levels at once


---

## Example: Movie ratings

<img class=center src=assets/img/rotten.png height='80%'/>

[http://www.rottentomatoes.com/](http://www.rottentomatoes.com/)


---

## Movie Data

```{r loadMovies,cache=TRUE}
download.file("http://www.rossmanchance.com/iscam2/data/movies03RT.txt",destfile="./data/movies.txt")
movies <- read.table("./data/movies.txt",sep="\t",header=T,quote="")
head(movies)
```

[http://www.rossmanchance.com/](http://www.rossmanchance.com/)

---

## Rotton tomatoes score vs. rating

```{r, dependson="loadMovies",fig.height=4,fig.width=4}
plot(movies$score ~ jitter(as.numeric(movies$rating)),col="blue",xaxt="n",pch=19)
axis(side=1,at=unique(as.numeric(movies$rating)),labels=unique(movies$rating))
```


---

## Average score by rating

```{r, dependson="loadMovies",fig.height=4,fig.width=4}
plot(movies$score ~ jitter(as.numeric(movies$rating)),col="blue",xaxt="n",pch=19)
axis(side=1,at=unique(as.numeric(movies$rating)),labels=unique(movies$rating))
meanRatings <- tapply(movies$score,movies$rating,mean)
points(1:4,meanRatings,col="red",pch="-",cex=5)
```

---

## Another way to write it down

<br><br>

$$S_i = b_0 + b_1 \mathbb{1}(Ra_i="PG") + b_2 \mathbb{1}(Ra_i="PG-13") + b_3 \mathbb{1}(Ra_i="R") + e_i $$

<br><br>

The notation $\mathbb{1}(Ra_i="PG")$ is a logical value that is one if the movie rating is "PG" and zero otherwise.


__Average values__

$b_0$ = average of the G movies

$b_0 + b_1$ = average of the PG movies

$b_0 + b_2$ = average of the PG-13 movies

$b_0 + b_3$ = average of the R movies

---

## Here is how you do it in R

```{r lmFitChunk, dependson="loadMovies",fig.height=4,fig.width=4}
lm1 <- lm(movies$score ~ as.factor(movies$rating))
summary(lm1)
```


---

## Plot fitted values

```{r, dependson="lmFitChunk",fig.height=4,fig.width=4}
plot(movies$score ~ jitter(as.numeric(movies$rating)),col="blue",xaxt="n",pch=19)
axis(side=1,at=unique(as.numeric(movies$rating)),labels=unique(movies$rating))
points(1:4,lm1$coeff[1] + c(0,lm1$coeff[2:4]),col="red",pch="-",cex=5)
```



---

## Question 1

__Average values__

$b_0$ = average of the G movies

$b_0 + b_1$ = average of the PG movies

$b_0 + b_2$ = average of the PG-13 movies

$b_0 + b_3$ = average of the R movies


__What is the average difference in rating between G and R movies?__

$b_0 + b_3 - b_0$ = $b_3$

---

## Question 1 in R 

```{r, dependson="loadMovies",fig.height=4,fig.width=4}
lm1 <- lm(movies$score ~ as.factor(movies$rating))
summary(lm1)
```

---

## Question 1 in R 

```{r, dependson="loadMovies",fig.height=4,fig.width=4}
lm1 <- lm(movies$score ~ as.factor(movies$rating))
confint(lm1)
```


---

## Question 2

__Average values__

$b_0$ = average of the G movies

$b_0 + b_1$ = average of the PG movies

$b_0 + b_2$ = average of the PG-13 movies

$b_0 + b_3$ = average of the R movies


__What is the average difference in rating between $PG-13$ and $R$ movies?__

$b_0 + b_2 - (b_0 + b_3)$ = $b_2-b_3$


---

## We could rewrite our model


$$S_i = b_0 + b_1 \mathbb{1}(Ra_i="G") + b_2 \mathbb{1}(Ra_i="PG") + b_3 \mathbb{1}(Ra_i="PG-13") + e_i $$

<br><br>


__Average values__

$b_0$ = average of the R movies

$b_0 + b_1$ = average of the G movies

$b_0 + b_2$ = average of the PG movies

$b_0 + b_3$ = average of the PG-13 movies

__What is the average difference in rating between $PG-13$ and $R$ movies?__

$b_0 + b_3 - b_0$ = $b_3$


---

## Question 2 in R


```{r, dependson="lmFitChunk",fig.height=4,fig.width=4}
lm2 <- lm(movies$score ~ relevel(movies$rating,ref="R"))
summary(lm2)
```


---

## Question 2 in R


```{r, dependson="lmFitChunk",fig.height=4,fig.width=4}
lm2 <- lm(movies$score ~ relevel(movies$rating,ref="R"))
confint(lm2)
```

---

## Question 3


<br><br>

$$S_i = b_0 + b_1 \mathbb{1}(Ra_i="PG") + b_2 \mathbb{1}(Ra_i="PG-13") + b_3 \mathbb{1}(Ra_i="R") + e_i $$

<br><br>

__Average values__

$b_0$ = average of the G movies

$b_0 + b_1$ = average of the PG movies

$b_0 + b_2$ = average of the PG-13 movies

$b_0 + b_3$ = average of the R movies

__Is there any difference in score between any of the movie ratings?__

---

## Question 3 in R

```{r, dependson="loadMovies",fig.height=4,fig.width=4}
lm1 <- lm(movies$score ~ as.factor(movies$rating))
anova(lm1)
```


---

## Sum of squares (G movies)

```{r, dependson="lmFitChunk",fig.height=4,fig.width=4}
gMovies <- movies[movies$rating=="G",]; xVals <- seq(0.2,0.8,length=4)
plot(xVals,gMovies$score,ylab="Score",xaxt="n",xlim=c(0,1),pch=19)
abline(h=mean(gMovies$score),col="blue",lwd=3); abline(h=mean(movies$score),col="red",lwd=3)
segments(xVals+0.01,rep(mean(gMovies$score),length(xVals)),xVals+0.01,
         rep(mean(movies$score),length(xVals)),col="red",lwd=2)
segments(xVals-0.01,gMovies$score,xVals-0.01,rep(mean(gMovies$score),length(xVals)),col="blue",lwd=2)
```


---

## Tukey's (honestly significant difference test) 

```{r, dependson="loadMovies",fig.height=4,fig.width=4}
lm1 <- aov(movies$score ~ as.factor(movies$rating))
TukeyHSD(lm1)
```

[http://en.wikipedia.org/wiki/Tukey's_range_test](http://en.wikipedia.org/wiki/Tukey's_range_test)
<!--=*=*=*=*-->
<!--./week4/006multipleVariables/index.Rmd
-->
---
title       : Multiple regression
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow  # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Key ideas

* Regression with multiple covariates
* Still using least squares/central limit theorem
* Interpretation depends on all variables


---

## Example - Millenium Development Goal 1

<img class=center src=assets/img/mdg1.png height='70%'/>

[http://www.un.org/millenniumgoals/pdf/MDG_FS_1_EN.pdf](http://www.un.org/millenniumgoals/pdf/MDG_FS_1_EN.pdf)

[http://apps.who.int/gho/athena/data/GHO/WHOSIS_000008.csv?profile=text&filter=COUNTRY:*;SEX:*](http://apps.who.int/gho/athena/data/GHO/WHOSIS_000008.csv?profile=text&filter=COUNTRY:*;SEX:*)

---

## WHO childhood hunger data


```{r whoDataLoad}
download.file("http://apps.who.int/gho/athena/data/GHO/WHOSIS_000008.csv?profile=text&filter=COUNTRY:*;SEX:*","./data/hunger.csv",method="curl")
hunger <- read.csv("./data/hunger.csv")
hunger <- hunger[hunger$Sex!="Both sexes",]
head(hunger)
```


---

## Plot percent hungry versus time

```{r, dependson="whoDataLoad",fig.height=4,fig.width=4}
lm1 <- lm(hunger$Numeric ~ hunger$Year)
plot(hunger$Year,hunger$Numeric,pch=19,col="blue")
```


---

## Remember the linear model

$$Hu_i = b_0 + b_1 Y_i + e_i$$

$b_0$ = percent hungry at Year 0

$b_1$ = decrease in percent hungry per year

$e_i$ = everything we didn't measure

---

## Add the linear model

```{r, dependson="whoDataLoad",fig.height=4,fig.width=4}
lm1 <- lm(hunger$Numeric ~ hunger$Year)
plot(hunger$Year,hunger$Numeric,pch=19,col="blue")
lines(hunger$Year,lm1$fitted,lwd=3,col="darkgrey")
```


---

## Color by male/female

```{r, dependson="whoDataLoad",fig.height=4,fig.width=4}
plot(hunger$Year,hunger$Numeric,pch=19)
points(hunger$Year,hunger$Numeric,pch=19,col=((hunger$Sex=="Male")*1+1))
```



---

## Now two lines

$$HuF_i = bf_0 + bf_1 YF_i + ef_i$$

$bf_0$ = percent of girls hungry at Year 0

$bf_1$ = decrease in percent of girls hungry per year

$ef_i$ = everything we didn't measure 


$$HuM_i = bm_0 + bm_1 YM_i + em_i$$

$bm_0$ = percent of boys hungry at Year 0

$bm_1$ = decrease in percent of boys hungry per year

$em_i$ = everything we didn't measure 



---

## Color by male/female

```{r, dependson="whoDataLoad",fig.height=4,fig.width=4}
lmM <- lm(hunger$Numeric[hunger$Sex=="Male"] ~ hunger$Year[hunger$Sex=="Male"])
lmF <- lm(hunger$Numeric[hunger$Sex=="Female"] ~ hunger$Year[hunger$Sex=="Female"])
plot(hunger$Year,hunger$Numeric,pch=19)
points(hunger$Year,hunger$Numeric,pch=19,col=((hunger$Sex=="Male")*1+1))
lines(hunger$Year[hunger$Sex=="Male"],lmM$fitted,col="black",lwd=3)
lines(hunger$Year[hunger$Sex=="Female"],lmF$fitted,col="red",lwd=3)
```


---

## Two lines, same slope

$$Hu_i = b_0 + b_1 \mathbb{1}(Sex_i="Male") + b_2 Y_i + e^*_i$$

$b_0$ - percent hungry at year zero for females

$b_0 + b_1$ - percent hungry at year zero for males

$b_2$ - change in percent hungry (for either males or females) in one year

$e^*_i$ - everything we didn't measure

---

## Two lines, same slope in R


```{r, dependson="whoDataLoad",fig.height=4,fig.width=4}
lmBoth <- lm(hunger$Numeric ~ hunger$Year + hunger$Sex)
plot(hunger$Year,hunger$Numeric,pch=19)
points(hunger$Year,hunger$Numeric,pch=19,col=((hunger$Sex=="Male")*1+1))
abline(c(lmBoth$coeff[1],lmBoth$coeff[2]),col="red",lwd=3)
# XXX hunger$Sex is 1 for boys so effectively the intercept is coeff[1] + coeff[3] (coeff for boys)
abline(c(lmBoth$coeff[1] + lmBoth$coeff[3],lmBoth$coeff[2] ),col="black",lwd=3)
```





---

## Two lines, different slopes (interactions)

$$Hu_i = b_0 + b_1 \mathbb{1}(Sex_i="Male") + b_2 Y_i + b_3 \mathbb{1}(Sex_i="Male")\times Y_i + e^+_i$$

$b_0$ - percent hungry at year zero for females

$b_0 + b_1$ - percent hungry at year zero for males

$b_2$ - change in percent hungry (females) in one year

$b_2 + b_3$ - change in percent hungry (males) in one year

$e^+_i$ - everything we didn't measure

---

## Two lines, different slopes in R


```{r lmBothChunk, dependson="whoDataLoad",fig.height=4,fig.width=4}
lmBoth <- lm(hunger$Numeric ~ hunger$Year + hunger$Sex + hunger$Sex*hunger$Year)
plot(hunger$Year,hunger$Numeric,pch=19)
points(hunger$Year,hunger$Numeric,pch=19,col=((hunger$Sex=="Male")*1+1))
abline(c(lmBoth$coeff[1],lmBoth$coeff[2]),col="red",lwd=3)
abline(c(lmBoth$coeff[1] + lmBoth$coeff[3],lmBoth$coeff[2] +lmBoth$coeff[4]),col="black",lwd=3)
```


---

## Two lines, different slopes in R


```{r, dependson="lmBothChunk",fig.height=4,fig.width=4}
summary(lmBoth)
```

---

## Interactions for continuous variables

$$Hu_i = b_0 + b_1 In_i + b_2 Y_i + b_3 In_i \times Y_i + e^+_i$$

$b_0$ - percent hungry at year zero for children with whose parents have no income

$b_1$ - change in percent hungry for each dollar of income in year zero

$b_2$ - change in percent hungry in one year for children whose parents have no income

$b_3$ - increased change in percent hungry by year for each dollar of income  - e.g. if income is $10,000, then change in percent hungry in one year will be

$$b_2 + 1e4 \times b_3$$

$e^+_i$ - everything we didn't measure

__Lot's of care/caution needed!__




<!--=*=*=*=*-->
<!--./week4/007realData/index.Rmd
-->
---
title       : Regression in the real world
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow  # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```


## Things to pay attention to

* Confounders
* Complicated interactions
* Skewness
* Outliers
* Non-linear patterns
* Variance changes
* Units/scale issues
* Overloading regression
* Correlation and causation


---

## The ideal data for regression

```{r,fig.height=4,fig.width=4}
library(UsingR); data(galton)
plot(galton$parent,galton$child,col="blue",pch=19)

```

---

## Confounders

__Confounder__: A variable that is correlated with both the outcome and the covariates


* Confounders can change the regression line
* They can even change the sign of the line
* They can sometimes be detected by careful exploration

---

## Example - Millenium Development Goal 1

<img class=center src=assets/img/mdg1.png height='70%'/>

[http://www.un.org/millenniumgoals/pdf/MDG_FS_1_EN.pdf](http://www.un.org/millenniumgoals/pdf/MDG_FS_1_EN.pdf)

---

## WHO childhood hunger data

```{r whoDataLoad,cache=TRUE}
download.file("http://apps.who.int/gho/athena/data/GHO/WHOSIS_000008.csv?profile=text&filt
              er=COUNTRY:*;SEX:*","./data/hunger.csv",method="curl")
hunger <- read.csv("./data/hunger.csv")
hunger <- hunger[hunger$Sex!="Both sexes",]
head(hunger)
```

---

## Hunger over time by region

```{r dependson="whoDataLoad",fig.height=4,fig.width=8}
par(mfrow=c(1,2))
plot(hunger$Year,hunger$Numeric,col=as.numeric(hunger$WHO.region),pch=19)
plot(1:10,type="n",xaxt="n",yaxt="n",xlab="",ylab="")
legend(1,10,col=unique(as.numeric(hunger$WHO.region)),legend=unique(hunger$WHO.region),pch=19)
```


---

## Region correlated with year

```{r dependson="whoDataLoad",fig.height=4,fig.width=4}
anova(lm(hunger$Year ~ hunger$WHO.region))
```


---

## Region correlated with hunger

```{r dependson="whoDataLoad",fig.height=4,fig.width=4}
anova(lm(hunger$Numeric ~ hunger$WHO.region))
```


---

## Including region - a complicated interaction

```{r, dependson="whoDataLoad",fig.height=4,fig.width=4}
plot(hunger$Year,hunger$Numeric,pch=19,col=as.numeric(hunger$WHO.region))
lmRegion <- lm(hunger$Numeric ~ hunger$Year + hunger$WHO.region + hunger$Year*hunger$WHO.region )
abline(c(lmRegion$coeff[1] + lmRegion$coeff[6],lmRegion$coeff[2]+ lmRegion$coeff[12]),col=5,lwd=3)

```


---

## Income data 

```{r downloadIncomeData,cache=TRUE}
download.file("http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data","./data/income.csv",method="curl")
incomeData <- read.csv("./data/income.csv",header=FALSE)
income <- incomeData[,3]
age <- incomeData[,1]
```

[http://archive.ics.uci.edu/ml/datasets/Census+Income](http://archive.ics.uci.edu/ml/datasets/Census+Income)

---

## Logs to address right-skew

```{r dependson="downloadIncomeData",fig.height=4,fig.width=10}
par(mfrow=c(1,4))
smoothScatter(age,income)
hist(income,col="blue",breaks=100)
hist(log(income+1),col="blue",breaks=100)
smoothScatter(age,log(income+1))
```

[Data transforms](http://en.wikipedia.org/wiki/Data_transformation_(statistics))

---

## Outliers

<br><br><br><br>

<center> "outliers" ... are data points that do not appear to follow the pattern of the other data  points.</center>

<br><br><br><br>

[A dataset that is 44% outliers](http://www.amstat.org/publications/jse/v13n1/datasets.hayden.html)



---

## Example - extreme points

```{r,fig.height=4,fig.width=4}
set.seed(1235)
xVals <- rcauchy(50)
hist(xVals,col="blue")
```


---

## Example - Outliers may be real

```{r dependson="downloadIncomeData",fig.height=4,fig.width=4}
# Add Tim Cook, CEO of Apple 2011 income
age <- c(age,52)
income <- c(income,378e6)
smoothScatter(age,income)
```
[http://www.macworld.com/article/2023491/apple-gives-tim-cook-51-percent-salary-increase.html](http://www.macworld.com/article/2023491/apple-gives-tim-cook-51-percent-salary-increase.html)

---

## Example - Does not fit the trend


<img class=center src=assets/img/hayden.png height='80%'/>

[A dataset that is 44% outliers](http://www.amstat.org/publications/jse/v13n1/datasets.hayden.html)


---

## Outliers - what you can do

* If you know they aren't real/of interest, remove them (but changes question!)
* Alternatively
  * Sensitivity analysis - is it a big difference if you leave it in/take it out?
  * Logs - if the data are right skewed (lots of outliers)
  * Robust methods - we've been doing averages, but there are more robust approaches ([Robust](http://cran.r-project.org/web/views/Robust.html),[rlm](http://stat.ethz.ch/R-manual/R-patched/library/MASS/html/rlm.html))
  



---

## A line isn't always the best summary

<img class=center src=assets/img/anscombe.png height='80%'/>

[http://en.wikipedia.org/wiki/Linear_regression](http://en.wikipedia.org/wiki/Linear_regression)


---

## You can end up saying some pretty silly stuff

<img class=center src=assets/img/sprint.png height='30%'/>

[http://www.nature.com/nature/journal/v431/n7008/fig_tab/431525a_F1.html](http://www.nature.com/nature/journal/v431/n7008/fig_tab/431525a_F1.html)

"We are students aged 16–18 in a Texas high school. Our biology teacher Vidya Rajan asked us to comment on the paper by A. J. Tatem and colleagues (Nature 431, 525; 2004); we believe the projection on which it is based is riddled with flaws..."
[http://www.nature.com/nature/journal/v432/n7014/full/432147c.html](http://www.nature.com/nature/journal/v432/n7014/full/432147c.html)

"They omit to mention, however, that (according to their analysis) a far more interesting race should occur in about 2636, when times of less than zero seconds will be recorded"
[http://www.nature.com/nature/journal/v432/n7014/full/432147b.html](http://www.nature.com/nature/journal/v432/n7014/full/432147b.html)


---

## Variance changes 

```{r bupaDataChunk,fig.height=4,fig.width=4}
bupaData <- read.csv("ftp://ftp.ics.uci.edu/pub/machine-learning-databases/liver-disorders/bupa.data",header=F)
ggt <- bupaData[,5]; aat <- bupaData[,3]
plot(log(ggt),aat,col="blue",pch=19)
```


---

## Plot the residuals

```{r, dependson="bupaDataChunk",fig.height=4,fig.width=4}
lm1 <- lm(aat ~ log(ggt))
plot(log(ggt),lm1$residuals,col="blue",pch=19)
```


[Power (a.k.a. Box-Cox) transform](http://en.wikipedia.org/wiki/Power_transform#cite_note-2)

---

## Changing variance - what you can do

* There is a long literature on this problem (heteroskedasticity)
* A few examples
  * [Box-Cox Transform](http://en.wikipedia.org/wiki/Power_transform#cite_note-2)
  * [Variance stabilizing transform](http://en.wikipedia.org/wiki/Variance-stabilizing_transformation)
  * [Weighted least squares](http://en.wikipedia.org/wiki/Weighted_least_squares#Weighted_least_squares)
  * [Huber-white standard errors](http://en.wikipedia.org/wiki/Heteroscedasticity-consistent_standard_errors)

---

## Variation in units

<img class=center src=assets/img/alldeaths.png height='80%'/>

[All Deaths](http://www.gapminder.org/world/#$majorMode=chart$is;shi=t;ly=2003;lb=f;il=t;fs=11;al=30;stl=t;st=t;nsl=t;se=t$wst;tts=C$ts;sp=5.59290322580644;ti=2008$zpv;v=0$inc_x;mmid=XCOORDS;iid=phAwcNAVuyj1jiMAkmq1iMg;by=ind$inc_y;mmid=YCOORDS;iid=taYeez4Mkk8OvdH-q4QAxxQ;by=ind$inc_s;uniValue=8.21;iid=phAwcNAVuyj0XOoBL_n5tAQ;by=ind$inc_c;uniValue=255;gid=CATID0;by=grp$map_x;scale=log;dataMin=194;dataMax=96846$map_y;scale=lin;dataMin=4;dataMax=826060$map_s;sma=50;smi=2$cd;bd=0$inds=)

---

## Relative units 

<img class=center src=assets/img/relativedeaths.png height='80%'/>

[Per 1000 Deaths](http://www.gapminder.org/world/#$majorMode=chart$is;shi=t;ly=2003;lb=f;il=t;fs=11;al=30;stl=t;st=t;nsl=t;se=t$wst;tts=C$ts;sp=5.59290322580644;ti=2008$zpv;v=0$inc_x;mmid=XCOORDS;iid=phAwcNAVuyj1jiMAkmq1iMg;by=ind$inc_y;mmid=YCOORDS;iid=t9FP3_OPrE2ug_I_iNsbePg;by=ind$inc_s;uniValue=8.21;iid=phAwcNAVuyj0XOoBL_n5tAQ;by=ind$inc_c;uniValue=255;gid=CATID0;by=grp$map_x;scale=log;dataMin=194;dataMax=96846$map_y;scale=lin;dataMin=1.027;dataMax=195$map_s;sma=50;smi=2$cd;bd=0$inds=)



---

## When there is variation in units

* Standardize, but keep track
  * Affects model fits
  * Affects interpretation
  * Affects inference

---

## Overloading regression

<img class=center src=assets/img/overload.png height='30%'/>

[http://bit.ly/YiB5Um](http://bit.ly/YiB5Um)
[http://wmbriggs.com/blog/?p=7026](http://wmbriggs.com/blog/?p=7026)


---

## Correlation and Causation

<img class=center src=assets/img/correlation.png height='60%'/>


[http://xkcd.com/552/](http://xkcd.com/552/)

---

## Even when looking for associations

<img class=center src=assets/img/chocolate.png height='80%'/>

[http://www.nejm.org/doi/full/10.1056/NEJMon1211064](ttp://www.nejm.org/doi/full/10.1056/NEJMon1211064)

---

## Again, it can get silly


<img class=center src=assets/img/lumley.png height='80%'/>

[http://www.statschat.org.nz/2012/10/12/even-better-than-chocolate/](http://www.statschat.org.nz/2012/10/12/even-better-than-chocolate/)

---

## Correlation vs. Causation

* Use caution when interpreting regression results
* Be critical of surprising associations
* Consider alternative explanations


<!--=*=*=*=*-->
<!--./week5/001multipleFactors/index.Rmd
-->
---
title       : ANOVA with multiple factors/variables
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow  # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Key ideas

* Outcome is still quantitative
* You have multiple explanatory variables
* Goal is to identify contributions of different variables

---

## A successful example

<img class=center src=assets/img/wired.png height='40%'/>

"For the button, an A/B test of three new word choices—”Learn More,” “Join Us Now,” and “Sign Up Now”—revealed that “Learn More” garnered 18.6 percent more signups per visitor than the default of “Sign Up.” Similarly, a black-and-white photo of the Obama family outperformed the default turquoise image by 13.1 percent. Using both the family image and “Learn More,” signups increased by a thundering 40 percent."

[http://www.wired.com/business/2012/04/ff_abtesting/](http://www.wired.com/business/2012/04/ff_abtesting/)

---

## Movie Data

```{r loadMovies,cache=FALSE}
download.file("http://www.rossmanchance.com/iscam2/data/movies03RT.txt",
              destfile="./data/movies.txt")
movies <- read.table("./data/movies.txt",sep="\t",header=T,quote="")
head(movies)
```

[http://www.rossmanchance.com/](http://www.rossmanchance.com/)


---

## Relating score to rating

<br><br>

$$S_i = b_0 + b_1 \mathbb{1}(Ra_i="PG") + b_2 \mathbb{1}(Ra_i="PG-13") + b_3 \mathbb{1}(Ra_i="R") + e_i $$

<br><br>

The notation $\mathbb{1}(Ra_i="PG")$ is a logical value that is one if the movie rating is "PG" and zero otherwise.


__Average values__

$b_0$ = average of the G movies

$b_0 + b_1$ = average of the PG movies

$b_0 + b_2$ = average of the PG-13 movies

$b_0 + b_3$ = average of the R movies

---

## ANOVA in R


```{r anovaMovies, dependson="loadMovies",cache=FALSE}
aovObject <- aov(movies$score ~ movies$rating)
aovObject
```

---

## ANOVA in R

```{r, dependson="anovaMovies"}
aovObject$coeff
```



---

## Adding a second factor

<br><br>

$$S_i = b_0 + b_1 \mathbb{1}(Ra_i="PG") + b_2 \mathbb{1}(Ra_i="PG-13") + b_3 \mathbb{1}(Ra_i="R")$$
$$+ \gamma_1 \mathbb{1}(G_i="action") +  \gamma_2 \mathbb{1}(G_i="animated") + ... + e_i$$

<br><br>

The notation $\mathbb{1}(Ra_i="PG")$ is a logical value that is one if the movie rating is "PG" and zero otherwise. 



---

## Adding a second factor

<br><br>

$$S_i = b_0 + \underbrace{b_1 \mathbb{1}(Ra_i="PG") + b_2 \mathbb{1}(Ra_i="PG-13") + b_3 \mathbb{1}(Ra_i="R")}_{rating}$$
$$+ \gamma_1 \underbrace{\mathbb{1}(G_i="action")  + \gamma_2 \mathbb{1}(G_i="animated") + ...}_{genre} + e_i $$

<br><br>

There are only 2 variables in this model. They have multiple levels. 


---

## Second variable


```{r twoVariableANOVA, dependson="loadMovies"}
aovObject2 <- aov(movies$score ~ movies$rating + movies$genre)
aovObject2
```

---

## ANOVA Summary


```{r, dependson="twoVariableANOVA"}
summary(aovObject2)
```

---

## Order matters

```{r, dependson="loadMovies"}
aovObject3 <- aov(movies$score ~ movies$genre + movies$rating)
summary(aovObject3)
summary(aovObject2)
```

---

## Adding a quantitative variable

<br><br>

$$S_i = b_0 + \underbrace{b_1 \mathbb{1}(Ra_i="PG") + b_2 \mathbb{1}(Ra_i="PG-13") + b_3 \mathbb{1}(Ra_i="R")}_{rating}$$
$$+ \gamma_1 \underbrace{\mathbb{1}(G_i="action")  + \gamma_2 \mathbb{1}(G_i="animated") + ...}_{genre} + \eta_1 \underbrace{BO_i}_{box\; office} + e_i $$

<br><br>

There are three variables in this model - box office is quantitative so only has one term. 

---

## ANOVA with quantitative variable in R

```{r, dependson="loadMovies"}
aovObject4 <- aov(movies$score ~ movies$genre + movies$rating + movies$box.office)
summary(aovObject4)
```


---

## Language and further resources

* Units - one observation
* Treatments - applied to units
* Factors - controlled by experimenters
* Replicates - multiple (independent) units with the same factors/treatments

* [Wikipedia on Experimental Design](http://en.wikipedia.org/wiki/Design_of_experiments)
* [Wikipedia on ANOVA](http://en.wikipedia.org/wiki/Analysis_of_variance)
* [Wikipedia on A/B Testing](http://en.wikipedia.org/wiki/A/B_testing)










<!--=*=*=*=*-->
<!--./week5/002binaryOutcomes/index.Rmd
-->
---
title       : Binary outcomes
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow  # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Key ideas

* Frequently we care about outcomes that have two values
  * Alive/dead
  * Win/loss
  * Success/Failure
  * etc
* Called binary outcomes or 0/1 outcomes 
* Linear regression (like we've seen) may not be the best

---

## Example: Baltimore Ravens

<img class=center src=assets/img/ravens.png height='60%'/>

[http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens](http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens)

---

## Ravens Data

```{r loadRavens,cache=TRUE}
download.file("https://dl.dropbox.com/u/7710864/data/ravensData.rda",
              destfile="./data/ravensData.rda",method="curl")
load("./data/ravensData.rda")
head(ravensData)
```

---

## Linear regression

$$ RW_i = b_0 + b_1 RS_i + e_i $$

$RW_i$ - 1 if a Ravens win, 0 if not

$RS_i$ - Number of points Ravens scored

$b_0$ - probability of a Ravens win if they score 0 points

$b_1$ - increase in probability of a Ravens win for each additional point

$e_i$ - variation due to everything we didn't measure


---

## Linear regression in R

```{r linearReg, dependson = "loadRavens"}
lmRavens <- lm(ravensData$ravenWinNum ~ ravensData$ravenScore)
summary(lmRavens)
```


---

## Linear regression

```{r dependson = "linearReg", fig.height=4.5,fig.width=4.5}
plot(ravensData$ravenScore,lmRavens$fitted,pch=19,col="blue",ylab="Prob Win",xlab="Raven Score")
```

---

## Odds

__Binary Outcome 0/1__

$$RW_i$$  

__Probability (0,1)__

$$\rm{Pr}(RW_i | RS_i, b_0, b_1 )$$


__Odds $(0,\infty)$__
$$\frac{\rm{Pr}(RW_i | RS_i, b_0, b_1 )}{1-\rm{Pr}(RW_i | RS_i, b_0, b_1)}$$ 

__Log odds $(-\infty,\infty)$__

$$\log\left(\frac{\rm{Pr}(RW_i | RS_i, b_0, b_1 )}{1-\rm{Pr}(RW_i | RS_i, b_0, b_1)}\right)$$ 

---

## Linear vs. logistic regression

__Linear__

$$ RW_i = b_0 + b_1 RS_i + e_i $$

or

$$ E[RW_i | RS_i, b_0, b_1] = b_0 + b_1 RS_i$$

__Logistic__

$$ \rm{Pr}(RW_i | RS_i, b_0, b_1) = \frac{\exp(b_0 + b_1 RS_i)}{1 + \exp(b_0 + b_1 RS_i)}$$

or

$$ \log\left(\frac{\rm{Pr}(RW_i | RS_i, b_0, b_1 )}{1-\rm{Pr}(RW_i | RS_i, b_0, b_1)}\right) = b_0 + b_1 RS_i $$


---

## Interpreting Logistic Regression

$$ \log\left(\frac{\rm{Pr}(RW_i | RS_i, b_0, b_1 )}{1-\rm{Pr}(RW_i | RS_i, b_0, b_1)}\right) = b_0 + b_1 RS_i $$


$b_0$ - Log odds of a Ravens win if they score zero points

$b_1$ - Log odds ratio of win probability for each point scored (compared to zero points)

$\exp(b_1)$ - Odds ratio of win probability for each point scored (compared to zero points)

---

## Explaining Odds

<img class=center src=assets/img/odds1.png height='60%'/>

[via Ken Rice](http://faculty.washington.edu/kenrice/teaching.html)

---

## Probability of Death

<img class=center src=assets/img/odds2.png height='60%'/>

[via Ken Rice](http://faculty.washington.edu/kenrice/teaching.html)

---

## Odds of Death

<img class=center src=assets/img/odds3.png height='60%'/>

[via Ken Rice](http://faculty.washington.edu/kenrice/teaching.html)

---

## Odds Ratio = 1, Continuous Covariate

<img class=center src=assets/img/odds4.png height='60%'/>

[via Ken Rice](http://faculty.washington.edu/kenrice/teaching.html)

---

## Different odds ratios

<img class=center src=assets/img/odds5.png height='60%'/>

[via Ken Rice](http://faculty.washington.edu/kenrice/teaching.html)

---

## Ravens logistic regression

```{r logReg, dependson = "loadRavens"}
logRegRavens <- glm(ravensData$ravenWinNum ~ ravensData$ravenScore,family="binomial")
summary(logRegRavens)
```


---

## Ravens fitted values

```{r dependson = "logReg",fig.height=4,fig.width=4}
plot(ravensData$ravenScore,logRegRavens$fitted,pch=19,col="blue",xlab="Score",ylab="Prob Ravens Win")
```


---

## Odds ratios and confidence intervals

```{r dependson = "logReg",fig.height=4,fig.width=4}
exp(logRegRavens$coeff)
exp(confint(logRegRavens))
```


---

## ANOVA for logistic regression

```{r dependson = "logReg",fig.height=4,fig.width=4}
anova(logRegRavens,test="Chisq")
```



---

## Simpson's paradox

<img class=center src=assets/img/simpsons.png height='60%'/>

[http://en.wikipedia.org/wiki/Simpson's_paradox](http://en.wikipedia.org/wiki/Simpson's_paradox)

---

## Interpreting Odds Ratios

* Not probabilities 
* Odds ratio of 1 = no difference in odds
* Log odds ratio of 0 = no difference in odds
* Odds ratio < 0.5 or > 2 commonly a "moderate effect"
* Relative risk $\frac{\rm{Pr}(RW_i | RS_i = 10)}{\rm{Pr}(RW_i | RS_i = 0)}$ often easier to interpret, harder to estimate
* For small probabilities RR $\approx$ OR but __they are not the same__!

[Wikipedia on Odds Ratio](http://en.wikipedia.org/wiki/Odds_ratio)


---

## Further resources

* [Wikipedia on Logistic Regression](http://en.wikipedia.org/wiki/Logistic_regression)
* [Logistic regression and glms in R](http://data.princeton.edu/R/glms.html)
* Brian Caffo's lecture notes on: [Simpson's paradox](http://ocw.jhsph.edu/courses/MethodsInBiostatisticsII/PDFs/lecture23.pdf), [Case-control studies](http://ocw.jhsph.edu/courses/MethodsInBiostatisticsII/PDFs/lecture24.pdf)
* [Open Intro Chapter on Logistic Regression](http://www.openintro.org/stat/down/oiStat2_08.pdf)





<!--=*=*=*=*-->
<!--./week5/003countOutcomes/index.Rmd
-->
---
title       : Count outcomes
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow  # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Key ideas

* Many data take the form of counts
  * Calls to a call center
  * Number of flu cases in an area
  * Number of cars that cross a bridge
* Data may also be in the form of rates
  * Percent of children passing a test
  * Percent of hits to a website from a country
* Linear regression with transformation is an option

---

## Poisson distribution

```{r simPois,fig.height=4,fig.width=8, cache=TRUE}
set.seed(3433); par(mfrow=c(1,2))
poisData2 <- rpois(100,lambda=100); poisData1 <- rpois(100,lambda=50)
hist(poisData1,col="blue",xlim=c(0,150)); hist(poisData2,col="blue",xlim=c(0,150))
```

---

## Poisson distribution


```{r dependson="simPois"}
c(mean(poisData1),var(poisData1))
c(mean(poisData2),var(poisData2))
```

---

## Example: Leek Group Website Traffic

<img class=center src=assets/img/leekgroup.png height='60%'/>

[http://biostat.jhsph.edu/~jleek/](http://biostat.jhsph.edu/~jleek/)

---

## Website data

```{r leekLoad,cache=TRUE}
download.file("https://dl.dropbox.com/u/7710864/data/gaData.rda",destfile="./data/gaData.rda",method="curl")
load("./data/gaData.rda")
gaData$julian <- julian(gaData$date)
head(gaData)
```

[http://skardhamar.github.com/rga/](http://skardhamar.github.com/rga/)


---

## Plot data

```{r, dependson="leekLoad",fig.height=4.5,fig.width=4.5}
plot(gaData$julian,gaData$visits,pch=19,col="darkgrey",xlab="Julian",ylab="Visits")
```


---

## Linear regression

$$ NH_i = b_0 + b_1 JD_i + e_i $$

$NH_i$ - number of hits to the website

$JD_i$ - day of the year (Julian day)

$b_0$ - number of hits on Julian day 0 (1970-01-01)

$b_1$ - increase in number of hits per unit day

$e_i$ - variation due to everything we didn't measure


---

## Linear regression line

```{r linReg, dependson="leekLoad",fig.height=4,fig.width=4, cache=TRUE}
plot(gaData$julian,gaData$visits,pch=19,col="darkgrey",xlab="Julian",ylab="Visits")
lm1 <- lm(gaData$visits ~ gaData$julian)
abline(lm1,col="red",lwd=3)
```


---

## Linear vs. Poisson regression

__Linear__

$$ NH_i = b_0 + b_1 JD_i + e_i $$

or

$$ E[NH_i | JD_i, b_0, b_1] = b_0 + b_1 JD_i$$

__Poisson/log-linear__

$$ \log\left(E[NH_i | JD_i, b_0, b_1]\right) = b_0 + b_1 JD_i $$

or

$$ E[NH_i | JD_i, b_0, b_1] = \exp\left(b_0 + b_1 JD_i\right) $$


---

## Multiplicative differences

<br><br>
$$ E[NH_i | JD_i, b_0, b_1] = \exp\left(b_0 + b_1 JD_i\right) $$

<br><br>

$$ E[NH_i | JD_i, b_0, b_1] = \exp\left(b_0 \right)\exp\left(b_1 JD_i\right) $$

<br><br>

If $JD_i$ is increased by one unit, $E[NH_i | JD_i, b_0, b_1]$ is multiplied by $\exp\left(b_1\right)$

---

## Poisson regression in R

```{r poisReg, dependson="linReg",fig.height=4.5,fig.width=4.5, cache=TRUE}
plot(gaData$julian,gaData$visits,pch=19,col="darkgrey",xlab="Julian",ylab="Visits")
glm1 <- glm(gaData$visits ~ gaData$julian,family="poisson")
abline(lm1,col="red",lwd=3); lines(gaData$julian,glm1$fitted,col="blue",lwd=3)
```


---

## Mean-variance relationship?

```{r, dependson="poisReg",fig.height=4.5,fig.width=4.5}
plot(glm1$fitted,glm1$residuals,pch=19,col="grey",ylab="Residuals",xlab="Date")
```

---

## Model agnostic standard errors 

```{r agnostic, dependson="poisReg",fig.height=4.5,fig.width=4.5,cache=TRUE}
library(sandwich)
confint.agnostic <- function (object, parm, level = 0.95, ...)
{
    cf <- coef(object); pnames <- names(cf)
    if (missing(parm))
        parm <- pnames
    else if (is.numeric(parm))
        parm <- pnames[parm]
    a <- (1 - level)/2; a <- c(a, 1 - a)
    pct <- stats:::format.perc(a, 3)
    fac <- qnorm(a)
    ci <- array(NA, dim = c(length(parm), 2L), dimnames = list(parm,
                                                               pct))
    ses <- sqrt(diag(sandwich::vcovHC(object)))[parm]
    ci[] <- cf[parm] + ses %o% fac
    ci
}
```
[http://stackoverflow.com/questions/3817182/vcovhc-and-confidence-interval](http://stackoverflow.com/questions/3817182/vcovhc-and-confidence-interval)

---

## Estimating confidence intervals

```{r, dependson="agnostic",fig.height=4.5,fig.width=4.5}
confint(glm1)
confint.agnostic(glm1)
```


---

## Rates 


<br><br>


$$ E[NHSS_i | JD_i, b_0, b_1]/NH_i = \exp\left(b_0 + b_1 JD_i\right) $$

<br><br>

$$ \log\left(E[NHSS_i | JD_i, b_0, b_1]\right) - \log(NH_i)  =  b_0 + b_1 JD_i $$

<br><br>

$$ \log\left(E[NHSS_i | JD_i, b_0, b_1]\right) = \log(NH_i) + b_0 + b_1 JD_i $$

---

## Fitting rates in R 

```{r ratesFit,dependson="agnostic", cache=TRUE,fig.height=4,fig.width=4}
glm2 <- glm(gaData$simplystats ~ julian(gaData$date),offset=log(visits+1),
            family="poisson",data=gaData)
plot(julian(gaData$date),glm2$fitted,col="blue",pch=19,xlab="Date",ylab="Fitted Counts")
points(julian(gaData$date),glm1$fitted,col="red",pch=19)
```

---

## Fitting rates in R 

```{r,dependson="ratesFit",fig.height=4,fig.width=4}
glm2 <- glm(gaData$simplystats ~ julian(gaData$date),offset=log(visits+1),
            family="poisson",data=gaData)
plot(julian(gaData$date),gaData$simplystats/(gaData$visits+1),col="grey",xlab="Date",
     ylab="Fitted Rates",pch=19)
lines(julian(gaData$date),glm2$fitted/(gaData$visits+1),col="blue",lwd=3)
```

---

## More information

* [Log-linear models and multiway tables](http://ww2.coastal.edu/kingw/statistics/R-tutorials/loglin.html)
* [Wikipedia on Poisson regression](http://en.wikipedia.org/wiki/Poisson_regression), [Wikipedia on overdispersion](http://en.wikipedia.org/wiki/Overdispersion)
* [Regression models for count data in R](http://cran.r-project.org/web/packages/pscl/vignettes/countreg.pdf)
* [pscl package](http://cran.r-project.org/web/packages/pscl/index.html) - the function _zeroinfl_ fits zero inflated models. 

<!--=*=*=*=*-->
<!--./week5/004modelChecking/index.Rmd
-->
---
title       : Model checking and model selection
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow  # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Model checking and model selection

* Sometimes model checking/selection not allowed
* Often it can lead to problems
  * Overfitting
  * Overtesting
  * Biased inference
* _But_ you don't want to miss something obvious

---

## Linear regression - basic assumptions

* Variance is constant
* You are summarizing a linear trend
* You have all the right terms in the model
* There are no big outliers

---

## Model checking - constant variance

```{r, fig.height=4,fig.width=8}
set.seed(3433); par(mfrow=c(1,2)) 
data <- rnorm(100,mean=seq(0,3,length=100),sd=seq(0.1,3,length=100))
lm1 <- lm(data ~ seq(0,3,length=100))
plot(seq(0,3,length=100),data,pch=19,col="grey"); abline(lm1,col="red",lwd=3)
plot(seq(0,3,length=100),lm1$residuals,,pch=19,col="grey"); abline(c(0,0),col="red",lwd=3)
```

---

## What to do

* See if another variable explains the increased variance
* Use the  _vcovHC_ {sandwich} variance estimators (if n is big)


---


## Using the sandwich estimate

```{r, fig.height=4,fig.width=8}
set.seed(3433); par(mfrow=c(1,2)); data <- rnorm(100,mean=seq(0,3,length=100),sd=seq(0.1,3,length=100))
lm1 <- lm(data ~ seq(0,3,length=100))
vcovHC(lm1)
summary(lm1)$cov.unscaled
```


---

## Model checking - linear trend

```{r, fig.height=4,fig.width=8}
set.seed(3433); par(mfrow=c(1,2)) 
data <- rnorm(100,mean=seq(0,3,length=100)^3,sd=2)
lm1 <- lm(data ~ seq(0,3,length=100))
plot(seq(0,3,length=100),data,pch=19,col="grey"); abline(lm1,col="red",lwd=3)
plot(seq(0,3,length=100),lm1$residuals,,pch=19,col="grey"); abline(c(0,0),col="red",lwd=3)
```

---

## What to do

* Use Poisson regression (if it looks exponential/multiplicative)
* Use a data transformation (e.g. take the log)
* Smooth the data/fit a nonlinear trend (next week's lectures)
* Use linear regression anyway
  * Interpret as the linear trend between the variables
  * Use the  _vcovHC_ {sandwich} variance estimators (if n is big)


---

## Model checking - missing covariate

```{r, fig.height=4,fig.width=10}
set.seed(3433); par(mfrow=c(1,3)); z <- rep(c(-0.5,0.5),50)
data <- rnorm(100,mean=(seq(0,3,length=100) + z),sd=seq(0.1,3,length=100))
lm1 <- lm(data ~ seq(0,3,length=100))
plot(seq(0,3,length=100),data,pch=19,col=((z>0)+3)); abline(lm1,col="red",lwd=3)
plot(seq(0,3,length=100),lm1$residuals,pch=19,col=((z>0)+3)); abline(c(0,0),col="red",lwd=3)
boxplot(lm1$residuals ~ z,col = ((z>0)+3) )
```


---

## What to do

* Use exploratory analysis to identify other variables to include
* Use the  _vcovHC_ {sandwich} variance estimators (if n is big)
* Report unexplained patterns in the data

---

## Model checking - outliers

```{r, fig.height=4,fig.width=10}
set.seed(343); par(mfrow=c(1,2)); betahat <- rep(NA,100)
x <- seq(0,3,length=100); y <- rcauchy(100); lm1 <- lm(y ~ x)
plot(x,y,pch=19,col="blue"); abline(lm1,col="red",lwd=3)
for(i in 1:length(data)){betahat[i] <- lm(y[-i] ~ x[-i])$coeff[2]}
plot(betahat - lm1$coeff[2],col="blue",pch=19); abline(c(0,0),col="red",lwd=3)
```

---

## What to do

* If outliers are experimental mistakes -remove and document them
* If they are real - consider reporting how sensitive your estimate is to the outliers
* Consider using a robust linear model fit like _rlm_ {MASS}


---

## Robust linear modeling

```{r robustLm, fig.height=4,fig.width=10}
set.seed(343); x <- seq(0,3,length=100); y <- rcauchy(100); 
lm1 <- lm(y ~ x); rlm1 <- rlm(y ~ x)
lm1$coeff
rlm1$coeff
```

---

## Robust linear modeling

```{r robustReg, fig.height=4,fig.width=10}
par(mfrow=c(1,2))
plot(x,y,pch=19,col="grey")
lines(x,lm1$fitted,col="blue",lwd=3); lines(x,rlm1$fitted,col="green",lwd=3)
plot(x,y,pch=19,col="grey",ylim=c(-5,5),main="Zoomed In")
lines(x,lm1$fitted,col="blue",lwd=3); lines(x,rlm1$fitted,col="green",lwd=3)
```


---

## Model checking - default plots


```{r, fig.height=4,fig.width=10}
set.seed(343); par(mfrow=c(1,2))
x <- seq(0,3,length=100); y <- rnorm(100); lm1 <- lm(y ~ x)
plot(lm1)
# ??? deviance in glm, difference from R squared, BIC, AIC
```

---

## Model checking - deviance

* Commonly reported for GLM's
* Usually compares the model where every point gets its own parameter to the model you are using
* On it's own it doesn't tell you what is wrong
* In large samples the deviance may be big even for "conservative" models
* You can not compare deviances for models with different sample sizes


---

## $R^2$ may be a bad summary

<img class=center src=assets/img/anscombe.png height='80%'/>


---

## Model selection 

* Many times you have multiple variables to evaluate
* Options for choosing variables
  * Domain-specific knowledge
  * Exploratory analysis
  * Statistical selection
* There are many statistical selection options
  * Step-wise
  * AIC
  * BIC 
  * Modern approaches: Lasso, Ridge-Regression, etc.
* Statistical selection may bias your inference
  * If possible, do selection on a held out sample


---

## Error measures

* $R^2$ alone isn't enough - more variables = bigger $R^2$
* [Adjusted $R^2$](http://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2) is $R^2$ taking into account the number of estimated parameters
* [AIC](http://en.wikipedia.org/wiki/Akaike_information_criterion) also penalizes models with more parameters
* [BIC](http://en.wikipedia.org/wiki/Bayesian_information_criterion) does the same, but with a bigger penalty

---

## Movie Data

```{r loadMovies,cache=TRUE}
download.file("http://www.rossmanchance.com/iscam2/data/movies03RT.txt",destfile="./data/movies.txt")
movies <- read.table("./data/movies.txt",sep="\t",header=T,quote="")
head(movies)
```

[http://www.rossmanchance.com/](http://www.rossmanchance.com/)


---

## Model selection  - step

```{r aic,dependson="loadMovies",warning=TRUE}
movies <- movies[,-1]
lm1 <- lm(score ~ .,data=movies) # XXX include all terms in the model           
aicFormula <- step(lm1)
```

---

## Model selection  - step

```{r,dependson="aic",warning=TRUE}
aicFormula
```

---

## Model selection  - regsubsets

```{r regsub,dependson="aic",warning=TRUE,fig.height=4,fig.width=4}
library(leaps);
regSub <- regsubsets(score ~ .,data=movies) # XXX regsubsets computes BIC for all combinations of covariates
plot(regSub)
```
[http://cran.r-project.org/web/packages/leaps/leaps.pdf](http://cran.r-project.org/web/packages/leaps/leaps.pdf)

---

## Model selection  - bic.glm

```{r,dependson="aid"}
library(BMA)
bicglm1 <- bic.glm(score ~.,data=movies,glm.family="gaussian")
print(bicglm1)
```

[http://cran.r-project.org/web/packages/BMA/BMA.pdf](http://cran.r-project.org/web/packages/BMA/BMA.pdf)


---

## Notes and further resources

* Exploratory/visual analysis is key
* Automatic selection produces an answer - but may bias inference
* You may think about separating the sample into two groups
* The goal is not to get the "causal" model


* [Lars package](http://cran.r-project.org/web/packages/lars/lars.pdf) 
* [Elements of machine learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)
<!--=*=*=*=*-->
<!--./week6/001predictionStudyDesign/index.Rmd
-->
---
title       : Prediction study design
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow  # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Key ideas

* Motivation
* Steps in predictive studies
* Choosing the right data
* Error measures
* Study design


---

## Why predict? Glory!

<img class=center src=assets/img/volinsky.png height='60%'/>


[http://www.zimbio.com/photos/Chris+Volinsky](http://www.zimbio.com/photos/Chris+Volinsky)



---

## Why predict? Riches!

<img class=center src=assets/img/heritage.png height='60%'/>


[http://www.heritagehealthprize.com/c/hhp](http://www.heritagehealthprize.com/c/hhp)



---

## Why predict? For sport!

<img class=center src=assets/img/kaggle.png height='60%'/>


[http://www.kaggle.com/](http://www.kaggle.com/)

---

## Why predict? To save lives!

<img class=center src=assets/img/oncotype.png height='40%'/>

[http://www.oncotypedx.com/en-US/Home](http://www.oncotypedx.com/en-US/Home)

---

## Steps in building a prediction

1. Find the right data
2. Define your error rate
3. Split data into:
  * Training 
  * Testing
  * Validation (optional)
4. On the training set pick features
5. On the training set pick prediction function
6. On the training set cross-validate
7. If no validation - apply 1x to test set
8. If validation - apply to test set and refine
9. If validation - apply 1x to validation 


---

## Find the right data

1. In some cases it is easy (movie ratings -> new movie ratings)
2. It may be harder (gene expression data -> disease)
3. Depends strongly on the definition of "good prediction".
3. Often [more data > better models](http://www.youtube.com/watch?v=yvDCzhbjYWs)
4. Know the bench mark
5. You need to start with raw data for predictions - processing is often cross-sample. 


---

## Know the benchmarks

Probability of perfect classification is approximately $\left(\frac{1}{2}\right)^{test \; set \; sample \; size}$

<img class=center src=assets/img/benchmark.png height='70%'/>

[http://www.heritagehealthprize.com/c/hhp/leaderboard](http://www.heritagehealthprize.com/c/hhp/leaderboard)

---

## Defining true/false positives

In general, __Positive__ = identified and __negative__ = rejected. Therefore:

__True positive__ = correctly identified

__False positive__ = incorrectly identified

__True negative__ = correctly rejected

__False negative__ = incorrectly rejected

_Medical testing example_:

__True positive__ = Sick people correctly diagnosed as sick

__False positive__= Healthy people incorrectly identified as sick

__True negative__ = Healthy people correctly identified as healthy

__False negative__ = Sick people incorrectly identified as healthy.

[http://en.wikipedia.org/wiki/Sensitivity_and_specificity](http://en.wikipedia.org/wiki/Sensitivity_and_specificity)

---

## Define your error rate

<img class=center src=assets/img/sensspec.png height='80%'/>


[http://en.wikipedia.org/wiki/Sensitivity_and_specificity](http://en.wikipedia.org/wiki/Sensitivity_and_specificity)

---

## Why your choice matters

<img class=center src=assets/img/sensspecex.png height='80%'/>


[http://en.wikipedia.org/wiki/Sensitivity_and_specificity](http://en.wikipedia.org/wiki/Sensitivity_and_specificity)

---

## Common error measures

1. Mean squared error (or root mean squared error)
  * Continuous data, sensitive to outliers
2. Median absolute deviation 
  * Continuous data, often more robust
3. Sensitivity (recall)
  * If you want few missed positives
4. Specificity
  * If you want few negatives called positives
5. Accuracy
  * Weights false positives/negatives equally
6. Concordance
  * One example is [kappa](http://en.wikipedia.org/wiki/Cohen%27s_kappa)
5. Predictive value of a positive (precision)
  * When you are screeing and prevelance is low

---

## Study design

<img class=center src=assets/img/studydesign.png height='60%'/>

[http://www2.research.att.com/~volinsky/papers/ASAStatComp.pdf](http://www2.research.att.com/~volinsky/papers/ASAStatComp.pdf)

---

## Key issues and further resources

_Issues_:

1. Accuracy 
2. Overfitting
3. Interpretability
4. Computational speed


_Resources_:

1. [Practical machine learning](http://www.cbcb.umd.edu/~hcorrada/PracticalML/)
2. [Elements of statistical learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)
3. [Coursera machine learning](https://www.coursera.org/course/ml)
4. [Machine learning for hackers](http://www.amazon.com/Machine-Learning-Hackers-Drew-Conway/dp/1449303714)

<!--=*=*=*=*-->
<!--./week6/002crossValidation/index.Rmd
-->
---
title       : Cross validation
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow  # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Key ideas

* Sub-sampling the training data
* Avoiding overfitting
* Making predictions generalizable

---

## Steps in building a prediction

1. Find the right data
2. Define your error rate
3. Split data into:
  * Training 
  * Testing
  * Validation (optional)
4. <redtext> On the training set pick features </redtext>
5. <redtext>On the training set pick prediction function </redtext>
6. <redtext> On the training set cross-validate </redtext>
7. If no validation - apply 1x to test set
8. If validation - apply to test set and refine
9. If validation - apply 1x to validation 

---

## Study design

<img class=center src=assets/img/studydesign.png height='60%'/>

[http://www2.research.att.com/~volinsky/papers/ASAStatComp.pdf](http://www2.research.att.com/~volinsky/papers/ASAStatComp.pdf)

---

## Overfitting

```{r generateData,fig.height=4,fig.width=4}
set.seed(12345)
x <- rnorm(10); y <- rnorm(10); z <- rbinom(10,size=1,prob=0.5)
plot(x,y,pch=19,col=(z+3))
```

---

## Classifier
If -0.2 < y < 0.6 call blue, otherwise green

```{r,dependson ="generateData",fig.height=4,fig.width=8}
par(mfrow=c(1,2))
zhat <- (-0.2 < y) & (y < 0.6)
plot(x,y,pch=19,col=(z+3)); plot(x,y,pch=19,col=(zhat+3))
```

---

## New data

If -0.2 < y < 0.6 call blue, otherwise green

```{r,dependson ="generateData",fig.height=4,fig.width=8}
set.seed(1233)
xnew <- rnorm(10); ynew <- rnorm(10); znew <- rbinom(10,size=1,prob=0.5)
par(mfrow=c(1,2)); zhatnew <- (-0.2 < ynew) & (ynew < 0.6)
plot(xnew,ynew,pch=19,col=(z+3)); plot(xnew,ynew,pch=19,col=(zhatnew+3))
```

---

## Key idea

1. Accuracy on the training set (resubstitution accuracy) is optimistic
2. A better estimate comes from an independent set (test set accuracy)
3. But we can't use the test set when building the model or it becomes part of the training set
4. So we estimate the test set accuracy with the training set. 

---

## Cross-validation

_Approach_:

1. Use the training set

2. Split it into training/test sets 

3. Build a model on the training set

4. Evaluate on the test set

5. Repeat and average the estimated errors

_Used for_:

1. Picking variables to include in a model

2. Picking the type of prediction function to use

3. Picking the parameters in the prediction function

4. Comparing different predictors

---

## Random subsampling


<img class=center src=assets/img/random.png height='60%'/>


---

## K-fold


<img class=center src=assets/img/kfold.png height='60%'/>


---

## Leave one out

<img class=center src=assets/img/loocv.png height='60%'/>

---

## Example

```{r,dependson ="generateData",fig.height=4,fig.width=8}
y1 <- y[1:5]; x1 <- x[1:5]; z1 <- z[1:5]
y2 <- y[6:10]; x2 <- x[6:10]; z2 <- z[6:10]; 
zhat2 <- (y2 < 1) & (y2 > -0.5)
par(mfrow=c(1,3))
plot(x1,y1,col=(z1+3),pch=19); plot(x2,y2,col=(z2+3),pch=19); plot(x2,y2,col=(zhat2+3),pch=19)
```

---

## Notes and further resources

* The training and test sets must come from the same popluation. 
* Sampling should be designed to mimic real patterns (e.g., sampling time chunks for time series)
* Cross validation estimates have variance - it is difficult to estimate how much 
* [Cross validation in R](http://www.unt.edu/rss/class/Jon/Benchmarks/CrossValidation1_JDS_May2011.pdf)
* [cvTools](http://cran.r-project.org/web/packages/cvTools/cvTools.pdf)
* [boot](http://cran.r-project.org/web/packages/boot/index.html)


<!--=*=*=*=*-->
<!--./week6/003predictingRegression/index.Rmd
-->
---
title       : Predicting with regression models
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow  # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Key ideas

* Use a standard regression model
  * lm
  * glm
* Predict new values with the coefficients
* Useful when the linear model is (nearly) correct

__Pros__:
* Easy to implement
* Easy to interpret

__Cons__:
* Often poor performance in nonlinear settings


---

## Example: Old faithful eruptions

<img class=center src=assets/img/yellowstone.png height='60%'/>

Image Credit/Copyright Wally Pacholka [http://www.astropics.com/](http://www.astropics.com/)

---

## Example: Old faithful eruptions

```{r faith}
data(faithful)
dim(faithful)
set.seed(333)
trainSamples <- sample(1:272,size=(272/2),replace=F)
trainFaith <- faithful[trainSamples,]
testFaith <- faithful[-trainSamples,]
head(trainFaith)
```

---

## Eruption duration versus waiting time

```{r dependson="faith",fig.height=4,fig.width=4}
plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
```

---

## Fit a linear model 

$$ ED_i = b_0 + b_1 WT_i + e_i $$

```{r faithlm,dependson="faith",fig.height=4,fig.width=4}
lm1 <- lm(eruptions ~ waiting,data=trainFaith)
summary(lm1)
```


---
## Model fit

```{r dependson="faithlm",fig.height=4,fig.width=4}
plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
lines(trainFaith$waiting,lm1$fitted,lwd=3)
```

---

## Predict a new value

$$\hat{ED} = \hat{b}_0 + \hat{b}_1 WT$$

```{r ,dependson="faithlm",fig.height=4,fig.width=4}
coef(lm1)[1] + coef(lm1)[2]*80
newdata <- data.frame(waiting=80)
predict(lm1,newdata)
```

---

## Plot predictions - training and test

```{r ,dependson="faithlm",fig.height=4,fig.width=8}
par(mfrow=c(1,2))
plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
lines(trainFaith$waiting,predict(lm1),lwd=3)
plot(testFaith$waiting,testFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
lines(testFaith$waiting,predict(lm1,newdata=testFaith),lwd=3)
```

---

## Get training set/test set errors

```{r ,dependson="faithlm",fig.height=4,fig.width=4}
# Calculate RMSE on training
sqrt(sum((lm1$fitted-trainFaith$eruptions)^2))

# Calculate RMSE on test
sqrt(sum((predict(lm1,newdata=testFaith)-testFaith$eruptions)^2))
```

---

## Prediction intervals

```{r ,dependson="faithlm",fig.height=4,fig.width=4}
pred1 <- predict(lm1,newdata=testFaith,interval="prediction")       # XXX interval="prediction" gives the interval around the predicted value
ord <- order(testFaith$waiting)
plot(testFaith$waiting,testFaith$eruptions,pch=19,col="blue")
matlines(testFaith$waiting[ord],pred1[ord,],type="l",,col=c(1,2,2),lty = c(1,1,1), lwd=3)
```

---

## Example with binary data: Baltimore Ravens

<img class=center src=assets/img/ravens.png height='60%'/>

[http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens](http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens)

---

## Ravens Data

```{r loadRavens,cache=TRUE}
download.file("https://dl.dropbox.com/u/7710864/data/ravensData.rda",
              destfile="./data/ravensData.rda",method="curl")
load("./data/ravensData.rda")
head(ravensData)
```

---

## Fit a logistic regression

$$logit(E[RW_i | RS_i]) = b_0 + b_1 RS_i$$
```{r glmRavens, dependson="loadRavens",fig.height=4,fig.width=8}
glm1 <- glm(ravenWinNum ~ ravenScore,family="binomial",data=ravensData)
par(mfrow=c(1,2))
boxplot(predict(glm1) ~ ravensData$ravenWinNum,col="blue")
boxplot(predict(glm1,type="response") ~ ravensData$ravenWinNum,col="blue")
```

---

## Choosing a cutoff (re-substitution)

```{r cutoff, dependson="glmRavens",fig.height=4,fig.width=4}
xx <- seq(0,1,length=10); err <- rep(NA,10)
for(i in 1:length(xx)){
  err[i] <- sum((predict(glm1,type="response") > xx[i]) != ravensData$ravenWinNum)
}
plot(xx,err,pch=19,xlab="Cutoff",ylab="Error")
```

---

## Comparing models with cross validation

```{r,dependson="glmRavens",fig.height=4,fig.width=4}
library(boot)
cost <- function(win, pred = 0) mean(abs(win-pred) > 0.5)
glm1 <- glm(ravenWinNum ~ ravenScore,family="binomial",data=ravensData) # ??? predict(glm1) returns values > 1 when glm1 is logistic regression
glm2 <- glm(ravenWinNum ~ ravenScore,family="gaussian",data=ravensData)
cv1 <- cv.glm(ravensData,glm1,cost,K=3)
cv2 <- cv.glm(ravensData,glm2,cost,K=3)
cv1$delta
cv2$delta
```

---

## Notes and further reading

* Regression models with multiple covariates can be included
* Often useful in combination with other models 
* [Elements of statistical learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)
* [Modern applied statistics with S](http://www.amazon.com/Modern-Applied-Statistics-W-N-Venables/dp/0387954570)

<!--=*=*=*=*-->
<!--./week6/004predictingTrees/index.Rmd
-->
---
title       : Predicting with trees
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow  # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Key ideas

* Iteratively split variables into groups
* Split where maximally predictive
* Evaluate "homogeneity" within each branch
* Fitting multiple trees often works better (forests)

__Pros__:
* Easy to implement
* Easy to interpret
* Better performance in nonlinear settings

__Cons__:
* Without pruning/cross-validation can lead to overfitting
* Harder to estimate uncertainty
* Results may be variable


---

## Example Tree

<img class=center src=assets/img/obamaTree.png height='80%'/>

[http://graphics8.nytimes.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg](http://graphics8.nytimes.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg)

---

## Basic algorithm

1. Start with all variables in one group
2. Find the variable/split that best separates the outcomes
3. Divide the data into two groups ("leaves") on that split ("node")
4. Within each split, find the best variable/split that separates the outcomes
5. Continue until the groups are too small or sufficiently "pure"

---

## Measures of impurity

$$\hat{p}_{mk} = \frac{1}{N_m}\sum_{x_i\; in \; Leaf \; m}\mathbb{1}(y_i = k)$$

__Misclassification Error__: 
$$ 1 - \hat{p}_{mk(m)}$$

__Gini index__:
$$ \sum_{k \neq k'} \hat{p}_{mk} \times \hat{p}_{mk'} = \sum_{k=1}^K \hat{p}_{mk}(1-\hat{p}_{mk}) $$

__Cross-entropy or deviance__:

$$ -\sum_{k=1}^K \hat{p}_{mk} \ln\hat{p}_{mk} $$


---

## Example: Iris Data
```{r iris}
data(iris)
names(iris)
table(iris$Species)
```
---

## Iris petal widths/sepal width
```{r ,dependson="iris",fig.height=4,fig.width=4}
plot(iris$Petal.Width,iris$Sepal.Width,pch=19,col=as.numeric(iris$Species))
legend(1,4.5,legend=unique(iris$Species),col=unique(as.numeric(iris$Species)),pch=19)
```


---

## Iris petal widths/sepal width
```{r createTree,dependson="iris"}
# An alternative is library(rpart)
library(tree)
tree1 <- tree(Species ~ Sepal.Width + Petal.Width,data=iris)
summary(tree1)
```

---

## Plot tree

```{r ,dependson="iris",fig.height=4.5,fig.width=4.5}
plot(tree1)
text(tree1)
```


---

## Another way of looking at a CART model

```{r ,dependson="createTree",fig.height=4.5,fig.width=4.5}
plot(iris$Petal.Width,iris$Sepal.Width,pch=19,col=as.numeric(iris$Species))
partition.tree(tree1,label="Species",add=TRUE)
legend(1.75,4.5,legend=unique(iris$Species),col=unique(as.numeric(iris$Species)),pch=19)
```

---

## Predicting new values

```{r newdata,dependson="createTree",fig.height=4.5,fig.width=4.5}
set.seed(32313)
newdata <- data.frame(Petal.Width = runif(20,0,2.5),Sepal.Width = runif(20,2,4.5))
pred1 <- predict(tree1,newdata)
pred1
```


---

## Overlaying new values

```{r ,dependson="newdata",fig.height=4.5,fig.width=4.5}
pred1 <- predict(tree1,newdata,type="class")
plot(newdata$Petal.Width,newdata$Sepal.Width,col=as.numeric(pred1),pch=19)
partition.tree(tree1,"Species",add=TRUE)
```

---

## Pruning trees example: Cars

```{r carsData,fig.height=3,fig.width=3}
data(Cars93,package="MASS")
head(Cars93)
```

---

## Build a tree

```{r, dependson="carsData",fig.height=4,fig.width=4}
treeCars <- tree(DriveTrain ~ MPG.city + MPG.highway + AirBags + 
                   EngineSize + Width + Length + Weight + Price + Cylinders + 
                   Horsepower + Wheelbase,data=Cars93)
plot(treeCars)
text(treeCars)
```

---

## Plot errors

```{r , dependson="treeCars",fig.height=4,fig.width=8}
par(mfrow=c(1,2))
plot(cv.tree(treeCars,FUN=prune.tree,method="misclass"))
plot(cv.tree(treeCars))
pruneTree <- prune.tree(treeCars,best=4)
plot(pruneTree)
text(pruneTree)
```

---

## Prune the tree

```{r pruneTree, dependson="treeCars",fig.height=4,fig.width=8,cache=TRUE}
pruneTree <- prune.tree(treeCars,best=4)
plot(pruneTree)
text(pruneTree)
```

---

## Show resubstitution error $^*$

```{r ,dependson="pruneTree",fig.height=4,fig.width=8}
table(Cars93$DriveTrain,predict(pruneTree,type="class"))
table(Cars93$DriveTrain,predict(treeCars,type="class"))
```

* Note that cross validation error is a better measure of test set accuracy


---

## Notes and further resources

* [Hector Corrada Bravo's Notes](http://www.cbcb.umd.edu/~hcorrada/PracticalML/pdf/lectures/trees.pdf), [code](http://www.cbcb.umd.edu/~hcorrada/PracticalML/src/trees.R)
* [Cosma Shalizi's notes](http://www.stat.cmu.edu/~cshalizi/350/lectures/22/lecture-22.pdf)
* [Elements of Statistical Learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)
* [Classification and regression trees](http://www.amazon.com/Classification-Regression-Trees-Leo-Breiman/dp/0412048418)
* [Random forests](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)

<!--=*=*=*=*-->
<!--./week7/001smoothing/index.Rmd
-->
---
title       : Smoothing
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow  # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Key ideas

* Sometimes there are non-linear trends in data
* We can use "smoothing" to try to capture these
* Still a risk of overfitting 
* Often hard to interpret

---

## CD4 Data

```{r cd4Data, cache=TRUE}
download.file("https://spark-public.s3.amazonaws.com/dataanalysis/cd4.data",
              destfile="./data/cd4.data",method="curl")
cd4Data <- read.table("./data/cd4.data", 
                      col.names=c("time", "cd4", "age", "packs", "drugs", "sex",
                                  "cesd", "id"))
cd4Data <- cd4Data[order(cd4Data$time),]
head(cd4Data)
```

[http://www.cbcb.umd.edu/~hcorrada/PracticalML/](http://www.cbcb.umd.edu/~hcorrada/PracticalML/)

---

## CD4 over time

```{r, dependson="cd4Data", cache=TRUE,fig.height=4,fig.width=4}
plot(cd4Data$time,cd4Data$cd4,pch=19,cex=0.1)
```

---

## Average first 2 points

```{r, dependson="cd4Data", cache=TRUE,fig.height=4,fig.width=4}
plot(cd4Data$time,cd4Data$cd4,pch=19,cex=0.1)
points(mean(cd4Data$time[1:2]),mean(cd4Data$cd4[1:2]),col="blue",pch=19)
```

---

## Average second and third points

```{r, dependson="cd4Data", cache=TRUE,fig.height=4,fig.width=4}
plot(cd4Data$time,cd4Data$cd4,pch=19,cex=0.1)
points(mean(cd4Data$time[1:2]),mean(cd4Data$cd4[1:2]),col="blue",pch=19)
points(mean(cd4Data$time[2:3]),mean(cd4Data$cd4[2:3]),col="blue",pch=19)
```


---

## A moving average

```{r, dependson="cd4Data", cache=TRUE,fig.height=4,fig.width=4}
plot(cd4Data$time,cd4Data$cd4,pch=19,cex=0.1)
aveTime <- aveCd4 <- rep(NA,length(3:(dim(cd4Data)[1]-2)))
for(i in 3:(dim(cd4Data)[1]-2)){
    aveTime[i] <- mean(cd4Data$time[(i-2):(i+2)])
    aveCd4[i] <- mean(cd4Data$cd4[(i-2):(i+2)])
}
lines(aveTime,aveCd4,col="blue",lwd=3)
```


---

## Average more points

```{r, dependson="cd4Data", cache=TRUE,fig.height=4,fig.width=4}
plot(cd4Data$time,cd4Data$cd4,pch=19,cex=0.1)
aveTime <- aveCd4 <- rep(NA,length(11:(dim(cd4Data)[1]-10)))
for(i in 11:(dim(cd4Data)[1]-2)){
  aveTime[i] <- mean(cd4Data$time[(i-10):(i+10)])
 aveCd4[i] <- mean(cd4Data$cd4[(i-10):(i+10)])
}
lines(aveTime,aveCd4,col="blue",lwd=3)
```

---

## Average many more

```{r, dependson="cd4Data", cache=TRUE,fig.height=4,fig.width=4}
plot(cd4Data$time,cd4Data$cd4,pch=19,cex=0.1)
aveTime <- aveCd4 <- rep(NA,length(201:(dim(cd4Data)[1]-200)))
for(i in 201:(dim(cd4Data)[1]-2)){
    aveTime[i] <- mean(cd4Data$time[(i-200):(i+200)])
    aveCd4[i] <- mean(cd4Data$cd4[(i-200):(i+200)])
}
lines(aveTime,aveCd4,col="blue",lwd=3)
```

---

## A faster way

```{r, dependson="cd4Data", cache=TRUE,fig.height=4,fig.width=4}
filtTime <- as.vector(filter(cd4Data$time,filter=rep(1,200))/200)
filtCd4 <- as.vector(filter(cd4Data$cd4,filter=rep(1,200))/200)
plot(cd4Data$time,cd4Data$cd4,pch=19,cex=0.1); lines(filtTime,filtCd4,col="blue",lwd=3)
```


---

## Averaging = weighted sums 

```{r, dependson="cd4Data", cache=TRUE,fig.height=4,fig.width=4}
filtCd4 <- as.vector(filter(cd4Data$cd4,filter=rep(1,4))/4)
filtCd4[2]
sum(cd4Data$cd4[1:4] * rep(1/4,4))
```


---

## Other weights -> should sum to one

```{r, dependson="cd4Data", cache=TRUE,fig.height=4,fig.width=4}
ws = 10; tukey = function(x) pmax(1 - x^2,0)^2
filt= tukey(seq(-ws,ws)/(ws+1));filt=filt/sum(filt)
plot(seq(-(ws),(ws)),filt,pch=19)
```

---

## Other weights -> should sum to one

```{r, dependson="cd4Data", cache=TRUE,fig.height=4,fig.width=4}
ws = 100; tukey = function(x) pmax(1 - x^2,0)^2
filt= tukey(seq(-ws,ws)/(ws+1));filt=filt/sum(filt)
filtTime <- as.vector(filter(cd4Data$time,filter=filt))
filtCd4 <- as.vector(filter(cd4Data$cd4,filter=filt))
plot(cd4Data$time,cd4Data$cd4,pch=19,cex=0.1); lines(filtTime,filtCd4,col="blue",lwd=3)
```

---

## Lowess (loess)

```{r lowess, dependson="cd4Data",fig.height=4,fig.width=4}
lw1 <- loess(cd4 ~ time,data=cd4Data)
plot(cd4Data$time,cd4Data$cd4,pch=19,cex=0.1)
lines(cd4Data$time,lw1$fitted,col="blue",lwd=3)
```


---

## Span

```{r, dependson="cd4Data",fig.height=4,fig.width=4}
plot(cd4Data$time,cd4Data$cd4,pch=19,cex=0.1,ylim=c(500,1500))
lines(cd4Data$time,loess(cd4 ~ time,data=cd4Data,span=0.1)$fitted,col="blue",lwd=3)
lines(cd4Data$time,loess(cd4 ~ time,data=cd4Data,span=0.25)$fitted,col="red",lwd=3)
lines(cd4Data$time,loess(cd4 ~ time,data=cd4Data,span=0.76)$fitted,col="green",lwd=3)
```


---

## Predicting with loess

```{r, dependson="lowess",fig.height=4,fig.width=4}
tme <- seq(-2,5,length=100); pred1 = predict(lw1,newdata=data.frame(time=tme),se=TRUE)
plot(tme,pred1$fit,col="blue",lwd=3,type="l",ylim=c(0,2500))
lines(tme,pred1$fit + 1.96*pred1$se.fit,col="red",lwd=3)
lines(tme,pred1$fit - 1.96*pred1$se.fit,col="red",lwd=3)
points(cd4Data$time,cd4Data$cd4,pch=19,cex=0.1)
```


---

## Splines

$$ Y_i = b_0 + \sum_{k=1}^K b_k s_k(x_i) + e_i $$


$Y_i$ - outcome for $i$th observation

$b_0$ - Intercept term

$b_k$ - Coefficient for $k$th spline function

$s_k$ - $k$th spline function

$x_i$ - covariate for $i$th observation

$e_i$ - everything we didn't measure/model


---

## Splines in R 

```{r splines, dependson="cd4Data",fig.height=4,fig.width=8}
library(splines)
ns1 <- ns(cd4Data$time,df=3)
par(mfrow=c(1,3))
plot(cd4Data$time,ns1[,1]); plot(cd4Data$time,ns1[,2]); plot(cd4Data$time,ns1[,3])
```

---

## Regression with splines

```{r splineReg, dependson="splines",fig.height=4,fig.width=4}
lm1 <- lm(cd4Data$cd4 ~ ns1)
summary(lm1)
```

---

## Fitted values

```{r, dependson="splineReg",fig.height=4,fig.width=4}
plot(cd4Data$time,cd4Data$cd4,pch=19,cex=0.1)
points(cd4Data$time,lm1$fitted,col="blue",pch=19,cex=0.5)
```

---

## Notes and further resources

__Notes__:

* Cross-validation with splines/smoothing is a good idea
* Do not predict outside the range of observed data

__Further resources__:

* [Hector Corrada Bravo's Lecture Notes](http://www.cbcb.umd.edu/~hcorrada/PracticalML/pdf/lectures/smoothing.pdf)
* [Rafa Irizarry's Lecture Notes on smoothing](http://www.biostat.jhsph.edu/~ririzarr/Teaching/649/section-06.pdf), [On splines](http://www.biostat.jhsph.edu/~ririzarr/Teaching/649/section-09.pdf)
* [Elements of Statistical Learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)
* [Advanced Data Analysis from An Elementary Point of View](http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf)




<!--=*=*=*=*-->
<!--./week7/002bootstrap/index.Rmd
-->
---
title       : The bootstrap
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow  # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Key ideas

* Treat the sample as if it were the population

__What it is good for__:

* Calculating standard errors
* Forming confidence intervals
* Performing hypothesis tests
* Improving predictors

---

## The "Central Dogma" of statistics

<img class=center src=assets/img/centraldogma.png height='70%'/>


[http://www.gs.washington.edu/academics/courses/akey/56008/lecture/lecture5.pdf](http://www.gs.washington.edu/academics/courses/akey/56008/lecture/lecture5.pdf)


---

## The bootstrap

<img class=center src=assets/img/bootstrap.png height='70%'/>


---

## Example

```{r,fig.height=4,fig.width=4}
set.seed(333); x <- rnorm(30)
bootMean <- rep(NA,1000); sampledMean <- rep(NA,1000)
for(i in 1:1000){bootMean[i] <- mean(sample(x,replace=TRUE))}
for(i in 1:1000){sampledMean[i] <- mean(rnorm(30))}
plot(density(bootMean)); lines(density(sampledMean),col="red")
```

---

## Example with boot package

```{r bootEx,fig.height=4,fig.width=4}
set.seed(333); x <- rnorm(30); sampledMean <- rep(NA,1000)
for(i in 1:1000){sampledMean[i] <- mean(rnorm(30))}
meanFunc <- function(x,i){mean(x[i])}
bootMean <- boot(x,meanFunc,1000)
bootMean
```


---

## Plotting boot package example

```{r, dependson="bootEx",fig.height=4,fig.width=4}
plot(density(bootMean$t)); lines(density(sampledMean),col="red")
```



---

## Nuclear costs

```{r bootNuc,fig.height=4,fig.width=4}
library(boot); data(nuclear)
nuke.lm <- lm(log(cost) ~ date,data=nuclear)
plot(nuclear$date,log(nuclear$cost),pch=19)
abline(nuke.lm,col="red",lwd=3)
```


---

## Nuclear costs


```{r nukeLm, dependson="bootNuc",fig.height=4,fig.width=8}
par(mfrow=c(1,3))
for(i in 1:3){
  nuclear0 <- nuclear[sample(1:dim(nuclear)[1],replace=TRUE),]
  nuke.lm0 <- lm(log(cost) ~ date,data=nuclear0)
  plot(nuclear0$date,log(nuclear0$cost),pch=19)
  abline(nuke.lm0,col="red",lwd=3)
}
```


---

## Bootstrap distribution

```{r nucRes, dependson="nukeLm",fig.height=4,fig.width=4}
bs <- function(data, indices,formula) {
  d <- data[indices,];fit <- lm(formula, data=d);return(coef(fit)) 
} 
results <- boot(data=nuclear, statistic=bs, R=1000, formula=log(cost) ~ date)
plot(density(results$t[,2]),col="red",lwd=3)
lines(rep(nuke.lm$coeff[2],10),seq(0,8,length=10),col="blue",lwd=3)
```

[http://www.statmethods.net/advstats/bootstrapping.html](http://www.statmethods.net/advstats/bootstrapping.html)

---

## Bootstrap confidence intervals

```{r dependson="nucRes",fig.height=4,fig.width=4}
boot.ci(results)
```


---

## Bootstrapping from a model

```{r nullBoot, dependson="nukeLm",fig.height=4,fig.width=4}
resid <- rstudent(nuke.lm)
fit0 <- fitted(lm(log(cost) ~ 1,data=nuclear))
newNuc <- cbind(nuclear,resid=resid,fit0=fit0)
bs <- function(data, indices) {
  return(coef(glm(data$fit0 + data$resid[indices] ~ data$date,data=data)))
} 
results <- boot(data=newNuc, statistic=bs, R=1000)
```

---

## Results

```{r, dependson="nullBoot",fig.height=4,fig.width=4}
plot(density(results$t[,2]),lwd=3,col="blue")
lines(rep(coef(nuke.lm)[2],10),seq(0,3,length=10),col="red",lwd=3)
```

---

## An empirical p-value 

$$ \hat{p} = \frac{1 + \sum_{b=1}^B |t_b^0| > |t|}{B+1}$$
```{r, dependson="nullBoot",fig.height=4,fig.width=4}
B <- dim(results$t)[1]
(1 + sum((abs(results$t[,2]) > abs(coef(nuke.lm)[2]))))/(B+1)
```


---

## Bootstrapping non-linear statistics

```{r,fig.height=4,fig.width=4}
set.seed(555); x <- rnorm(30); sampledMed <- rep(NA,1000)
for(i in 1:1000){sampledMed[i] <- median(rnorm(30))}
medFunc <- function(x,i){median(x[i])}; bootMed <- boot(x,medFunc,1000)
plot(density(bootMed$t),col="red",lwd=3)
lines(density(sampledMed),lwd=3)
```

---

## Things you can't bootstrap (max)

```{r,fig.height=4,fig.width=4}
set.seed(333); x <- rnorm(30); sampledMax <- rep(NA,1000)
for(i in 1:1000){sampledMax[i] <- max(rnorm(30))}
maxFunc <- function(x,i){max(x[i])}; bootMax <- boot(x,maxFunc,1000)
plot(density(bootMax$t),col="red",lwd=3,xlim=c(1,3))
lines(density(sampledMax),lwd=3)
```


---

## Notes and further resources

__Notes__:

* Can be useful for complicated statistics
* Be careful near the boundaries
* Be careful with non-linear functions

__Further resources__:

* [Brian Caffo's bootstrap notes](http://ocw.jhsph.edu/courses/MethodsInBiostatisticsI/PDFs/lecture12.pdf)
* [Nice basic intro to boot package](http://www.mayin.org/ajayshah/KB/R/documents/boot.html)
* [Another basic boot tutorial](http://www.statmethods.net/advstats/bootstrapping.html)
* [An introduction to the bootstrap](http://www.amazon.com/Introduction-Bootstrap-Monographs-Statistics-Probability/dp/0412042312)
* [Confidence limits on phylogenies](http://www.jstor.org/discover/10.2307/2408678?uid=3739704&uid=2&uid=4&uid=3739256&sid=21101897412687)

<!--=*=*=*=*-->
<!--./week7/003bootstrapPrediction/index.Rmd
-->
---
title       : Bootstrapping for prediction
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow  # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Key ideas

* Bootstrapping can be used for
  * Cross-validation type error rates
  * Prediction errors in regression models
  * Improving prediction

---

## Bootstrapping prediction errors

```{r nucData,fig.height=4,fig.width=4}
library(boot); data(nuclear)
nuke.lm <- lm(log(cost) ~ date,data=nuclear)
plot(nuclear$date,log(nuclear$cost),pch=19)
abline(nuke.lm,col="red",lwd=3)
```

---

## Bootstrapping prediction errors

```{r nucBoot, dependson="nucData",fig.height=4,fig.width=4,cache=TRUE}
newdata <- data.frame(date = seq(65,72,length=100))
nuclear <- cbind(nuclear,resid=rstudent(nuke.lm),fit=fitted(nuke.lm))
nuke.fun <- function(data,inds,newdata){
  lm.b <- lm(fit + resid[inds] ~ date,data=data)
  pred.b <- predict(lm.b,newdata)
  return(pred.b)
}
nuke.boot <- boot(nuclear,nuke.fun,R=1000,newdata=newdata)
head(nuke.boot$t)
```

---

## Bootstrapping prediction errors

```{r,dependson="nucBoot",fig.height=4,fig.width=4,cache=TRUE}
pred <- predict(nuke.lm,newdata)
predSds <- apply(nuke.boot$t,2,sd)
plot(newdata$date,pred,col="black",type="l",lwd=3,ylim=c(0,10))
lines(newdata$date,pred + 1.96*predSds,col="red",lwd=3)
lines(newdata$date,pred - 1.96*predSds,col="red",lwd=3)
```


---

## Bootstrap aggregating (bagging)

__Basic idea__: 

1. Resample cases and recalculate predictions
2. Average or majority vote

__Notes__:

* Similar bias 
* Reduced variance
* More useful for non-linear functions


---

## Bagged loess

```{r ozoneData}
library(ElemStatLearn); data(ozone,package="ElemStatLearn")
ozone <- ozone[order(ozone$ozone),]
head(ozone)
```
[http://en.wikipedia.org/wiki/Bootstrap_aggregating](http://en.wikipedia.org/wiki/Bootstrap_aggregating)

---

## Bagged loess

```{r baggedOzone, dependson="ozoneData"}
ll <- matrix(NA,nrow=10,ncol=155)
for(i in 1:10){
  ss <- sample(1:dim(ozone)[1],replace=T)
  ozone0 <- ozone[ss,]; ozone0 <- ozone0[order(ozone0$ozone),]
  loess0 <- loess(temperature ~ ozone,data=ozone0,span=0.2)
  ll[i,] <- predict(loess0,newdata=data.frame(ozone=1:155))
}
```

---

## Bagged loess

```{r, dependson="baggedOzone",fig.height=4.5,fig.width=4.5}
plot(ozone$ozone,ozone$temperature,pch=19,cex=0.5)
for(i in 1:10){lines(1:155,ll[i,],col="grey",lwd=2)}
lines(1:155,apply(ll,2,mean),col="red",lwd=2)
```

---

## Bagged trees

__Basic idea__:

1. Resample data
2. Recalculate tree
3. Average/[mode](http://en.wikipedia.org/wiki/Mode_(statistics)) of predictors 

__Notes__:

1. More stable
2. May not be as good as random forests


---

## Iris data

```{r irisData,fig.height=3,fig.width=3}
data(iris)
head(iris)
```

---

## Bagging a tree

```{r bagTree, dependson="irisData",fig.height=4,fig.width=4}
library(ipred)
bagTree <- bagging(Species ~.,data=iris,coob=TRUE)
print(bagTree)
```


---

## Looking at bagged tree one

```{r, dependson="bagTree",fig.height=4,fig.width=4}
bagTree$mtrees[[1]]$btree
```

---

## Looking at bagged tree two

```{r, dependson="bagTree",fig.height=4,fig.width=4}
bagTree$mtrees[[2]]$btree
```


---

## Random forests

1. Bootstrap samples
2. At each split, bootstrap variables
3. Grow multiple trees and vote

__Pros__:

1. Accuracy

__Cons__:

1. Speed
2. Interpretability
3. Overfitting


---

## Random forests
```{r forestIris, dependson="irisData",fig.height=4,fig.width=4}
library(randomForest)
forestIris <- randomForest(Species~ Petal.Width + Petal.Length,data=iris,prox=TRUE)
forestIris
```

---

## Getting a single tree

```{r , dependson="forestIris",fig.height=4,fig.width=4}
getTree(forestIris,k=2)
```

---

## Class "centers"

```{r, dependson="forestIris",fig.height=4,fig.width=4}
iris.p <- classCenter(iris[,c(3,4)], iris$Species, forestIris$prox)
plot(iris[,3], iris[,4], pch=21, xlab=names(iris)[3], ylab=names(iris)[4],
bg=c("red", "blue", "green")[as.numeric(factor(iris$Species))],
main="Iris Data with Prototypes")
points(iris.p[,1], iris.p[,2], pch=21, cex=2, bg=c("red", "blue", "green"))
```


---

## Combining random forests

```{r, dependson="forestIris",fig.height=4,fig.width=4}
forestIris1 <- randomForest(Species~Petal.Width + Petal.Length,data=iris,prox=TRUE,ntree=50)
forestIris2 <- randomForest(Species~Petal.Width + Petal.Length,data=iris,prox=TRUE,ntree=50)
forestIris3 <- randomForest(Species~Petal.Width + Petal.Length,data=iris,prox=TRUE,nrtee=50)
combine(forestIris1,forestIris2,forestIris3)
```

---

## Predicting new values

```{r predForest, dependson="forestIris",fig.height=4,fig.width=4}
newdata <- data.frame(Sepal.Length<- rnorm(1000,mean(iris$Sepal.Length),
                                           sd(iris$Sepal.Length)),
                      Sepal.Width <- rnorm(1000,mean(iris$Sepal.Width),
                                           sd(iris$Sepal.Width)),
                      Petal.Width <- rnorm(1000,mean(iris$Petal.Width),
                                           sd(iris$Petal.Width)),
                      Petal.Length <- rnorm(1000,mean(iris$Petal.Length),
                                            sd(iris$Petal.Length)))

pred <- predict(forestIris,newdata)
```

---

## Predicting new values

```{r, dependson="predForest",fig.height=4,fig.width=4}
plot(newdata[,4], newdata[,3], pch=21, xlab="Petal.Length",ylab="Petal.Width",
bg=c("red", "blue", "green")[as.numeric(pred)],main="newdata Predictions")
```

---

## Notes and further resources

__Notes__:

* Bootstrapping is useful for nonlinear models
* Care should be taken to avoid overfitting (see [rfcv](http://cran.r-project.org/web/packages/randomForest/randomForest.pdf) funtion)
* Out of bag estimates are efficient estimates of test error

__Further resources__:

* [Random forests](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)
* [Random forest Wikipedia](http://en.wikipedia.org/wiki/Random_forest)
* [Bagging](http://en.wikipedia.org/wiki/Bootstrap_aggregating)
* [Bagging and boosting](http://stat.ethz.ch/education/semesters/FS_2008/CompStat/sk-ch8.pdf)

<!--=*=*=*=*-->
<!--./week7/004combiningPredictors/index.Rmd
-->
---
title       : Combining predictors
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow  # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Key ideas

* You can combine classifiers by averaging/voting
* Combining classifiers improves accuracy
* Combining classifiers reduces interpretability

---

## Netflix prize

BellKor = Combination of 107 predictors 

<img class=center src=assets/img/netflix.png height='70%'/>

[http://www.netflixprize.com//leaderboard](http://www.netflixprize.com//leaderboard)

---

## Heritage health prize - Progress Prize 1

<img class=center src=assets/img/makers.png height='40%'/>
[Market Makers](https://kaggle2.blob.core.windows.net/wiki-files/327/e4cd1d25-eca9-49ca-9593-b254a773fe03/Market%20Makers%20-%20Milestone%201%20Description%20V2%201.pdf)

<img class=center src=assets/img/mestrom.png height='40%'/>
[Mestrom](https://kaggle2.blob.core.windows.net/wiki-files/327/09ccf652-8c1c-4a3d-b979-ce2369c985e4/Willem%20Mestrom%20-%20Milestone%201%20Description%20V2%202.pdf)


---

## Basic intuition - majority vote

Suppose we have 5 completely independent classifiers

If accuracy is 70% for each:
  * $10\times(0.7)^3(0.3)^2 + 5\times(0.7)^4(0.3)^2 + (0.7)^5$
  * 83.7% majority vote accuracy

With 101 independent classifiers
  * 99.9% majority vote accuracy
  

---

## Approaches for combining classifiers

1. Bagging (see previous lecture)
2. [Boosting](http://en.wikipedia.org/wiki/AdaBoost)
3. Combining different classifiers

---

## Example

```{r generateData}
#library(devtools)
#install_github("medley","mewo2")
library(medley)
set.seed(453234)
y <- rnorm(1000)
x1 <- (y > 0); x2 <- y*rnorm(1000)
x3 <- rnorm(1000,mean=y,sd=1); x4 <- (y > 0) & (y < 3)
x5 <- rbinom(1000,size=4,prob=exp(y)/(1+exp(y)))
x6 <- (y < -2) | (y > 2)
data <- data.frame(y=y,x1=x1,x2=x2,x3=x3,x4=x4,x5=x5,x6=x6)
train <- sample(1:1000,size=500)
trainData <- data[train,]; testData <- data[-train,]
```

---

## Basic models

```{r basicModels, dependson="generateData",fig.height=4,fig.width=4}
library(tree)
lm1 <- lm(y ~.,data=trainData)
rmse(predict(lm1,data=testData),testData$y)
tree1 <- tree(y ~.,data=trainData)
rmse(predict(tree1,data=testData),testData$y)
tree2 <- tree(y~.,data=trainData[sample(1:dim(trainData)[1]),])
```

---

## Combining models

```{r, dependson="basicModels",fig.height=4,fig.width=4}
combine1 <- predict(lm1,data=testData)/2 + predict(tree1,data=testData)/2
rmse(combine1,testData$y)
combine2 <- (predict(lm1,data=testData)/3 + predict(tree1,data=testData)/3 
             + predict(tree2,data=testData)/3)
rmse(combine2,testData$y)
```

---

## Medley package

```{r medley, dependson="generateData",cache=TRUE}
#library(devtools)
#install_github("medley","mewo2")
library(medley)
library(e1071)
library(randomForests)
x <- trainData[,-1]
y <- trainData$y
newx <- testData[,-1]
```


[http://www.kaggle.com/users/10748/martin-o-leary](http://www.kaggle.com/users/10748/martin-o-leary)

---

## Blending models (part 1)

```{r blend1, dependson="medley"}
m <- create.medley(x, y, errfunc=rmse);
for (g in 1:10) {
  m <- add.medley(m, svm, list(gamma=1e-3 * g));
}
```

---

## Blending models (part 2)

```{r blend2, dependson="blend1"}
for (mt in 1:2) {
  m <- add.medley(m, randomForest, list(mtry=mt));
}
m <- prune.medley(m, 0.8);
rmse(predict(m,newx),testData$y)
```

---

## Notes and further resources

__Notes__:

* Even simple blending can be useful
* Majority vote is typical model for binary/multiclass data
* Makes models hard to interpret

__Further resources__:

* [Bayesian model averaging](http://www.research.att.com/~volinsky/bma.html)
* [Heritage health prize](https://www.heritagehealthprize.com/c/hhp/details/milestone-winners)
* [Netflix model blending](http://www2.research.att.com/~volinsky/papers/chance.pdf)

<!--=*=*=*=*-->
<!--./week8/001multipleTesting/index.Rmd
-->
---
title       : Multiple testing
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow  # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Key ideas

* Hypothesis testing/significance analysis is commonly overused
* Correcting for multiple testing avoids false positives or discoveries
* Two key components
  * Error measure
  * Correction


---

## Three eras of statistics

__The age of Quetelet and his successors, in which huge census-level data sets were brought to bear on simple but important questions__: Are there more male than female births? Is the rate of insanity rising?

The classical period of Pearson, Fisher, Neyman, Hotelling, and their successors, intellectual giants who __developed a theory of optimal inference capable of wringing every drop of information out of a scientific experiment__. The questions dealt with still tended to be simple Is treatment A better than treatment B? 

__The era of scientific mass production__, in which new technologies typified by the microarray allow a single team of scientists to produce data sets of a size Quetelet would envy. But now the flood of data is accompanied by a deluge of questions, perhaps thousands of estimates or hypothesis tests that the statistician is charged with answering together; not at all what the classical masters had in mind. Which variables matter among the thousands measured? How do you relate unrelated information?

[http://www-stat.stanford.edu/~ckirby/brad/papers/2010LSIexcerpt.pdf](http://www-stat.stanford.edu/~ckirby/brad/papers/2010LSIexcerpt.pdf)

---

## Reasons for multiple testing

<img class=center src=assets/img/datasources.png height='70%'/>


---

## Why correct for multiple tests?

<img class=center src=assets/img/jellybeans1.png height='70%'/>


[http://xkcd.com/882/](http://xkcd.com/882/)

---

## Why correct for multiple tests?

<img class=center src=assets/img/jellybeans2.png height='70%'/>

[http://xkcd.com/882/](http://xkcd.com/882/)


---

## Types of errors

Suppose you are testing a hypothesis that a parameter $\beta$ equals zero versus the alternative that it does not equal zero. These are the possible outcomes. 
</br></br>

                    | $\beta=0$   | $\beta\neq0$   |  Hypotheses
--------------------|-------------|----------------|---------
Claim $\beta=0$     |      $U$    |      $T$       |  $m-R$
Claim $\beta\neq 0$ |      $V$    |      $S$       |  $R$
    Claims          |     $m_0$   |      $m-m_0$   |  $m$

</br></br>

__Type I error or false positive ($V$)__ Say that the parameter does not equal zero when it does

__Type II error or false negative ($T$)__ Say that the parameter equals zero when it doesn't 


---

## Error rates

__False positive rate__ - The rate at which false results ($\beta = 0$) are called significant: $E\left[\frac{V}{m_0}\right]$*

__Family wise error rate (FWER)__ - The probability of at least one false positive ${\rm Pr}(V \geq 1)$

__False discovery rate (FDR)__ - The rate at which claims of significance are false $E\left[\frac{V}{R}\right]$

* The false positive rate is closely related to the type I error rate [http://en.wikipedia.org/wiki/False_positive_rate](http://en.wikipedia.org/wiki/False_positive_rate)

---

## Controlling the false positive rate

If P-values are correctly calculated calling all $P < \alpha$ significant will control the false positive rate at level $\alpha$ on average. 

<redtext>Problem</redtext>: Suppose that you perform 10,000 tests and $\beta = 0$ for all of them. 

Suppose that you call all $P < 0.05$ significant. 

The expected number of false positives is: $10,000 \times 0.05 = 500$  false positives. 

__How do we avoid so many false positives?__


---

## Controlling family-wise error rate (FWER)


The [Bonferroni correction](http://en.wikipedia.org/wiki/Bonferroni_correction) is the oldest multiple testing correction. 

__Basic idea__: 
* Suppose you do $m$ tests
* You want to control FWER at level $\alpha$ so $Pr(V \geq 1) < \alpha$
* Calculate P-values normally
* Set $\alpha_{fwer} = \alpha/m$
* Call all $P$-values less than $\alpha_{fwer}$ significant

__Pros__: Easy to calculate, conservative
__Cons__: May be very conservative


---

## Controlling false discovery rate (FDR)

This is the most popular correction when performing _lots_ of tests say in genomics, imagining, astronomy, or other signal-processing disciplines. 

__Basic idea__: 
* Suppose you do $m$ tests
* You want to control FDR at level $\alpha$ so $E\left[\frac{V}{R}\right]$
* Calculate P-values normally
* Order the P-values from smallest to largest $P_{(1)},...,P_{(m)}$
* Call any $P_{(i)} \leq \alpha \times \frac{i}{m}$ significant

__Pros__: Still pretty easy to calculate, less conservative (maybe much less)

__Cons__: Allows for more false positives, may behave strangely under dependence

---

## Example with 10 P-values

<img class=center src=assets/img/example10pvals.png height='70%'/>

Controlling all error rates at $\alpha = 0.20$

---

## Adjusted P-values

* One approach is to adjust the threshold $\alpha$
* A different approach is to calculate "adjusted p-values"
* They _are not p-values_ anymore
* But they can be used directly without adjusting $\alpha$

__Example__: 
* Suppose P-values are $P_1,\ldots,P_m$
* You could adjust them by taking $P_i^{fwer} = \max{m \times P_i,1}$ for each P-value.
* Then if you call all $P_i^{fwer} < \alpha$ significant you will control the FWER. 

---

## Case study I: no true positives

```{r createPvals,cache=TRUE}
set.seed(1010093)
pValues <- rep(NA,1000)
for(i in 1:1000){
  y <- rnorm(20)
  x <- rnorm(20)
  pValues[i] <- summary(lm(y ~ x))$coeff[2,4]
}

# Controls false positive rate
sum(pValues < 0.05)
```

---

## Case study I: no true positives

```{r, dependson="createPvals"}
# Controls FWER 
sum(p.adjust(pValues,method="bonferroni") < 0.05)
# Controls FDR 
sum(p.adjust(pValues,method="BH") < 0.05)
```


---

## Case study II: 50% true positives

```{r createPvals2,cache=TRUE}
set.seed(1010093)
pValues <- rep(NA,1000)
for(i in 1:1000){
  x <- rnorm(20)
  # First 500 beta=0, last 500 beta=2
  if(i <= 500){y <- rnorm(20)}else{ y <- rnorm(20,mean=2*x)}
  pValues[i] <- summary(lm(y ~ x))$coeff[2,4]
}
trueStatus <- rep(c("zero","not zero"),each=500)
table(pValues < 0.05, trueStatus)
```

---


## Case study II: 50% true positives

```{r, dependson="createPvals2"}
# Controls FWER 
table(p.adjust(pValues,method="bonferroni") < 0.05,trueStatus)
# Controls FDR 
table(p.adjust(pValues,method="BH") < 0.05,trueStatus)
```


---


## Case study II: 50% true positives

__P-values versus adjusted P-values__
```{r, dependson="createPvals2",fig.height=4,fig.width=8}
par(mfrow=c(1,2))
plot(pValues,p.adjust(pValues,method="bonferroni"),pch=19)
plot(pValues,p.adjust(pValues,method="BH"),pch=19)
```


---


## Notes and resources

__Notes__:
* Multiple testing is an entire subfield
* A basic Bonferroni/BH correction is usually enough
* If there is strong dependence between tests there may be problems
  * Consider method="BY"

__Further resources__:
* [Multiple testing procedures with applications to genomics](http://www.amazon.com/Multiple-Procedures-Applications-Genomics-Statistics/dp/0387493166/ref=sr_1_2/102-3292576-129059?ie=UTF8&s=books&qid=1187394873&sr=1-2)
* [Statistical significance for genome-wide studies](http://www.pnas.org/content/100/16/9440.full)
* [Introduction to multiple testing](http://ies.ed.gov/ncee/pubs/20084018/app_b.asp)


<!--=*=*=*=*-->
<!--./week8/002simulationForModelChecking/index.Rmd
-->
---
title       : Simulation for model checking
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow  # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Basic ideas

* Way back in the first week we talked about simulating data from distributions in R using the _rfoo_ functions. 
* In general simulations are way more flexible/useful
  * For bootstrapping as we saw in week 7
  * For evaluating models
  * For testing different hypotheses
  * For sensitivity analysis
* At minimum it is useful to simulate
  * A best case scenario
  * A few examples where you know your approach won't work
  * [The importance of simulating the extremes](http://simplystatistics.org/2013/03/06/the-importance-of-simulating-the-extremes/)
  


---

## Simulating data from a model

Suppose that you have a regression model

$$ Y_i = b_0 + b_1 X_i  + e_i$$

Here is an example of generating data from this model where $X_i$ and $e_i$ are normal:

```{r}
set.seed(44333)
x <- rnorm(50)
e <- rnorm(50)
b0 <- 1; b1 <- 2
y <- b0 + b1*x + e
```


---

## Violating assumptions

```{r simulate1}
set.seed(44333)
x <- rnorm(50)
e <- rnorm(50); e2 <- rcauchy(50)
b0 <- 1; b1 <- 2
y <- b0 + b1*x + e; y2 <-  b0 + b1*x + e2
```

---

## Violating assumptions

```{r,dependson="simulate1",fig.height=4,fig.width=8}
par(mfrow=c(1,2))
plot(lm(y ~ x)$fitted,lm(y~x)$residuals,pch=19,xlab="fitted",ylab="residuals")
plot(lm(y2 ~ x)$fitted,lm(y2~x)$residuals,pch=19,xlab="fitted",ylab="residuals")
```

---

## Repeated simulations

```{r simulate2,fig.height=4,fig.width=8,cache=TRUE}
set.seed(44333)
betaNorm <- betaCauch <- rep(NA,1000)
for(i in 1:1000){
  x <- rnorm(50); e <- rnorm(50); e2 <- rcauchy(50); b0 <- 1; b1 <- 2
  y <-  b0 + b1*x + e; y2 <- b0 + b1*x + e2
  betaNorm[i] <- lm(y ~ x)$coeff[2]; betaCauch[i] <- lm(y2 ~ x)$coeff[2]
}
quantile(betaNorm)
quantile(betaCauch)
```

---

## Monte Carlo Error

```{r,dependson="simulate2",fig.height=4,fig.width=8,cache=TRUE}
boxplot(betaNorm,betaCauch,col="blue",ylim=c(-5,5))
```

---

## Simulation based on a data set


```{r galton,fig.height=4,fig.width=8}
library(UsingR); data(galton); nobs <- dim(galton)[1]
par(mfrow=c(1,2))
hist(galton$child,col="blue",breaks=100)
hist(galton$parent,col="blue",breaks=100)
```

---

## Calculating means,variances

```{r,dependson="galton",fig.height=4,fig.width=8}
lm1 <- lm(galton$child ~ galton$parent)
parent0 <- rnorm(nobs,sd=sd(galton$parent),mean=mean(galton$parent))
child0 <- lm1$coeff[1] + lm1$coeff[2]*parent0 + rnorm(nobs,sd=summary(lm1)$sigma)
par(mfrow=c(1,2))
plot(galton$parent,galton$child,pch=19)
plot(parent0,child0,pch=19,col="blue")
```

---

## Simulating more complicated scenarios

```{r stamps,fig.height=4,fig.width=4}
library(bootstrap); data(stamp); nobs <- dim(stamp)[1]
hist(stamp$Thickness,col="grey",breaks=100,freq=F)
dens <- density(stamp$Thickness)
lines(dens,col="blue",lwd=3)
```


---

## A simulation that is too simple

```{r, dependson="stamps",fig.height=4,fig.width=4}
plot(density(stamp$Thickness),col="black",lwd=3)
for(i in 1:10){
  newThick <- rnorm(nobs,mean=mean(stamp$Thickness),sd=sd(stamp$Thickness))
  lines(density(newThick),col="grey",lwd=3)
}
```


---

## How density estimation works

<img class=center src=assets/img/kde.png height='80%' />

[http://en.wikipedia.org/wiki/File:Comparison_of_1D_histogram_and_KDE.png](http://en.wikipedia.org/wiki/File:Comparison_of_1D_histogram_and_KDE.png)

---

## Simulating from the density estimate

```{r, dependson="stamps",fig.height=4,fig.width=4}
plot(density(stamp$Thickness),col="black",lwd=3)
for(i in 1:10){
  newThick <- rnorm(nobs,mean=stamp$Thickness,sd=dens$bw)
  lines(density(newThick),col="grey",lwd=3)
}
```


---

## Increasing variability 

```{r, dependson="stamps",fig.height=4,fig.width=4}
plot(density(stamp$Thickness),col="black",lwd=3)
for(i in 1:10){
  newThick <- rnorm(nobs,mean=stamp$Thickness,sd=dens$bw*1.5)
  lines(density(newThick,bw=dens$bw),col="grey",lwd=3)
}
```


---

## Notes and further resources

__Notes__

* Simulation can be applied to missing data problems - simulate what missing data might be
* Simulation values are often drawn from standard distributions, but this may not be appropriate
* Sensitivity analysis means trying different simulations with different assumptions and seeing how estimates change

__Further resources__

* [Advanced Data Analysis From An Elementary Point of View](http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf)
* [The design of simulation studies in medical statistics](http://www.soph.uab.edu/ssg/files/Club_ssg/MPadilla_07.pdf)
* [Simulation studies in statistics](http://www4.stat.ncsu.edu/~davidian/st810a/simulation_handout.pdf)


<!--=*=*=*=*-->
<!--./week8/003courseWrapUp/index.Rmd
-->
---
title       : Course wrap-up
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow  # 
widgets     : [mathjax,bootstrap]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Why we do applied statistics

"It is not the critic who counts: not the man who points out how the strong man stumbles or where the doer of deeds could have done better. The credit belongs to the man who is actually in the arena, whose face is marred by dust and sweat and blood, who strives valiantly, who errs and comes up short again and again, because there is no effort without error or shortcoming, but who knows the great enthusiasms, the great devotions, who spends himself for a worthy cause; who, at the best, knows, in the end, the triumph of high achievement, and who, at the worst, if he fails, at least he fails while daring greatly, so that his place shall never be with those cold and timid souls who knew neither victory nor defeat."

![teddy](./assets/img/teddy.jpeg) _Theodore Roosevelt, 26th President of the United States_

[Statistics and the science game](http://simplystatistics.org/2012/06/22/statistics-and-the-science-club/)

---

## The key challenge in applied statistics

<q>Ask yourselves, what problem have you solved, ever, that was worth solving, where you knew knew all of the given information in advance? Where you didn’t have a surplus of information and have to filter it out, or you didn’t have insufficient information and have to go find some?</q>

<img src=assets/img/meyer.jpg height='30%' /> [Dan Myer, Mathematics Educator](http://www.ted.com/talks/dan_meyer_math_curriculum_makeover.html)


---


## Why applied statistics?

<img class=center src=assets/img/deluge.jpeg height='80%' />

---

## Why applied statistics?

<img class=center src=assets/img/nytimes.png height='80%' />

---

## Why applied statistics? 

<img class=center src=assets/img/mckinsey.png height='80%' />

---

## Why are you lucky?

<img class=center src=assets/img/bezos.jpg height='80%' />

---

## Why are you lucky

<img class=center src=assets/img/heritage.png height='80%' />

[Heritage Health Prize](http://www.heritagehealthprize.com/c/hhp)

---

## New data drives new statistical ideas

* How do we make better beer?    
  * __Data__: Measures of beer quality
  * __Statistic__:The [t-statistic](http://en.wikipedia.org/wiki/T-statistic)
* What characteristics of field lead to better crops?
  * __Data__: Field characteristics, crop yields
  * __Statistic__: [Analysis of variance (ANOVA)](http://en.wikipedia.org/wiki/Analysis_of_variance)
* How long do people live?
  * __Data__: Survival times of people (censored)
  * __Statistic__: [Kaplan-Meier Estimator](http://en.wikipedia.org/wiki/Kaplan%E2%80%93Meier_estimator)
* What movies will you like? 
  * __Data__: Lots of other peoples movie ratings
  * __Statistic(s)__: [Recommender systems](http://en.wikipedia.org/wiki/Recommender_system) 

---

## Who is an applied statistican? 

<div class='span4'>
<a href="http://en.wikipedia.org/wiki/Daryl_Morey">Daryl Morey</a>
</br></br>
 <img height=30% src=assets/img/morey.jpeg />
</div>
<div class='span4'>
<a href="http://www.hilarymason.com/">Hilary Mason</a>
</br></br>
 <img height=30% src=assets/img/mason.jpeg />
</div>
<div class='span4'>
<a href="http://ai.stanford.edu/~koller/"> Daphne Koller</a>
</br></br>
 <img  height=30% src=assets/img/koller.jpeg />
</div>
<div class='span4'>
<a href="http://fivethirtyeight.blogs.nytimes.com/">Nate Silver</a>
</br></br>
 <img height=30% src=assets/img/silver.jpeg />
</div>

---

## An important goal

<img class=center src=assets/img/venn.png height='80%' />

[Drew Conway](http://www.drewconway.com/zia/?p=2378)

---

## These might be useful

<img class=center src=assets/img/openintro.png height='80%' />

[http://www.openintro.org/](http://www.openintro.org/)


---

## These might be useful

<img class=center src=assets/img/elemlearn.jpeg height='80%' />

[http://www-stat.stanford.edu/~tibs/ElemStatLearn/](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)

---

## These might be useful

<img class=center src=assets/img/ada.png height='80%' />


[Advanced Data Analysis from An Elementary Point of View](http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf)

---

## Also check out

* [Andrew Gelman's blog](http://andrewgelman.com/)
* [Larry Wasserman's blog](http://normaldeviate.wordpress.com/)
* [Statsblogs](http://www.statsblogs.com/)
* [Flowing Data](http://flowingdata.com/)
* [junkcharts](http://junkcharts.typepad.com/)
* [Hilary Mason's Blog](http://www.hilarymason.com/) and [@hmason](https://twitter.com/hmason)
* [Cosma Shalizi's Blog](http://masi.cscs.lsa.umich.edu/~crshalizi/weblog/)
* [Some other guys' blog](http://simplystatistics.org/)


[The top Biostatistics Department in the World](http://biostat.jhsph.edu/) - No bias here :-)


---

## It has been my exteme pleasure

<center> Thank you so much for all of your dedication, time, and enthusiasm. This has been a wonderful experience for me and I hope it has been for you too. </center>



<!--=*=*=*=*-->
